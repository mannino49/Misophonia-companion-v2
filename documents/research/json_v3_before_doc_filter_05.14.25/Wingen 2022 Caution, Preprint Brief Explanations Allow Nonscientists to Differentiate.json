{
  "doc_type": "scientific paper",
  "title": "Caution, Preprint! Brief Explanations Allow Nonscientists to Differentiate Between Preprints and Peer-Reviewed Journal Articles",
  "authors": [
    "Tobias Wingen",
    "Jana B. Berkessel",
    "Simone Dohle"
  ],
  "year": 2022,
  "journal": "Advances in Methods and Practices in Psychological Science",
  "doi": "10.1177/25152459211070559Advances",
  "abstract": "A growing number of psychological research findings are initially published as preprints. Preprints are not peer reviewed and thus did not undergo the established scientific quality-control process. Many researchers hence worry that these preprints reach nonscientists, such as practitioners, journalists, and policymakers, who might be unable to differentiate them from the peer-reviewed literature. Across five studies in Germany and the United States, we investigated whether this concern is warranted and whether this problem can be solved by providing nonscientists with a brief explanation of preprints and the peer-review process. Studies 1 and 2 showed that without an explanation, nonscientists perceive research findings published as preprints as equally credible as findings published as peer-reviewed articles. However, an explanation of the peer-review process reduces the credibility of preprints (Studies 3 and 4). In Study 5, we developed and tested a shortened version of this explanation, which we recommend adding to preprints. This explanation again allowed nonscientists to differentiate between preprints and the peer-reviewed literature. In sum, our research demonstrates that even a short explanation of the concept of preprints and their lack of peer review allows nonscientists who evaluate scientific findings to adjust their credibility perception accordingly. This would allow harvesting the benefits of preprints, such as faster and more accessible science communication, while reducing concerns about public overconfidence in the presented findings.",
  "keywords": [
    "preprints",
    "peer review",
    "credibility",
    "science communication",
    "publishing"
  ],
  "research_topics": [
    "preprints in psychological research",
    "peer review process",
    "credibility perception by nonscientists",
    "science communication",
    "public understanding of scientific publication",
    "open science movement"
  ],
  "created_at": "2025-05-05T04:00:06.305433Z",
  "source_pdf": "documents/research/Global/Wingen 2022 Caution, Preprint Brief Explanations Allow Nonscientists to Differentiate.pdf",
  "sections": [
    {
      "section": "Page 1",
      "page_number": 1,
      "text": "ASSOCIATION FOR\nPSYCHOLOGICAL SCIENCE\nCreative Commons NonCommercial CC BY-NC: This article is distributed under the terms of the Creative Commons Attribution-NonCommercial 4.0 \nLicense (https://creativecommons.org/licenses/by-nc/4.0/), which permits noncommercial use, reproduction, and distribution of the work without \nfurther permission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-at-sage).https://doi.org/10.1177/25152459211070559Advances in Methods and  \nPractices in Psychological Science\nJanuary-March 2022, Vol. 5, No. 1, \npp. 1 –15\n© The Author(s) 2022\nArticle reuse guidelines:  \nsagepub.com/journals-permissions\nDOI: 10.1177/25152459211070559\nwww.psychologicalscience.org/AMPPSEmpirical Article\nScientific findings, in psychology and beyond, are rapidly \nbecoming more open and accessible. As part of this open-\nscience movement, preprints—that is, scientific manu -\nscripts preceding formal peer review and publication—have \ngained popularity, and their number is growing exponen -\ntially (see Fig. 1). This development has been accelerated \nby the COVID-19 crisis, during which researchers aim to \nrapidly disseminate their findings instead of going through \nthe traditional peer-review process (Kwon, 2020; Polka \net al., 2021; Rahal & Heycke, 2020). Moreover, this devel -\nopment was facilitated by an increasing availability of \npreprint servers in general (e.g., OSF Preprints) but also \nfor specific disciplines (e.g., PsyArXiv for psychological \nresearch).The fact that preprints are typically not peer reviewed \ndoes not seem to be a significant barrier to their success. \nOne reason for this may be that the peer-review process \nhas several drawbacks. First, the peer-review process is \ntime-consuming and contributes to a substantial delay \nbetween the discovery and the publication of research \nfindings (Cooke et al., 2016; Huisman & Smits, 2017). \nSecond, peer reviewers are humans, and thus their judg -\nments can be biased and influenced by factors other 1070559 AMPXXX10.1177/25152459211070559 Wingen et al. Caution, Preprint!\nresearch-article 2022\nCorresponding Author:\nTobias Wingen, Department of Psychology, University of Cologne, \nRichard-Strauss-Str. 2, 50931 Cologne, Germany \nEmail: tobias.wingen@uni-koeln.deCaution, Preprint! Brief Explanations Allow \nNonscientists to Differentiate Between \nPrepri nts and Peer-Reviewed Journal Articles\nTobias Wingen1, Jana B. Berkessel2, and Simone Dohle1\n1Department of Psychology, University of Cologne, Cologne, Germany, and 2Mannheim Centre for \nEuropean Social Research, University of Mannheim, Mannheim, Germany\nAbstract\nA growing number of psychological research findings are initially published as preprints. Preprints are not peer reviewed \nand thus did not undergo the established scientific quality-control process. Many researchers hence worry that these \npreprints reach nonscientists, such as practitioners, journalists, and policymakers, who might be unable to differentiate \nthem from the peer-reviewed literature. Across five studies in Germany and the United States, we investigated whether \nthis concern is warranted and whether this problem can be solved by providing nonscientists with a brief explanation \nof preprints and the peer-review process. Studies 1 and 2 showed that without an explanation, nonscientists perceive \nresearch findings published as preprints as equally credible as findings published as peer-reviewed articles. However, an \nexplanation of the peer-review process reduces the credibility of preprints (Studies 3 and 4). In Study 5, we developed and \ntested a shortened version of this explanation, which we recommend adding to preprints. This explanation again allowed \nnonscientists to differentiate between preprints and the peer-reviewed literature. In sum, our research demonstrates that \neven a short explanation of the concept of preprints and their lack of peer review allows nonscientists who evaluate \nscientific findings to adjust their credibility perception accordingly. This would allow harvesting the benefits of preprints, \nsuch as faster and more accessible science communication, while reducing concerns about public overconfidence in the \npresented findings.\nKeywords\npreprints, peer review, credibility, science communication, publishing\nReceived 6/18/21; Revision accepted 11/27/21"
    },
    {
      "section": "Page 2",
      "page_number": 2,
      "text": "2 Wingen et al.\nthan scientific quality (Helmer et al., 2017; Jukola, 2017; \nOkike et al., 2016). Finally, peer review may further \nhinder scientific progress because some reviewers \noppose unconventional theories, methods, and prac -\ntices, such as publishing nonsignificant findings or \nfailed replications (Eisenhart, 2002; Elson et al., 2020; \nFrench, 2012; Olson et al., 2002). For these reasons, \nsome scholars even argue that peer review is a deeply \nflawed process and should be abolished (Heesen & \nBright, 2021; Smith, 2006).\nNevertheless, peer review is currently the established \nstandard quality-control process for scientific publica -\ntions (e.g., Elson et al., 2020; Nosek & Bar-Anan, 2012). \nIndeed, there is empirical evidence that peer-reviewed \nmanuscripts have a higher quality of reporting compared \nwith their non-peer-reviewed version (Carneiro et al., \n2019; Cobo et al., 2011; Goodman et al., 1994). More -\nover, various studies have shown that peer reviewers \nusually detect some errors in manuscripts (Godlee et al., \n1998; Okike et al., 2016; Schroter et al., 2004). Hence, \nresearchers across disciplines consider peer review as a \nguiding principle on which work they read and cite. For \nexample, a large international survey found that scien -\ntists considered peer review as the most significant factor \nfor determining the quality and trustworthiness of \nresearch (Tenopir et  al., 2016), and most scientists \nemphasize that it is important that preprints are ulti -\nmately submitted to a peer-reviewed journal (Soderberg \net al., 2020).However, preprints are not available only to scientists \n(who, in general, can be assumed to know that preprints \nare not peer reviewed). Instead, because preprints typi -\ncally are published in open access, they are also openly \navailable to the general public, who might not be aware \nthat preprints are usually not peer reviewed. In fact, \nespecially during the COVID-19 crisis, many preprints \nbecame part of the public discourse through traditional \nand social media (Fraser et al., 2021). For example, a \nnow-retracted preprint that described an “uncanny simi -\nlarity” between SARS-CoV-2 and HIV spurred discussion \non social media on whether SARS-CoV-2 is a genetically \nengineered bioweapon (Koerber, 2021), which later \nbecame one of the leading coronavirus-related conspir -\nacy theories (Imhoff & Lamberty, 2020). Presumably \nbecause of this incident, the preprint server bioRxiv, who \nprovided this questionable preprint, added a warning to \ntheir website that preprints are preliminary, non-peer-\nreviewed reports (Forster, 2020). In another example, a \npreprint on the SARS-CoV-2 viral load in children was \ndisparaged on the title page of the largest German news -\npaper (Niggemeier, 2020). The newspaper, however, \nignored that the work was a preprint and heavily criti -\ncized some preliminary analyses. This public debate over \na preprint might have damaged trust in science in Germany \n(Lindner, 2020), which could have had serious conse -\nquences for the adherence and adoption of recom -\nmended protective behaviors (Dohle et al., 2020). These \nexamples illustrate what many researchers fear: members \nof the general public treating non-peer-reviewed pre -\nprints as established evidence, leading to ill-advised \ndecisions and potentially damaging public trust in sci -\nence (Fox, 2018; Heimstädt, 2020; Rahal & Heycke, 2020; \nSheldon, 2018).\nThis concern about preprints, which has been \ndescribed as the most frequent argument against them \n(Vazire, 2020), goes beyond COVID-19-related research \nand is highly relevant for all research findings of public \ninterest. Indeed, media outlets and public-science com -\nmunication blogs also cover preprints on psychological \ntopics such as climate change anxiety (Chow, 2021), \npersonality (Adam, 2019), or even the trustworthiness of \npsychological research as a whole (Chivers, 2020). Pre -\nprints in psychology may be especially likely to catch \nthe public eye because they deal with questions related \nto human behavior and society. It thus seems likely that \nsome nonscientists even directly seek out psychological \npreprints because they often address topics highly rel -\nevant to their lives.\nThe central assumption underlying concerns about \nthe public availability of preprints is that nonscientists \nfail to differentiate between preprints and peer-reviewed \nliterature and thus treat them as equally credible sources. \nHowever, this assumption currently lacks empirical 010002000300040005000600070008000900010000\n2017 2018 2019 2020\nYearNumber of Published Manuscripts\nOSF Preprints PsyArXiv\nFig. 1. Development of the number of manuscripts published per year \non two major preprint servers for psychology and the social sciences \nsince 2017. Numbers were derived by searching for available preprints \non Google Scholar and filtering for each year and server."
    },
    {
      "section": "Page 3",
      "page_number": 3,
      "text": "Caution, Preprint!  3\nevidence. Because preprints are often presented with no \nor very little accompanying information (e.g., simply \nstating that the results stem from a preprint), we believe \nthat in such a situation, nonscientists will indeed fail to \nincorporate this information in their credibility judgment. \nThis is because they lack the necessary background \nknowledge that preprints are not peer reviewed. We \nhypothesize that without an additional explanation of \npreprints and their lack of peer review, people will \nperceive research findings from preprints as equally \ncredible compared with research findings from the peer-\nreviewed literature (Hypothesis 1).\nHowever, recent research suggests that even very brief \nexplanations (e.g., warning labels) allow nonscientists \nto adjust their credibility ratings (Koch et al., 2021), even \nfor complex scientific topics (Anvari & Lakens, 2018; \nHendriks et al., 2020; Wingen et al., 2020). If such a brief \nexplanation of preprints includes that they are not peer \nreviewed and thus did not undergo the established stan -\ndard quality-control process for psychological publica -\ntions, nonscientists might perceive preprints as less \ncredible. Emphasizing increased quality control, for exam -\nple through consumer reviews or quality-management \nsystems (Adena et al., 2019; Boiral, 2012; Resnick et al., \n2006; Silva & Topolinski, 2018), and highlighting adher -\nence to community norms and standards (Bachmann & \nInkpen, 2011; Blanchard et al., 2011; Wenegrat et al., \n1996) are linked to increased credibility and trustworthi -\nness. We thus hypothesize that after receiving an explana -\ntion of preprints and their lack of peer review, nonscientists \nwould perceive preprints as less credible than peer-\nreviewed articles (Hypothesis 2).\nOverview of Studies\nWe conducted five experimental studies to test whether \nnonscientists perceive preprints as less credible than \npeer-reviewed literature and whether this depends on \nwhether they receive an explanation of the peer-review \nprocess. We focused on preprints covering research find -\nings from psychology and the social sciences because \nthey seem particularly likely to be comprehensible and \ninteresting to the general public. In the pilot study, we \nexplored whether preprints in psychology and the social \nsciences typically provide an explanation of preprints \nand the peer-review process. We coded 200 recent pre -\nprints and examined whether they sufficiently explain \ntheir lack of peer review. Study 1 (German sample) and \nStudy 2 (U.S. sample) tested whether nonscientists would \nbe able to differentiate between peer-reviewed literature \nand preprints without an explanation of preprints and \nthe peer-review process. Study 3 (within-subjects design) \nand Study 4 (between-subjects design) tested whether \nnonscientists would perceive preprints as less credible than peer-reviewed articles after receiving an explana -\ntion of preprints and their lack of peer review. Finally, \nin Study 5, we developed a shortened version of this \nexplanation and tested whether this very brief explana -\ntion allowed nonscientists to differentiate between pre -\nprints and peer-reviewed literature. We, moreover, \ncross-sectionally explored how this explanation may \nwork (mediation) and whether the effect of this explana -\ntion depends on education and familiarity with the pub -\nlication process (moderation).\nPreregistration\nStudies 1 to 5 and the Supplemental Study 1 are prereg -\nistered. All preregistration forms are shared on the OSF \n(https://osf.io/egkpb). The pilot study, which focused \non coding existing data, was not preregistered.\nData, materials, and online resources\nAll materials, anonymized data sets, and analyses code \nare shared on the OSF. Statistical analyses were con -\nducted using R (Version 4.0.4; R Development Core \nTeam, 2021), and for the main analyses, we relied on \nthe packages effsize  (Torchiano, 2020), lavaan  (Rosseel, \n2012), psych  (Revelle, 2021), pwr (Champely et al., \n2018), yarrr  (Phillips, 2017), and TOSTER  (Lakens, \n2017). Details regarding our recruitment strategy and \nregarding one additional study (see Reporting section) \nare reported in the Supplemental Material available \nonline.\nReporting\nFor each study, we report how we determined our sam -\nple size, all data exclusions, all manipulations, and all \nmeasures in the study.\nThe studies are numbered 1 through 5 for narrative \nstyle. Chronologically, the studies were run in the fol -\nlowing order: 3, 4, 1, 5, 2. Coding for the pilot study was \ncompleted shortly after Study 3. We conducted one fur -\nther study before Study 5. We found that this study likely \ncontains a high percentage of inattentive respondents \n(for details, see the Supplemental Material available \nonline), which render the obtained null results largely \nuninterpretable. We thus refrain from discussing this \nstudy in the main text, but to increase transparency, we \nprovide details about this study in the Supplemental \nMaterial available online and on the OSF. All analyses \nwith a preregistered hypothesis were tested with one-\nsided p values. In all studies in which we predicted the \nabsence of an effect, we relied on equivalence tests with \npreregistered equivalence bounds. This is a commonly \nrecommended frequentist method to provide evidence"
    },
    {
      "section": "Page 4",
      "page_number": 4,
      "text": "4 Wingen et al.\nfor the absence of a meaningful effect (Lakens, 2017; \nLakens et al., 2018).\nAll participants who completed our studies were \nincluded in the analyses unless they met preregistered \nexclusion criteria or did not respond to our central depen -\ndent variable (i.e., perceived credibility, not explicitly \npreregistered). Participants were blocked from participat -\ning in more than one study to avoid nonnaïveté (Chandler \net al., 2015). Sample sizes were preregistered in Studies \n1 to 5; however, some deviations occurred because we \nrecruited participants online and thus had limited control \nover the final sample size (for details regarding sample \nsizes and deviations, see the Supplemental Material avail -\nable online). However, in no case was the final sample \nsize determined based on the obtained results.\nEthical approval\nAll studies were conducted consistently with the Decla -\nration of Helsinki, and all are exempt from institutional \nreview board approval by guidelines of the German \nPsychological Society (2018).\nPilot Study\nMethod\nFor the pilot study, we collected the information pre -\nsented in the 303 most recent manuscripts (at the time \nof coding; June 2020) on two popular social science \npreprint servers, commonly used by psychological sci -\nentists. These servers were PsyArXiv (https://psyarxiv  \n.com) and the social and behavioral sciences section at \nOSF Preprints (https://osf.io/preprints). We first col -\nlected general bibliographic information (authors, pub -\nlication date, language, doi, whether the manuscript was \na postprint). We excluded 63 manuscripts from our \nanalyses because they appeared to be accepted versions \nof articles (postprints) and thus peer reviewed, thereby \nnot meeting our definition of preprints. We furthermore \nexcluded 33 non-English preprints and, finally, seven documents that were not preprints (e.g., supplemental \nmaterials, book chapter scans).\nGiven these necessary exclusions, the coders contin -\nued coding (by going back further in time and coding \nearlier preprints) until eventually 200 suitable manu -\nscripts (100 from each server) were included. We coded \nwhether the authors of the preprint (a) mentioned that \nit is a preprint, (b) mentioned that it is thus not peer \nreviewed, (c) explained that peer review serves as a \nquality-control process, (d) explained that peer review \nis the standard procedure for scientific publication, (e) \nand/or added another indication that the findings might \nbe preliminary or less credible.\nResults\nThe results showed that only 27.50% of the preprints \nexplicitly stated that they were preprints. Even fewer \npreprints (15.50%) contained information that they had \nnot undergone peer review yet. Finally, not a single pre -\nprint provided information explaining that peer review \nserves as a quality-control measure. Detailed results for \neach preprint server are presented in Table 1.\nStudy 1\nIn Study 1, we tested whether participants would evalu -\nate psychological research findings that were published \nas peer-reviewed articles as equally credible as research \nfindings published as preprints.\nMethod\nParticipants and design. Participants were German \nuniversity students recruited online in exchange for course \ncredits and individuals recruited through postings in pub -\nlic German social media groups for voluntary research \nparticipation. The study employed a between-subjects \nexperimental design. We randomly assigned participants \nto one of two between-subjects conditions (preprint con -\ndition, peer-review condition). Sample size considerations Table 1.  Information About Peer Review in Recent Preprints on Two Major Preprint Servers\nNumber of preprints informing their readers that: OSF Preprints PsyArXiv Overall\nThey are a preprint (or similar) 30.00% 25.00% 27.50%\nAre not peer reviewed (or similar) 13.00% 18.00% 15.50%a\nPeer review is typically part of the scientific \npublication process 1.00%  0.00% 0.50%\nPeer review serves as a quality-control measure  0.00%  0.00% 0.00%\nTheir findings might be preliminary (or similar)  6.00%  0.00% 3.00%\naThe overall number includes nine publications mentioning that they are “under review” but not 11 \npublications mentioning that they have been “submitted for publication” because we believe the latter \ndoes not clearly indicate to nonscientists that the work has not yet been peer reviewed."
    },
    {
      "section": "Page 5",
      "page_number": 5,
      "text": "Caution, Preprint!  5\nwere made in relation to Study 4, which chronologically \ntook place before Study 1; compared with Study 4, we \naimed to double our sample size. The recruited sample \nwas slightly larger and consisted of 277 participants (after \nexcluding 35 participants who already took part in Study \n4, as preregistered), out of which 204 provided responses \nto all credibility ratings and were therefore included in the \nmain analysis (74.5% female; age: M = 25.41 years, SD = \n7.09). Power analyses revealed that the sample size of 204 \nhad a 99.87% power to detect the effect observed in Study \n4 (d = 0.70, α = .05) and a 95% power to demonstrate in \nan equivalence test that an observed effect is considerably \nsmaller than the effect observed in Study 4 (preregistered \nequivalence bound of d < 0.5 compared with d = 0.70 in \nStudy 4).\nProcedure. Participants were presented with five differ -\nent research findings (for an overview of research findings \nused as stimuli, see Table 2). The findings were described \nas being published either as a peer-reviewed journal article \nor as a preprint, depending on condition. For each research \nfinding, participants indicated their perceived credibility \n(“How credible is this study result?”) on a 7-point scale \n(1 = not at all credible , 7 = very credible ). Participants \nreceived no further information (e.g., an explanation of the \npeer-review process). In fact, all five findings (Gervais & \nNorenzayan, 2012; Hauser et al., 2014; Nishi et al., 2015; \nShah et al., 2012; Wilson et al., 2014) were published in the \npeer-reviewed journals Nature  or Science . Descriptions of \nthese findings were adapted from prior work and were \nproved to be comprehensible to nonscientists (Hoogeveen \net al., 2020). Findings covered various psychological and \neconomic behavioral science topics, and participants \njudged the credibility of these five research findings. An \naverage credibility score across all five ratings was com -\nputed and served as the dependent variable.\nResults\nIn line with our preregistration, we computed an average \ncredibility score across all five credibility ratings. As \npredicted, without a brief explanation, participants considered research findings published as preprints ( M = \n4.09, SD = 0.80) to be equally credible compared with \nfindings published as peer-reviewed journal articles ( M = \n4.24, SD = 0.88), t(202) = 1.25, p = .211, d = 0.18, 95% \nconfidence interval [CI] = [–0.10, 0.46]. This finding is \npresented in Figure 2. A preregistered equivalence test, \na test that provides support for the absence of a mean -\ningful effect, showed that the observed effect size, which \nis conventionally considered very small, was equivalent \nwith an interval containing only small to medium effects \n(d < 0.5), t(202) = 2.29, p = .012. Descriptive statistics \nfor the perceived credibility across studies and condi -\ntions throughout this article are presented in Table 3.\nStudy 2\nIn Study 2, we aimed to replicate the findings from Study \n1 in a different population using an even larger sample Table 2.  Overview of Research Findings Used as Stimuli in Studies 1, 2, 4, and 5\nAuthors Short description\nGervais and Norenzayan (2012) Analytical thinking promotes religious disbelief.\nHauser et al. (2014) When making collective decisions, people share more \ncommon resources for future generations.\nNishi et al. (2015) Financial inequality between group members remains \nwhen people are informed about each member’s wealth.\nShah et al. (2012) Poverty drains people’s attention.\nWilson et al. (2014) People dislike doing nothing and prefer an engaged mind.\nPublished asPerceived Credibility (1−7)\n1.522.533.544.555.566.57\nPreprint Peer-Reviewed Article\nFig. 2. Pirate plot showing perceived credibility as a function of pub -\nlication mode in Study 1 (German participants), in which participants \nreceived no explanation of preprints. The black dots represent the \njittered raw data, which are shown with smoothed densities indicating \nthe distributions in each condition. The central tendency is the mean, \nand the intervals represent two standard errors around the mean."
    },
    {
      "section": "Page 6",
      "page_number": 6,
      "text": "6 Wingen et al.\nsize ( N = 466; U.S. sample) and a stricter preregistered \ncriterion of what constitutes a negligible difference ( d < \n0.3). The design was identical to Study 1 except that \nStudy 2 also included a basic text-comprehension check \nthat had to be answered correctly to ensure that partici -\npants were aware that the five research findings were \npublished as preprints or peer-reviewed journal articles, \nrespectively.\nMethod\nParticipants and design. Participants were U.S.-based \nindividuals recruited on the Amazon Mechanical Turk \n(MTurk) platform in exchange for $0.50. The target sample \nsize was set to 578, which allowed us to detect group dif -\nferences of d = 0.30 (1 – β = 0.95, α = .05) and, moreover, \nprovided sufficient power for an equivalence tests (1 – β = \n0.95, equivalence bounds of d = 0.3). To increase data qual -\nity, we opted to exclude participants who failed a basic \ntext-comprehension check (see below). This decision was \nbased on previous research raising concerns about MTurk \nworkers not reading study materials or even being bots \n(Chmielewski & Kucker, 2020). To compensate for poten -\ntial exclusions, we recruited 753 participants, of which 476 \npassed the preregistered comprehension check. Finally, 466 \nparticipants answered all credibility items and were there -\nfore included in the main analysis (42.15% female; age:  M = 37.00 years, SD = 11.97). Despite this reduced sample \nsize, a sensitivity analysis revealed that the final sample size \nhad an 80% power (with α = .05) to detect an effect of d = \n0.26 and a 95% power to detect d = 0.33.\nProcedure. For the text-comprehension check, partici -\npants had to answer how the research findings were pub -\nlished and were presented with eight options (e.g., “as \ntextbooks,” “as preprints”). If participants answered the \ntext-understanding question incorrectly, they were asked \nto carefully read the text again. If they failed the text-\nunderstanding question again, they were excluded from \nour analyses. We also added a few exploratory questions \nabout whether participants perceived the research find -\nings as strictly quality controlled, whether they believed \nthat the researchers adhered to the standard publication \nprocedure, and participants’ education and familiarity with \nthe publication process (to ensure comparability with \nStudy 5). Apart from this, the procedure and design were \nidentical to Study 1.\nResults\nWe computed an average credibility score across all five \ncredibility ratings. As predicted and in line with Study \n1, participants rated research findings from preprints \n(M = 4.58, SD = 1.00) as equally credible as research Table 3.  Perceived Credibility of Research Findings Depending on Source and Explanation Across All Studies \nPresented in This Article\nStudy M SD t valueadf p valuebCohen’s d [95% \nconfidence \ninterval]\nStudy 1 (without explanation)  \n Peer-reviewed article 4.24 0.88  \n Preprint 4.09 0.80 1.25 202 .211 0.18 [–0.10, 0.46]\nStudy 2 (without explanation)  \n Peer-reviewed article 4.73 1.11  \n Preprint 4.58 1.00 1.50 464 .136 0.14 [–0.04, 0.32]\nStudy 3 (with explanation)  \n Peer-reviewed article 5.63 1.34  \n Preprint 4.00 0.93 10.06  51< .001 (one-sided) dz = 1.39\nStudy 4 (with explanation)  \n Peer-reviewed article 4.15 0.65  \n Preprint 3.67 0.72 3.74 111< .001 (one-sided) 0.70 [0.32, 1.09]\nStudy 5 (with explanation)  \n Peer-reviewed article 4.65 1.00  \n Limited information 4.42 1.15 2.02 379 .044 0.21 [0.01, 0.41]\n Authors’ explanation 4.39 1.07 2.31 359 .010 (one-sided) 0.25 [0.04, 0.45]\n External explanation 4.31 1.02 3.25 379< .001 (one-sided) 0.33 [0.13, 0.54]\naThe t-tests results refer to the comparison of the respective condition with the peer-review condition in each study. These are t \ntests for dependent samples in Study 3 and for independent samples in the other studies.\nbOne-sided p values are reported for directional hypotheses."
    },
    {
      "section": "Page 7",
      "page_number": 7,
      "text": "Caution, Preprint!  7\nresults from peer-reviewed journal articles ( M = 4.73, \nSD = 1.11), t(464) = 1.50, p = .136, d = 0.14, 95% CI = \n[–0.04, 0.32]. This finding is presented in Figure 3. An \nequivalence test showed that this observed effect size, \nwhich is conventionally considered very small, was \nequivalent with an interval containing only small effects \n(d < 0.3), t(464) = 1.741, p = .041.\nStudy 3\nStudies 1 and 2 found that without an explanation, non -\nscientists rated research findings from preprints as equally \ncredible as research findings from peer-reviewed journal \narticles. Study 3 tested whether nonscientists truly believe \nthat the two types are equally credible or whether they \nstart to differentiate once they get an explanation of \npreprints and the peer-review process and can directly \ncompare these two options. Study 3 straightforwardly \ntested this by employing a within-subjects design in \nwhich participants rated research findings in general.\nMethod\nParticipants and design. Participants were recruited \nthrough postings in public German social media groups for \nvoluntary research participation. The targeted sample size \nwas set to 45, based on an a priori power analysis for \n95% power (one-sided α of .05) to detect a moderate \neffect of dz = 0.5 that would be typical for similar social-\npsychological research. The recruited sample was slightly \nlarger, as is often the case in online studies, and consisted \nof 65 participants. Of these participants, 52 responded to all credibility items and were therefore included in the main \nanalysis (73.08% female; age: M = 30.83 years, SD = 9.71).\nProcedure. This study employed a within-subjects design. \nParticipants read a short, jargon-free description of the \npeer-review process, which highlighted that peer review \nserves as a quality-control process and that peer review cur -\nrently is the standard procedure for scientific publication. \nThey were also informed that some research findings are \ninitially published before the peer-review process as pre -\nprints to achieve rapid dissemination of results. The full \ndescription reads as follows (translation by authors):\nUsually, scientific articles are subject to an extensive \npeer-review process. This means that other scientists \nanonymously review articles submitted to a scien -\ntific journal. They then speak out for or against a \npublication and provide important suggestions for \narticle improvement. This procedure is considered \nthe gold standard of scientific journals. Only articles \nthat receive positive reviews have a chance of being \npublished. This procedure is intended to ensure that \nthe articles are of particularly high quality. However, \nsome articles are now published online as preprints \nwithout having been peer reviewed. This allows \nscientists to make their results available to the public \nvery rapidly, whereas the time-consuming peer-\nreview process can take several months. Normally, \npeer review is then carried out after the article has \nbeen submitted to a scientific journal.\nAfterward, participants reported the perceived credi -\nbility of research findings published as peer-reviewed \narticles (“How credible are research findings that are \npublished as journal articles [with peer review]?”) and as \npreprints (“How credible are research findings that are \npublished as preprints [without peer review]?”) on a \n7-point rating scale (1 = not at all credible , 7 = very cred -\nible). Finally, participants indicated whether they had \nheard about preprints and peer-reviewed articles before \nthe study, completed demographics, and were debriefed.\nResults\nAs predicted, participants rated research findings from \npreprints ( M = 4.00, SD = 0.93) as less credible than \nresearch results from peer-reviewed journal articles ( M = \n5.63, SD = 1.34), t(51) = 10.06, one-sided p < .001 , dz = \n1.39 (see Fig. 4).\nStudy 4\nStudy 4 tested whether the finding of Study 3 generalizes \nto a more realistic situation in which participants do not Published asPerceived Credibility (1−7)\n11.522.533.544.555.566.57\nPreprint Peer-Reviewed Article\nFig. 3. Pirate plot showing perceived credibility as a function of \npublication mode in Study 2 (U.S. participants), in which participants \nreceived no explanation of preprints."
    },
    {
      "section": "Page 8",
      "page_number": 8,
      "text": "8 Wingen et al.\ndirectly compare preprints and peer-reviewed articles \nwith each other. Instead, participants judged specific \nresearch findings, and the design was largely identical \n(and thus directly comparable) to Studies 1 and 2.\nMethod\nParticipants and design. Participants were German \nuniversity students recruited online in exchange for course \ncredits. The study employed a between-subjects experi -\nmental design. We randomly assigned participants to one \nof two between-subjects conditions (preprint condition, \npeer-review condition). The target sample size was set to \n102, based on an a priori power analysis for 80% power \n(one-sided α of .05) to detect a moderate effect of d = 0.5. \nThe recruited sample consisted of 140 participants, of \nwhich 113 responded to all credibility items and were \ntherefore included in the main analysis (76.11% female; \nage: M = 23.75 years, SD = 5.10).\nProcedure. Participants read the same short descriptions \nof the peer-review process and preprints as in Study 3 and \nanswered two exploratory text-comprehension questions. \nParticipants judged the credibility of five research findings \n(the same research findings used in Studies 1 and 2). The \nfindings were described as being published either as peer-\nreviewed journal articles or as preprints. Ratings were \nmade on a 7-point scale (1 = not at all credible , 7 = very \ncredible ). Participants also indicated whether they had \nheard about preprints and peer-reviewed articles before \nthe study, received an exploratory open-entry question on \nhow they made their credibility judgments, completed \ndemographics, and were debriefed.Results\nIn line with our preregistration, we computed an average \ncredibility score across all five credibility ratings. As pre -\ndicted, participants rated research findings from preprints \n(M = 3.67, SD = 0.72) as less credible than research find -\nings from peer-reviewed journal articles ( M = 4.15,  \nSD = 0.65), t(111) = 3.74, one-sided p < .001, d = 0.70, 95% \nCI = [0.32, 1.09]. This pattern is depicted in Figure 5.\nStudy 5\nIn Study 5, we developed a shortened version of the \nexplanation used in Studies 3 and 4, which could be \neasily added to preprints. We tested whether this expla -\nnation allows nonscientists to differentiate between pre -\nprints and the peer-reviewed literature. We further tested \nwhether it matters if this brief explanation is provided \nby the authors or by an external source but expected the \nexplanation to be effective in both cases. Because most \npreprints are published in English, we tested this in an \nEnglish-speaking population ( N = 727; U.S. sample). We \nalso aimed to explore the underlying mechanism of our \nexplanation and tested preregistered mediators (per -\nceived quality control and perceived adherence to pub -\nlication standards) and moderators (education and \nfamiliarity with the publication process).\nMethod\nParticipants and design. Participants were U.S.-based \nindividuals recruited on the Amazon MTurk platform in \nexchange for $0.50. We randomly assigned participants  \nto one of four between-subjects conditions (peer-review \ncondition, preprint: limited-information condition, pre -\nprint: authors’-explanation condition, preprint: external-\nexplanation condition). The target sample size was set to \n1,000, which allowed us to detect group differences of \nd = 0.29 (1 – β = 0.95, one-sided α of .05) and, moreover, \nprovided sufficient power for an equivalence tests (1 – β = \n0.91, equivalence bounds of d = 0.3). We recruited 1,051 \nparticipants, of which 739 passed the preregistered text-\ncomprehension check. For the text-comprehension check, \nparticipants had to answer how the research findings were \npublished (see Study 2). If an additional explanation of \npeer review and preprints was given, they also indicated \nfor three additional text-comprehension questions whether \nthey were true or false (“Scientific articles are usually peer \nreviewed”; “As part of the peer-review process, indepen -\ndent researchers evaluate the quality of the work”; and \n“Preprints have been peer reviewed”). If participants \nanswered any of the questions incorrectly, they were \nasked to read the text carefully again. If they again failed \nany of the text-comprehension questions, they were excluded \nfrom our analyses. Finally, 727 participants responded to Published asPerceived Credibility (1−7)\n22.533.544.555.566.57\nPreprint Peer-Reviewed Article\nFig. 4. Pirate plot showing perceived credibility as a function of \npublication mode in Study 3 (German participants), in which partici -\npants received an explanation and directly compared the two options."
    },
    {
      "section": "Page 9",
      "page_number": 9,
      "text": "Caution, Preprint!  9\nall credibility items and were thus included in the main \nanalyses (43.39% female; age: M = 39.24 years, SD = \n12.68). One participant did not respond to the remaining \nitems, which reduced the sample size for secondary anal -\nyses to 726. Despite this reduced sample size, a sensitivity \nanalysis revealed that for all possible group comparisons, \nour sample had at least an 80% power (with α = .05) to \ndetect an effect of d = 0.30 and a 95% power to detect  \nd = 0.39.\nProcedure. Participants learned that they would judge \nthe credibility of five research findings and were randomly \nassigned to one of four conditions. In the peer-review \ncondition, participants were informed that the findings \nwent through a peer-review process and were published \nin a scientific journal. The preprint: limited-information \ncondition stated that the findings were preprints, but in \ncontrast to Studies 1 and 2, it was also added that preprints \nare not peer reviewed (without further information, how -\never, what is meant by peer review). In the other two \nconditions, the research findings were presented as non-\npeer-reviewed preprints, but participants received an \nadditional explanation of preprints and the peer-review \nprocess. This additional explanation was allegedly either \nprovided by the authors of the preprint (preprint: authors’-\nexplanation condition) or without any reference to the \nsource in the introduction of the study (preprint: external-\nexplanation condition).\nThe explanation was drafted building on the informa -\ntion provided in Studies 3 and 4 but incorporated further \nfeedback from colleagues from various disciplines (anthro -\npology, biology, psychology, and sociology) and from \nnonscientists to ensure an interdisciplinary perspective and comprehensibility. The explanation highlighted two \nimportant aspects: that peer review serves as a quality-\ncontrol process and that peer review currently is the stan -\ndard procedure for scientific publication. Compared with \nStudies 3 and 4, we aimed to keep this explanation as \ncomprehensive as possible. This explanation read:\nScientific articles usually go through a peer-review \nprocess. This means that independent researchers \nevaluate the quality of the work, provide sugges -\ntions, and speak for or against the publication. \nPlease note that the present article has not (yet) \nundergone this standard procedure for scientific \npublications.\nAfter judging the credibility of the research findings, \nparticipants were also asked about the perceived quality \ncontrol of the research findings, the perceived adherence \nto scientific publication standards, their education, and \ntheir familiarity with the publication process. Credibility \nratings were given on a 7-point rating scale (1 = not at \nall credible , 7 = very credible ). Familiarity with the pub -\nlication process (“I am familiar with the scientific pub -\nlication process”), perceived quality control of the \nresearch findings (“The quality of the research findings \nhas been strictly controlled”), and perceived adherence \nto scientific publication standards (“When publishing \ntheir findings, the researchers followed the standard pro -\ncedure of the research community”) were measured on \na 7-point scale (1 = strongly disagree , 7 = strongly agree ).\nResults\nMain analyses. We again computed an average credi -\nbility score across all five credibility ratings. As predicted, \nacross both preprint-explanation conditions, participants \nreported lower credibility of research findings compared \nwith the peer-review condition ( M = 4.65, SD = 1.00). This \nwas the case when participants received the explanation \nby the authors ( M = 4.39, SD = 1.07), t(359) = 2.32, one-\nsided p = .010, d = 0.25, 95% CI = [0.04, 0.45], and by an \nexternal source ( M = 4.31, SD = 1.02), t(379) = 3.25, one-\nsided p < .001, d = 0.33, 95% CI = [0.13, 0.54] (see Figure \n6). Unexpectedly, this was also the case when participants \nreceived only very limited information ( M = 4.42, SD = \n1.15), t(379) = 2.02, p = .044, d = 0.21, 95% CI = [0.01, \n0.41]. The three preprint conditions did not significantly \ndiffer from each other (all ps > .317, all ds < .10), and the \nobserved differences between these conditions were all \nequivalent with an interval containing only small effects  \n(d < .3), all ps < .031 (see OSF analyses for details).\nQuality control and adherence to scientific publica -\ntion standards. However, the three preprint explana -\ntions differed regarding the perceived quality control of \nthe research findings and the perceived adherence to Published asPerceived Credibility (1 −7)\n1.522.533.544.555.56\nPreprint Peer-Reviewed Article\nFig. 5. Pirate plot showing perceived credibility as a function of pub -\nlication mode in Study 4 (German participants). This study provided \nparticipants with an explanation of preprints and the peer-review \nprocess."
    },
    {
      "section": "Page 10",
      "page_number": 10,
      "text": "10 Wingen et al.\nscientific publication standards. Participants who received \nan explanation reported lower perceived quality control \nof preprints compared with the limited information condi -\ntion ( M = 4.27, SD = 1.79). This was the case no matter \nwhether participants received this explanation by the \nauthors ( M = 3.72, SD = 1.63), t(344) = 2.97, one-sided p = \n.002, d = 0.32, 95% CI = [0.11,0.53], or by an external \nsource ( M = 3. 81, SD = 1.71), t(363) = 2.51, one-sided p = \n.006, d = 0.26, 95% CI = [0.06, 0.47]. Likewise, after receiv -\ning an explanation, participants reported lower perceived \nadherence to scientific publication standards compared \nwith the limited-information condition. This was again the \ncase no matter whether participants received this explana -\ntion by the authors ( M = 3.83, SD = 1.72), t(344) = 3.15, \none-sided p < .001, d = 0.34, 95% CI = [0.13,0.55], or by an \nexternal source ( M = 3.95, SD = 1.79), t(363) = 2.50, one-\nsided p = .007, d = 0.26, 95% CI = [0.05, 0.47].\nModeration analyses. In line with our preregistration, \nwe also tested whether education or familiarity with the \nscientific publication process moderated the effect of our \nexplanation on the perceived credibility of research find -\nings (compared with the peer-review condition). For these \nanalyses, we merged the preprint: authors’-explanation \ncondition and the preprint: external-explanation condition \nbecause they did not differ on any of the relevant vari -\nables. We conducted multiple linear regression analyses to \ntest whether any of our potential moderator variables \nmoderated the relationship between explanation (detailed explanation vs. peer review) and credibility. Indeed, whereas \ncentered education did not significantly interact with our \nexplanation ( b = −0.22, SE = 0.14), t(539) = 1.58, p = .115, \ncentered familiarity with the publication process was a \nsignificant moderator, which indicates that our explana -\ntion was more effective for people who indicated a higher \nfamiliarity with the scientific publication process ( b = \n−0.12, SE = 0.05), t(539) = 2.49, p = .013 (see Table 4).\nMediation analyses. Finally, we explored preregistered \nmediators of the effect of our explanation on credibility \n(compared with the peer-review condition). For these \ncross-sectional analyses, we again merged the authors’-\nexplanation condition and the external-explanation condi -\ntion because they did not differ on any of the relevant \nvariables. We investigated whether perceived quality con -\ntrol or perceived adherence to publication standards \nmediated the negative effect of explaining preprints on \nperceived credibility. To test this, we ran a parallel media -\ntion model (see Fig. 7) with 10,000 bootstrap resamples \nusing the R package lavaan  (Rosseel, 2012). This model \nrevealed that both perceived quality control ( b = −0.23, \n95% CI = [−0.14, −0.33]) and perceived adherence to pub -\nlication standards ( b = −0.25, 95% CI = [−0.15, −0.35]) \nsimultaneously mediated the effect.\nBecause this mediation model relied on cross-  \nsectional data, these results should be considered with \ncaution because the mediating variables were not experi -\nmentally manipulated and may be biased (Bullock et al., Published asPerceived Credibility (1−7)\n11.522.533.544.555.566.57\nPreprint:\nExt. ExplanationPreprint:\nAuthors’ Explanation Preprint:\nLimited InformationPeer-Reviewed\nArticle\nFig. 6.  Pirate plot showing perceived credibility as a function of publication mode and explanation \nin Study 5 (U.S. participants). Raw data are not visualized in the figure because of the large number \nof data points."
    },
    {
      "section": "Page 11",
      "page_number": 11,
      "text": "Caution, Preprint!  11\n2010). An observed statistical mediation cannot con -\nclusively prove actual mediation (Fiedler et al., 2011) \nand should, rather, be seen as a tentative hint for \nmediation.\nGeneral Discussion\nA central argument against preprints is that nonscientists \nmight fail to differentiate them from the peer-reviewed \nliterature (Fox, 2018; Vazire, 2020). Indeed, nonscientists \nfrom Germany (Study 1) and the United States (Study 2) \nperceived research findings published as preprints as \nequally credible as research findings published in peer-\nreviewed journals. However, a brief explanation of the \npeer-review process combined with the information that \npreprints are not peer reviewed led nonscientists to per -\nceive identical research findings published as preprints \nas less credible than the peer-reviewed literature. This \neffect was observed for research findings in general \n(Study 3) and specific psychological research findings \n(Studies 4 and 5). Study 5 further suggested that even a \nvery brief explanation, which could be added to all preprints, allowed nonscientists to differentiate them \nfrom the peer-reviewed literature. Note that this effect \nemerged independently of whether this explanation was \nallegedly provided by the preprints’ authors or by  \nan external source, albeit the effect was descriptively \nsmaller in the former situation. The explanation seemed \nto be especially effective for individuals who are rather \nfamiliar with the scientific publication system, and it \nseems to work by influencing whether nonscientists see \npreprints as quality controlled and as adhering to pub -\nlication standards. In other words, when nonscientists \nare well informed about the source of information, they \ncan adjust their credibility ratings accordingly.\nIn practice, however, most psychological preprints do \nnot contain such an explanation. The pilot study, in which \nwe coded recent preprints from two popular psychologi -\ncal preprint servers, revealed that less than 30% of pre -\nprints contained information that they are a preprint. Even \nfewer mentioned that they are not peer reviewed, and \nvirtually none provided an explanation similar to the one \nused in our studies. Taking this current status quo into \naccount, our findings suggest that nonscientists might Table 4.  Multiple Linear Regression Predicting Perceived Credibility From \nCondition, Centered Familiarity With the Publication Process, and Their Interaction \nTerm\nPredictor B SE B t(539) p\nCondition (0 = peer review, 1 = explanation) –0.22 0.09 2.46 .014\nFamiliarity (centered)  0.23 0.04 5.85 < .001\nCondition × Familiarity (Centered) –0.12 0.05 2.49 .013\nQuality Control\nExplanation of\nPreprints\nStandards.09∗ (−.15 ∗∗)\n.33∗∗∗.43∗∗∗ −.27∗∗∗\n−.37∗∗∗Credibility\nFig. 7.  Parallel mediation analyses involving perceived quality control and adherence to \npublication standards as dual, simultaneous mediators for the link between explanations \n(0 = peer-review condition, 1 = merged-explanation conditions) and perceived credibility. \nValues represent standardized path coefficients. The total effect is presented in parentheses. \nAsterisks indicate significance at the p < .05 level ( *), at the p < .01 level ( **), and at the \np < .001 level ( ***)."
    },
    {
      "section": "Page 12",
      "page_number": 12,
      "text": "12 Wingen et al.\ncurrently be unable to differentiate between preprints and \nthe peer-reviewed literature.\nSome scholars (e.g., Elmore, 2018) have pointed to \nthe fact that the term “preprint” is a misnomer because \nthere may never be a future print version in a scientific \njournal (e.g., if the preprint does not pass the peer-\nreview process). Nonscientists might, however, believe \nthat preprints are in fact earlier versions of already pub -\nlished and peer-reviewed articles. This discrepancy \ncould explain why Study 5 found that simply stating that \npreprints have not yet passed peer review—something \nthat many individuals are probably not aware of—\nalready reduced perceived credibility. The same study, \nhowever, also demonstrated that a more detailed expla -\nnation led to a stronger differentiation between preprints \nand peer-reviewed literature regarding their perceived \nquality control and their perceived adherence to publica -\ntion standards, which were relevant mediators. We thus \nrecommend that future authors of preprints, but also \npreprint servers or science journalists covering preprints, \nshould briefly explain the peer-review process and high -\nlight that preprints are not peer reviewed. Our research \nsuggests that such an explanation might be especially \neffective if it includes elements that indicate that peer \nreview serves as a quality-control process and that it is \nthe standard procedure for scientific publication.\nOne important discussion point, however, is whether \nit is desirable that nonscientists differentiate between \npreprints and peer-reviewed literature in terms of cred -\nibility. Although the peer-review system leads to improve -\nments of a manuscript (Carneiro et al., 2019; Godlee \net al., 1998; Goodman et al., 1994; Schroter et al., 2004), \nit also has serious drawbacks (Heesen & Bright, 2021; \nHuisman & Smits, 2017; Jukola, 2017), and one might \nargue that preprints are not necessarily less credible than \npeer-reviewed articles. Regardless of whether peer-\nreviewed articles are objectively more credible, we find \nthat if provided with information about the differences \nbetween preprints and peer-reviewed articles, partici -\npants used this information to inform their credibility \njudgments. We, therefore, argue that this information \nshould not be withheld. In contrast to more patronizing \nstatements, such as the statement by BioRxiv (preprints \n“should not be regarded as conclusive, guide clinical \npractice/health-related behavior, or be reported in news \nmedia as established information”), our approach leaves \nit up to the reader to decide whether a preprint is less \ncredible.\nEven if one agrees that preprints are on average less \ncredible than peer-reviewed articles, it could be argued \nthat it is not desirable to reduce the perceived credibil -\nity of all preprints because some preprints may in fact \nbe highly credible. However, because psychological \nresearch findings are often nonreplicable (Open Science \nCollaboration, 2015) and context sensitive (Van Bavel \net al., 2016), we argue that it is better to err on the side of caution by increasing nonscientists’ vigilance toward \npreprints even if this may not always be necessary. This \ndoes not imply, of course, that nonscientists should rely \nsolely on whether a manuscript is a preprint when evalu -\nating its content. In fact, recent work suggests that non -\nscientists are also sensitive to other important aspects \nof scientific research, such as the strength of evidence \n(Hoogeveen et al., 2020) or successful replications of \nthe presented work (Hendriks et al., 2020).\nIt is also important to discuss the generality of our \nfindings (Simons et al., 2017). First, because we repli -\ncated our findings in rather different samples (U.S. \nMTurk users and German students), we expect our find -\nings to replicate also in more representative samples for \nthese and other Western countries. Note, however, we \nfound that our explanation was more effective for par -\nticipants who reported a high familiarity with the pub -\nlication process. This might explain why we observed \nsubstantially larger effects in Germany: Because the Ger -\nman samples mostly consisted of undergraduate stu -\ndents, they might be more familiar with the publication \nprocess compared with the U.S. samples of Amazon \nMTurk users. Thus, familiarity with the publication pro -\ncess might constrain the generality of our findings. From \nan applied perspective, it seems likely that nonscientists \nseeking out preprints might be rather familiar with the \npublication process (e.g., journalists), which means that \nour explanation would be rather effective in such a situ -\nation. However, it is also possible that this is a method -\nological artifact: Participants who read our materials \nmore closely might consequently report a higher famil -\niarity with the publication process and being more \nstrongly affected by the manipulation.\nMoreover, it seems likely that the effectiveness of our \nexplanation depends on participants’ general trust in \nscience because our explanation highlights that preprints \ndo not follow the established scientific publication pro -\ncedure. If, however, participants’ trust in the established \nscientific knowledge is generally low, a deviation from \nestablished standards might not reduce trust but could \neven increase it. This could, for example, be the case \nfor politically highly conservative participants, who are \ncontemporarily characterized by relatively low trust in \nscience (Gauchat, 2012).\nFinally, it would also be vital to test whether our find -\nings generalize to other forms of non-peer-reviewed sci -\nence communication, such as blogs, podcasts, or popular \nscience magazines. For example, during the COVID-19 \ncrisis, some scientists shared their findings through non-\npeer-reviewed podcasts and even press conferences \n(Kupferschmidt, 2020). In such a situation, it might also \nbe desirable to inform the public that the presented \nresearch findings have not been peer reviewed to avoid \npublic overconfidence in the presented research. It, \nhowever, remains possible that the public already per -\nceives such publication formats as rather uncommon and"
    },
    {
      "section": "Page 13",
      "page_number": 13,
      "text": "Caution, Preprint!  13\nthus less credible, which would leave no room for such \nan explanation to have an additional effect. This remains \nan interesting question for future research.\nIn sum, our work suggests that concerns about non -\nscientists not differentiating between preprints and peer-\nreviewed psychological literature are legitimate. \nHowever, we also suggest and test a solution: Preprint \nauthors, preprint servers, and other relevant institutions \ncan likely mitigate this problem by briefly explaining the \nconcept of preprints and their lack of peer review. This \nwould allow harvesting the benefits of preprints, such \nas faster and more accessible science communication, \nwhile reducing concerns about public overconfidence \nin the presented findings.\nTransparency\nAction Editor:  Alexa Tullett\nEditor:  Daniel J. Simons\nAuthor Contributions\nT. Wingen generated the idea for the research project, with \nfeedback from J. B. Berkessel and S. Dohle. T. Wingen and \nJ. B. Berkessel jointly programmed the study and collected \nthe data. T. Wingen wrote the analysis code and analyzed \nthe data, and J. B. Berkessel verified the accuracy of those \nanalyses. T. Wingen wrote the first draft of the manuscript, \nand all authors critically edited it. All of the authors \napproved the final manuscript for submission.\nDeclaration of Conflicting Interests\nThe author(s) declare that there were no conflicts of interest \nwith respect to the authorship or the publication of this \narticle. \nFunding\nThe research was partly funded by a DFG grant (Deutsche \nForschungsgemeinschaft, DO 1900/3-1) awarded to S. Dohle.\nOpen Practices\nOpen Data: https://osf.io/r3p6t/\nOpen Materials: https://osf.io/r3p6t/\nPreregistration: https://osf.io/r3p6t/\nAll data have been made publicly available via OSF and \ncan be accessed at https://osf.io/r3p6t/. All materials have \nbeen made publicly available via OSF and can be accessed \nat https://osf.io/r3p6t/. The analysis plan was preregistered \nat OSF prior to data collection and can be accessed at \nhttps://osf.io/r3p6t/. This article has received badges for\nOpen Data, Open Materials, and Preregistration. More infor -\nmation about the Open Practices badges can be found at \nhttp://www.psychologicalscience.org/publications/badges.\nORCID iD\nTobias Wingen  https://orcid.org/0000-0002-1559-859X\nAcknowledgments\nWe thank Nicolas Alef and Antonia Dörnemann for their valu -\nable practical support. We further thank Paul Davies and Luzie \nU. Wingen for their extensive feedback on materials. We finally thank Angela Dorrough and Jan Landwehr for valuable com -\nments on an earlier version of this manuscript. The submitted \nmanuscript was previously posted on a preprint archive, \ndoi:10.31219/osf.io/7n3mj.\nSupplemental Material\nAdditional supporting information can be found at http://jour  \nnals.sagepub.com/doi/suppl/10.1177/25152459211070559\nReferences\nAdam, D. (2019, March 12). Does a ‘Dark Triad’ of personality \ntraits make you more successful? Science|AAAS . https://\nwww.sciencemag.org/news/2019/03/does-dark-triad-per  \nsonality-traits-make-you-more-successful\nAdena, M., Alizade, J., Bohner, F., Harke, J., & Mesters, F. \n(2019). Quality certification for nonprofits, charitable giv -\ning, and donor’s trust: Experimental evidence. Journal of \nEconomic Behavior & Organization , 159, 75–100. https://\ndoi.org/10.1016/j.jebo.2019.01.007\nAnvari, F., & Lakens, D. (2018). The replicability crisis and pub -\nlic trust in psychological science. Comprehensive Results in \nSocial Psychology , 3(3), 266–286. https://doi.org/10/ghzsdc\nBachmann, R., & Inkpen, A. C. (2011). Understanding insti -\ntutional-based trust building processes in inter-organiza -\ntional relationships. Organization Studies , 32(2), 281–301. \nhttps://doi.org/10/bp6q87\nBlanchard, A. L., Welbourne, J. L., & Boughton, M. D. (2011). \nA model of online trust: The mediating role of norms and \nsense of virtual community. Information, Communication \n& Society , 14(1), 76–106. https://doi.org/10/db32d9\nBoiral, O. (2012). ISO 9000 and organizational effectiveness: \nA systematic review. Quality Management Journal , 19(3), \n16–37. https://doi.org/10/gk9pn7\nBullock, J. G., Green, D. P., & Ha, S. E. (2010). Yes, but what’s \nthe mechanism? (Don’t expect an easy answer). Journal of \nPersonality and Social Psychology , 98(4), 550–558. https://\ndoi.org/10.1037/a0018933\nCarneiro, C. F., Queiroz, V. G., Moulin, T. C., Carvalho, C. A.,  \nHaas, C. B., Rayêe, D., Henshall, D. E., De-Souza, E. A.,  \nEspinelli, F., & Boos, F. Z. (2019). Comparing qual -\nity of reporting between preprints and peer-reviewed \narticles in the biomedical literature . bioRxiv. https://doi  \n.org/10.1101/581892\nChampely, S., Ekstrom, C., Dalgaard, P., Gill, J., Weibelzahl, S.,  \nAnandkumar, A., Ford, C., Volcic, R., & De Rosario, H. \n(2018). Package ‘pwr’  (R package version 1.3-0) [Computer \nsoftware]. https://cran.r-project.org/web/packages/pwr/\npwr.pdf\nChandler, J., Paolacci, G., Peer, E., Mueller, P., & Ratliff, K. A.  \n(2015). Using nonnaive participants can reduce effect \nsizes. Psychological Science , 26(7), 1131–1139. https://doi  \n.org/10/f7ktxj\nChivers, T. (2020). Geek tip: If it doesn’t say “Registered \nReport,” don’t trust it - The Post. UnHerd . https://unherd  \n.com/thepost/geek-tip-if-it-doesnt-say-registered-report-\ndont-trust-it/\nChmielewski, M., & Kucker, S. C. (2020). An MTurk crisis? \nShifts in data quality and the impact on study results. Social \nPsychological and Personality Science , 11(4), 464–473. \nhttps://doi.org/10/gf92b6"
    },
    {
      "section": "Page 14",
      "page_number": 14,
      "text": "14 Wingen et al.\nChow, D. (2021, September 14). More Europeans are taking \nclimate change seriously. In the U.S., not so much. NBC \nNews. https://www.nbcnews.com/science/environment/\neuropeans-are-taking-climate-change-seriously-us-not-\nmuch-rcna1990\nCobo, E., Cortés, J., Ribera, J. M., Cardellach, F., Selva-\nO’Callaghan, A., Kostov, B., García, L., Cirugeda, L., \nAltman, D. G., González, J. A., Sànchez, J. A., Miras, F., \nUrrutia, A., Fonollosa, V., Rey-Joly, C., & Vilardell, M. \n(2011). Effect of using reporting guidelines during peer \nreview on quality of final manuscripts submitted to a bio -\nmedical journal: Masked randomised trial. BMJ, 343, Article \nd6783. https://doi.org/10.1136/bmj.d6783\nCooke, S. J., Nguyen, V. M., Wilson, A. D. M., Donaldson, \nM. R., Gallagher, A. J., Hammerschlag, N., & Haddaway, \nN. R. (2016). The need for speed in a crisis discipline: \nPerspectives on peer-review duration and implications for \nconservation science. Endangered Species Research , 30, \n11–18. https://doi.org/10/f8vtbf\nDohle, S., Wingen, T., & Schreiber, M. (2020). Acceptance \nand adoption of protective measures during the COVID-\n19 pandemic: The role of trust in politics and trust in sci -\nence. Social Psychological Bulletin , 15(4), 1–23. https://\ndoi.org/10.32872/spb.4315\nEisenhart, M. (2002). The paradox of peer review: Admitting \ntoo much or allowing too little? Research in Science \nEducation , 32(2), 241–255. https://doi.org/10/fm3v6x\nElmore, S. A. (2018). Preprints: What role do these have in \ncommunicating scientific results?  SAGE.\nElson, M., Huff, M., & Utz, S. (2020). Metascience on peer \nreview: Testing the effects of a study’s originality and \nstatistical significance in a field experiment. Advances in \nMethods and Practices in Psychological Science , 3(1), 53–\n65. https://doi.org/10.1177/2515245919895419\nFiedler, K., Schott, M., & Meiser, T. (2011). What mediation \nanalysis can (not) do. Journal of Experimental Social \nPsychology , 47(6), 1231–1236. https://doi.org/10.1016/j  \n.jesp.2011.05.007\nForster, V. (2020, February 2). No, the Coronavirus was not \ngenetically engineered to put pieces of HIV in it. Forbes . \nhttps://www.forbes.com/sites/victoriaforster/2020/02/02/\nno-coronavirus-was-not-bioengineered-to-put-pieces-of-\nhiv-in-it/\nFox, F. (2018). The preprint dilemma: Good for science, bad \nfor the public?  A discussion paper for the scientific com -\nmunity. Science Media Centre. https://www.sciencemedia  \ncentre.org/the-preprint-dilemma-good-for-science-bad-\nfor-the-public-a-discussion-paper-for-the-scientific-  \ncommunity/\nFraser, N., Brierley, L., Dey, G., Polka, J. K., Pálfy, M., Nanni, F.,  \n& Coates, J. A. (2021). The evolving role of preprints in \nthe dissemination of COVID-19 research and their impact \non the science communication landscape. PLOS Biology , \n19(4), Article e3000959. https://doi.org/10.1371/journal  \n.pbio.3000959\nFrench, C. (2012, March 15). Precognition studies and the curse \nof the failed replications. The Guardian . https://www.the  \nguardian.com/science/2012/mar/15/precognition-studies-\ncurse-failed-replications\nGauchat, G. (2012). Politicization of science in the public \nsphere: A study of public trust in the United States, 1974 to 2010. American Sociological Review , 77(2), 167–187. \nhttps://doi.org/10/gc3d6j\nGerman Psychological Society. (2018). Ethisches Handeln \nin der Psychologischen Forschung. Empfehlungen der \nDeutschen Gesellschaft für Psychologie für Forschende und \nEthikkommissionen  [Ethical Guidelines for Psychological \nResearch]. Hogrefe Verlag.\nGervais, W. M., & Norenzayan, A. (2012). Analytic thinking \npromotes religious disbelief. Science , 336(6080), 493–496. \nhttps://doi.org/10/r2x\nGodlee, F., Gale, C. R., & Martyn, C. N. (1998). Effect on the \nquality of peer review of blinding reviewers and asking \nthem to sign their reports: A randomized controlled trial. \nJAMA , 280(3), 237–240. https://doi.org/10/b5vnc9\nGoodman, S. N., Berlin, J., Fletcher, S. W., & Fletcher, R. H. \n(1994). Manuscript quality before and after peer review and \nediting at Annals of Internal Medicine. Annals of Internal \nMedicine , 121(1), 11–21. https://doi.org/10/gk9pph\nHauser, O. P., Rand, D. G., Peysakhovich, A., & Nowak, M. A. \n(2014). Cooperating with the future. Nature , 511(7508), \n220–223. https://doi.org/10/f59dm3\nHeesen, R., & Bright, L. K. (2021). Is peer review a good \nidea? British Journal for the Philosophy of Science , 72(3), \n635–663. https://doi.org/10/ggkwvx\nHeimstädt, M. (2020). Between fast science and fake news: \nPreprint servers are political. Impact of Social Sciences . \nhttps://blogs.lse.ac.uk/impactofsocialsciences/2020/04/03/\nbetween-fast-science-and-fake-news-preprint-servers-are-\npolitical/\nHelmer, M., Schottdorf, M., Neef, A., & Battaglia, D. (2017). \nGender bias in scholarly peer review. Elife, 6, Article \ne21718. https://doi.org/10.7554/eLife.21718\nHendriks, F., Kienhues, D., & Bromme, R. (2020). Replication \ncrisis = trust crisis? The effect of successful vs failed rep -\nlications on laypeople’s trust in researchers and research. \nPublic Understanding of Science , 29(3), 270–288. https://\ndoi.org/10.1177/0963662520902383\nHoogeveen, S., Sarafoglou, A., & Wagenmakers, E-J. (2020). \nLaypeople can predict which social-science studies will be \nreplicated successfully. Advances in Methods and Practices \nin Psychological Science , 3(3), 267–285. https://doi.org/10/\ngg877f\nHuisman, J., & Smits, J. (2017). Duration and quality of the peer \nreview process: The author’s perspective. Scientometrics , \n113(1), 633–650. https://doi.org/10/gb3b8s\nImhoff, R., & Lamberty, P. (2020). A bioweapon or a hoax? \nThe link between distinct conspiracy beliefs about the \nCoronavirus disease (COVID-19) outbreak and pandemic \nbehavior. Social Psychological and Personality Science , \n11(8), 1110–1118. https://doi.org/10/gg4cq5\nJukola, S. (2017). A social epistemological inquiry into biases \nin journal peer review. Perspectives on Science , 25(1), \n124–148. https://doi.org/10/gk9ppm\nKoch, T., Frischlich, L., & Lermer, E. (2021). The effects of \nwarning labels and social endorsement cues on cred -\nibility perceptions of and engagement intentions with  \nfake news . PsyArXiv. https://doi.org/10.31234/osf.io/fw3zq\nKoerber, A. (2021). Is it fake news or is it open science? Science \ncommunication in the COVID-19 pandemic. Journal of \nBusiness and Technical Communication , 35(1), 22–27. \nhttps://doi.org/10/ghcwmd"
    },
    {
      "section": "Page 15",
      "page_number": 15,
      "text": "Caution, Preprint!  15\nKupferschmidt, K. (2020, April 28). How the pandemic made \nthis virologist an unlikely cult figure. Science|AAAS . https://\nwww.sciencemag.org/news/2020/04/how-pandemic-  \nmade-virologist-unlikely-cult-figure\nKwon, D. (2020). How swamped preprint servers are block -\ning bad coronavirus research. Nature , 581(7807), 130–131. \nhttps://doi.org/10/ggvwt6\nLakens, D. (2017). Equivalence tests: A practical primer for t \ntests, correlations, and meta-analyses. Social Psychological \nand Personality Science , 8(4), 355–362. https://doi.org/10/\ngbf8nt\nLakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence \ntesting for psychological research: A tutorial. Advances \nin Methods and Practices in Psychological Science , 1(2), \n259–269. https://doi.org/10/gdj7s9\nLindner, J. (2020, May 28). Warum über die Drosten-Studie \ngestritten wird? [Why is the Drosten-study debated?]. ZDF.\nde. https://www.zdf.de/uri/606f8734-2eca-42fe-b811-\n73b5a6860710\nNiggemeier, S. (2020, May 26). Die Machtprobe: Worum es \nbeim Kampf von “Bild” gegen Drosten geht [A test of \npower: What is the fight between “Bild” and Drosten \nall about]. Übermedien . https://uebermedien.de/49542/\ndie-machtprobe-worum-es-beim-kampf-von-bild-gegen-  \ndrosten-geht/\nNishi, A., Shirado, H., Rand, D. G., & Christakis, N. A. (2015). \nInequality and visibility of wealth in experimental social \nnetworks. Nature , 526(7573), 426–429. https://doi.org/10  \n.1038/nature15392\nNosek, B. A., & Bar-Anan, Y. (2012). Scientific utopia: I. \nOpening scientific communication. Psychological Inquiry , \n23(3), 217–243. https://doi.org/10/gcsk27\nOkike, K., Hug, K. T., Kocher, M. S., & Leopold, S. S. (2016). \nSingle-blind vs double-blind peer review in the setting of \nauthor prestige. JAMA , 316(12), 1315–1316. https://doi.org/  \n10/gh7f7z\nOlson, C. M., Rennie, D., Cook, D., Dickersin, K., Flanagin, \nA., Hogan, J. W., Zhu, Q., Reiling, J., & Pace, B. (2002). \nPublication bias in editorial decision making. JAMA , \n287(21), 2825–2828. https://doi.org/10/b26cf3\nOpen Science Collaboration. (2015). Estimating the reproduc -\nibility of psychological science. Science , 349(6251), Article \naac4716. https://doi.org/10.1126/science.aac4716\nPhillips, N. (2017). Yarrr: A companion to the e-book “yarrr!: \nThe pirate’s guide to r”  (R package version 0.1.5) [Computer \nsoftware]. https://CRAN.R-project.org/package=yarrr\nPolka, J. K., Dey, G., Pálfy, M., Nanni, F., Brierley, L., Fraser, \nN., & Coates, J. A. (2021). Preprints in motion: Tracking \nchanges between posting and journal publication . bioRxiv. \nhttps://doi.org/10/gh5mhm\nRahal, R-M., & Heycke, T. (2020). Hoarding in science, no \nthanks. Openness and transparency in crisis mode and \nbeyond . OSF Preprints. https://doi.org/10.31222/osf.io/\nakd6c\nR Development Core Team. (2021). R: A language and environ -\nment for statistical computing . https://www.R-project.org/\nResnick, P., Zeckhauser, R., Swanson, J., & Lockwood, K. \n(2006). The value of reputation on eBay: A controlled \nexperiment. Experimental Economics , 9(2), 79–101. https://\ndoi.org/10/c3qhkbRevelle, W. R. (2021). psych: Procedures for personality and \npsychological research  (R package version 2.1.6) [Computer \nsoftware]. https://CRAN.R-project.org/package=psych\nRosseel, Y. (2012). Lavaan: An R package for structural equa -\ntion modeling and more. Version 0.5–12 (BETA). Journal \nof Statistical Software , 48, 1–36. https://doi.org/10.18637/\njss.v048.i02\nSchroter, S., Black, N., Evans, S., Carpenter, J., Godlee, F., \n& Smith, R. (2004). Effects of training on quality of peer \nreview: Randomised controlled trial. BMJ, 328(7441), \nArticle 673. https://doi.org/10/d52tpr\nShah, A. K., Mullainathan, S., & Shafir, E. (2012). Some conse -\nquences of having too little. Science , 338(6107), 682–685. \nhttps://doi.org/10/f4ccsq\nSheldon, T. (2018). Preprints could promote confusion and \ndistortion. Nature , 559(7714), 445–446. https://doi.org/10  \n.1038/d41586-018-05789-4\nSilva, R. R., & Topolinski, S. (2018). My username is IN! The \ninfluence of inward vs. outward wandering usernames \non judgments of online seller trustworthiness. Psychology \n& Marketing , 35(4), 307–319. https://doi.org/10/gk9pp3\nSimons, D. J., Shoda, Y., & Lindsay, D. S. (2017). Constraints \non generality (COG): A proposed addition to all empiri -\ncal papers. Perspectives on Psychological Science , 12(6), \n1123–1128. https://doi.org/10/gcmvgf\nSmith, R. (2006). Peer review: A flawed process at the heart \nof science and journals. Journal of the Royal Society of \nMedicine , 99(4), 178–182.\nSoderberg, C. K., Errington, T. M., & Nosek, B. A. (2020). \nCredibility of preprints: An interdisciplinary survey of \nresearchers. Royal Society Open Science , 7(10), Article \n201520. https://doi.org/10.1098/rsos.201520\nTenopir, C., Levine, K., Allard, S., Christian, L., Volentine, R.,  \nBoehm, R., Nichols, F., Nicholas, D., Jamali, H. R., & \nHerman, E. (2016). Trustworthiness and authority of scholarly \ninformation in a digital age: Results of an international ques -\ntionnaire. Journal of the Association for Information Science \nand Technology , 67(10), 2344–2361. https://doi.org/10/gfs4nn\nTorchiano, M. (2020). effsize: Efficient effect size computation  \n(R package version 0.8.1) [Computer software]. https://doi  \n.org/10.5281/zenodo.1480624\nVan Bavel, J. J., Mende-Siedlecki, P., Brady, W. J., & Reinero, \nD. A. (2016). Contextual sensitivity in scientific reproduc -\nibility. Proceedings of the National Academy of Sciences, \nUSA, 113(23), 6454–6459. https://doi.org/10/f8qqzh\nVazire, S. (2020, June 25). Peer-reviewed scientific journals don’t \nreally do their job. Wired . https://www.wired.com/story/\npeer-reviewed-scientific-journals-dont-really-do-their-job/\nWenegrat, B., Castillo-Yee, E., & Abrams, L. (1996). Social norm \ncompliance as a signaling system. II. Studies of fitness-\nrelated attributions consequent on a group norm violation. \nEthology and Sociobiology , 17(6), 417–429.\nWilson, T. D., Reinhard, D. A., Westgate, E. C., Gilbert, D. T., \nEllerbeck, N., Hahn, C., Brown, C. L., & Shaked, A. (2014). \nJust think: The challenges of the disengaged mind. Science , \n345(6192), 75–77. https://doi.org/10/th9\nWingen, T., Berkessel, J. B., & Englich, B. (2020). No replica -\ntion, no trust? How low replicability influences trust in \npsychology. Social Psychological and Personality Science , \n11(4), 454–463. https://doi.org/10.1177/1948550619877412"
    }
  ]
}