{
  "doc_type": "scientific paper",
  "title": "Sound Quality Factors Inducing the Autonomous Sensory Meridian Response",
  "authors": [
    "Ryota Shimokura"
  ],
  "year": 2022,
  "journal": "Audiology Research",
  "doi": "10.3390/audiolres12050056",
  "abstract": "The acoustical characteristics of auditory triggers often recommended to generate the autonomous sensory meridian response (ASMR) on Internet platforms were investigated by parameterizing their sound qualities following Zwicker’s procedure and calculating autocorrelation (ACF)/interaural cross-correlation (IACF) functions. For 20 triggers (10 human- and 10 nature-generated sounds), scores (on a five-point Likert scale) of the ASMR, perceived loudness, perceived pitch, comfort, and perceived closeness to the sound image were obtained for 26 participants by questionnaire. The results show that the human-generated sounds were more likely to trigger stronger ASMR than nature-generated sounds, and the primary psychological aspect relating to the ASMR was the perceived closeness, with the triggers perceived more closely to a listener having higher ASMR scores. The perceived closeness was evaluated by the loudness and roughness (among Zwicker’s parameter) for the nature-generated sounds and the interaural cross-correlation coefficient (IACC) (among ACF/IACF parameters) for the human-generated sounds. The nature-generated sounds with higher loudness and roughness and the human-generated sounds with lower IACC were likely to evoke the ASMR sensation.",
  "keywords": [
    "autonomous sensory meridian response",
    "loudness",
    "roughness",
    "interaural cross-correlation coefficient"
  ],
  "research_topics": [
    "Autonomous Sensory Meridian Response (ASMR)",
    "Sound Quality Analysis",
    "Acoustical Characteristics",
    "Zwicker’s Sound Quality Parameters",
    "Autocorrelation Function (ACF)",
    "Interaural Cross-Correlation Function (IACF)",
    "Psychoacoustics",
    "Auditory Perception",
    "Human-Generated Sounds",
    "Nature-Generated Sounds"
  ],
  "created_at": "2025-05-05T02:40:00.874721Z",
  "source_pdf": "documents/research/Global/Shimokura 2022 Sound Quality Factors Inducing the Autonomous Sensory Meridian Response.pdf",
  "sections": [
    {
      "section": "Page 1",
      "page_number": 1,
      "text": "Citation: Shimokura, R. Sound\nQuality Factors Inducing the\nAutonomous Sensory Meridian\nResponse. Audiol. Res. 2022 ,12,\n574–584. https://doi.org/10.3390/\naudiolres12050056\nAcademic Editor: Agnieszka\nSzczepek\nReceived: 25 August 2022\nAccepted: 12 October 2022\nPublished: 13 October 2022\nPublisher’s Note: MDPI stays neutral\nwith regard to jurisdictional claims in\npublished maps and institutional afﬁl-\niations.\nCopyright: © 2022 by the author.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nArticle\nSound Quality Factors Inducing the Autonomous Sensory\nMeridian Response\nRyota Shimokura\nGraduate School of Engineering Science, Osaka University, Room D436, 1-3 Machikaneyama,\nToyonaka 560-8531, Japan; rshimo@sys.es.osaka-u.ac.jp; Tel./Fax: +81-6-6850-6376\nAbstract: The acoustical characteristics of auditory triggers often recommended to generate the\nautonomous sensory meridian response (ASMR) on Internet platforms were investigated by pa-\nrameterizing their sound qualities following Zwicker’s procedure and calculating autocorrelation\n(ACF)/interaural cross-correlation (IACF) functions. For 20 triggers (10 human- and 10 nature-\ngenerated sounds), scores (on a ﬁve-point Likert scale) of the ASMR, perceived loudness, perceived\npitch, comfort, and perceived closeness to the sound image were obtained for 26 participants by ques-\ntionnaire. The results show that the human-generated sounds were more likely to trigger stronger\nASMR than nature-generated sounds, and the primary psychological aspect relating to the ASMR was\nthe perceived closeness, with the triggers perceived more closely to a listener having higher ASMR\nscores. The perceived closeness was evaluated by the loudness and roughness (among Zwicker’s\nparameter) for the nature-generated sounds and the interaural cross-correlation coefﬁcient (IACC)\n(among ACF/IACF parameters) for the human-generated sounds. The nature-generated sounds with\nhigher loudness and roughness and the human-generated sounds with lower IACC were likely to\nevoke the ASMR sensation.\nKeywords: autonomous sensory meridian response; loudness; roughness; interaural cross-correlation\ncoefﬁcient\n1. Introduction\nThe autonomous sensory meridian response (ASMR) is an atypical sensory phe-\nnomenon in which individuals experience a tingling, static sensation across the scalp and\nback of the neck in response to speciﬁc triggering audio and visual stimuli or to light\ntouch [ 1]. This sensation is widely reported to promote relaxation, wellbeing, and sleep,\nand there are many ASMR-related channels on YouTube. Some researchers have examined\nthe relationship between the ASMR and misophonia [ 2–4]. Misophonia is an auditory disor-\nder of decreased tolerance to speciﬁc sounds or their associated stimuli such as oral sounds\n(e.g., loud breathing, chewing, swallowing), clicking sounds (e.g., keyboard tapping, ﬁnger\ntapping, windshield wipers), and sounds associated with movement (e.g., ﬁdgeting) [ 5–8].\nThe ASMR triggers produce positive emotions associated with an increase of wellbeing,\nwhile the misophonia triggers produce negative emotions associated with ﬁght-or-ﬂight\nresponses. Although the displayed emotions are opposite, both are caused commonly by\nhypersensitivities to sound triggers, and it is possible that the acoustical characteristics of\nthe ASMR triggers may explain the occurrence mechanism of the misophonia. Actually, a\nprevious study reported that people who experienced the ASMR were more likely to have\na risk of misophonia [2].\nSeveral common audio and visual stimuli (triggers) that induce the ASMR are known,\nand an online ASMR experience questionnaire completed by 475 individuals identiﬁed\nthe trigger types as whispering (75%), personal attention (69%), crisp sounds (64%), and\nslow movements (53% participants reporting the ASMR experience) [ 1]. Following this\nquestionnaire, many studies on the ASMR have empirically selected such highly possible\nAudiol. Res. 2022 ,12, 574–584. https://doi.org/10.3390/audiolres12050056 https://www.mdpi.com/journal/audiolres"
    },
    {
      "section": "Page 2",
      "page_number": 2,
      "text": "Audiol. Res. 2022 ,12 575\ntriggers [ 9–13]. However, it is not clear which physical characteristics of these triggers\ninduce the ASMR.\nIn the case of audio signals, numerical models have been proposed to deﬁne the\nsound quality. Perceptual characteristics of the hearing of sound are the loudness, pitch,\nand timbre, and the sound quality is expressed generally by numerical algorithms based\non varying sound pressure. As an example, Zwicker’s parameters (loudness, sharpness,\nroughness, and ﬂuctuation strength) have been used to evaluate the sound quality of\nenvironmental noise [ 14]. The loudness is the psychological sound intensity, and it is\ncalculated by transforming the frequency onto the Bark scale, considering the effects of\nfrequency and temporal masking, and counting the area of the loudness pattern [ 15]. The\nloudness of a pure tone with a frequency of 1 kHz and sound pressure level of 40 dB is\ndeﬁned as being 1 sone. The sharpness is a measure of the sound acuity and high-frequency\ncomponent, and is obtained by adding a weight function to its speciﬁc loudness [ 16]. The\nsharpness of a noise at 60 dB in a critical band at 1 kHz is deﬁned as being 1 acum. The\nroughness is a fundamental hearing sensation caused by sound with rapid amplitude\nmodulation (15–300 Hz) and is quantiﬁed on the basis of the modulation frequency and\ndepth of the time-varying loudness [ 16]. The roughness of a 1 kHz tone at 60 dB with a\n100% amplitude modulation (modulation depth of 1) at 70 Hz is deﬁned as being 1 asper.\nThe ﬂuctuation strength is similar in principle to roughness except that it quantiﬁes the\nsubjective perception of the slower (up to 20 Hz) amplitude modulation of a sound, and it is\ncalculated from the modulation frequency and depth of the time-varying loudness [ 16]. The\nﬂuctuation strength produced by a 1 kHz tone at 60 dB with a 100% amplitude modulated\nat 4 Hz is deﬁned as being 1 vacil.\nThe other procedure for evaluating sound quality is using the autocorrelation and\ninteraural cross-correlation functions (ACF and IACF) frequently used for music and\nacoustics in concert halls [ 17]. Our auditory perceptions are deeply related to the timing of\nnerve ﬁrings caused by binaurally detected sounds, and the ACF and IACF are modeled\nin the processors of the auditory nerve [ 18,19]. Three parameters can be calculated from\nACF analyses of monoaurally recorded sound: (1) the delay time of the maximum peak\n(\u001c1), (2) the amplitude of the ﬁrst maximum peak ( \u001e1) and (3) the width of the peak at the\noriginal time [W F(0)] (see Section 2.2 for details). The fundamental frequency (1/ \u001c1Hz)\nand the pitch strength of the sound are \u001c1and\u001e1, respectively. The spectral centroid of the\noriginal signal is W F(0), with longer and shorter values, respectively, corresponding to lower\nand higher centroid values of spectral energy signals. These ACF parameters explain not\nonly the musical motif suitable for a speciﬁc concert hall [ 17] but also annoyance induced\nby noise [ 20,21] and speech intelligibility [ 22,23]. From the IACF analyses of binaurally\nrecorded sound, the interaural cross-correlation coefﬁcient (IACC) can be calculated (see\nSection 2.1 for details). The IACC is the maximum peak amplitude of the IACF whose\ndelay time is within \u00061 ms. The IACC is related to the subjective sound diffuseness, which\nmeans that a higher IACC corresponds to the listener perceiving a well-deﬁned direction of\nthe incoming sound, whereas a lower IACC corresponds to a well-diffused sound. Such\nACF and IACF parameters have also been used for the evaluation of several types of\nnoise [24–27].\nThe present study identiﬁed physical factors that induce the auditory-based ASMR\nsensation using the four Zwicker parameters and four ACF/IACF parameters. We prepared\na total of 20 sound motifs likely to induce the ASMR and calculated the eight sound quality\nparameters. To conﬁrm the occurrence of the ASMR, previous studies have adopted\nphysiological (e.g., functional magnetic resonance imaging or heat rate) [ 11,28,29] and\npsychological (e.g., questionaries) [ 1,9,10,12,13] procedures. The present study adopted\nthe psychological approach, with participants quantifying the degree of the perceived\nASMR on a ﬁve-point Likert scale. In addition to the ASMR, the participants scored four\nsubjective sensations (subjective loudness, pitch, comfort, and closeness) at the same time.\nWe examined the correlation of the ASMR scores with the four subjective sensations and\neight sound quality parameters."
    },
    {
      "section": "Page 3",
      "page_number": 3,
      "text": "Audiol. Res. 2022 ,12 576\n2. Method\n2.1. ASMR Triggers and Sound Quality Parameters\nThe 10 auditory ASMR triggers (human-generated sounds) used in the study, and\n10 healing sounds (nature-generated sounds) recorded binaurally were added for the\ncomparison (Table 1). The human- and nature-generated sounds were obtained from several\nwebsites and music distribution sites, respectively. The human-generated sounds were\nrecorded by a dummy head microphone or a binaurally wearing microphone. Although the\nnature-generated sounds do not have information on the recording devices, the participants\nof this study could perceive the sound images close to them with binaural hearing. For the\nsake of expediency, both sounds are called as trigger. The human- and nature-generated\nsounds, respectively, represent sounds generated by human behaviors (e.g., the cutting of\nvegetables and typing at a keyboard) and natural phenomena (e.g., waves and rain). The\ntime length of each trigger was 50 s, and the sound energy was set at the same equivalent\ncontinuous A-weighted sound pressure level ( LAeq) of 45 dBA.\nTable 1 lists the sound quality parameters. The Zwicker parameters were calculated\nusing a Matlab command embedded in Auditory Toolbox [ 30]. The calculation algorithms\nwere based on work in the literature [ 14–16]. The calculations of roughness and ﬂuctuation\nstrength had running steps of 0.5 ms and 2 ms, respectively, along the time length of 50 s,\nand Table 1 lists average values of the time-varying parameters.\nTable 1. Human- and nature-generated sounds and calculated Zwicker’s and ACF/IACF parameters.\nSound Source Zwicker’s Parameters ACF/IACF Parameters\nShort\nTitleContentsLoudness\n[sone]Sharpness\n[acum]Roughness\n[asper]Fluctuation\nStrength\n[vacil]\u001c1\n[ms]\u001e1WF(0)\n[ms]IACC\nHuman-\ngenerated\nsoundCutting Cutting vegetable 6.20 1.63 0.07 1.31 2.52 0.20 0.26 0.58\nFizzwaterStirring carbonated\nwater4.15 3.25 0.06 0.02 0.22 0.29 0.06 0.09\nTyping Typing a keyboard 5.75 2.22 0.10 0.59 0.86 0.15 0.09 0.19\nHeelsFootsteps of high\nheels5.58 1.58 0.05 0.43 1.56 0.19 0.36 0.37\nBook Flipping a book 6.01 1.94 0.07 0.06 1.40 0.13 0.13 0.23\nBrush Brushing something 6.79 1.78 0.07 0.05 1.99 0.15 0.14 0.49\nShampooWashing hair with\nshampoo5.67 2.33 0.08 0.33 1.92 0.04 0.10 0.05\nHair Cutting hair 6.34 2.17 0.01 0.39 0.93 0.42 0.09 0.33\nPen Writing with pen 6.08 2.54 0.01 0.39 0.42 0.29 0.06 0.29\nEarpick Earpick 6.86 1.30 0.11 0.74 6.45 0.05 0.40 0.02\nNature-\ngenerated\nsoundFire Building a ﬁre 7.28 1.88 0.13 0.03 3.32 0.11 0.12 0.86\nBubbleBubbles under\nwater6.23 0.70 0.06 0.07 6.74 0.21 0.77 0.40\nBrook Murmur of a brook 5.43 1.87 0.11 0.07 1.70 0.13 0.15 0.12\nWaves Sound of waves 5.83 1.43 0.05 0.06 3.63 0.05 0.30 0.38\nRain Sound of rain 5.92 2.11 0.06 0.10 3.63 0.05 0.30 0.58\nLava Lava ﬂowing 5.90 2.53 0.15 0.02 0.68 0.09 0.07 0.72\nCricket Bell-ringing cricket 3.78 3.19 0.06 0.02 0.48 0.84 0.07 0.76\nCicada Evening cicada 2.77 2.69 0.02 0.02 0.28 0.95 0.09 0.93\nVolcanoBubbles of mud\nvolcano7.11 1.46 0.12 0.29 1.65 0.15 0.22 0.07\nBambooWind through\nbamboo forest4.98 3.13 0.07 0.06 3.76 0.02 0.06 0.26"
    },
    {
      "section": "Page 4",
      "page_number": 4,
      "text": "Audiol. Res. 2022 ,12 577\nThe ACF parameters were calculated from the normalized ACF:\nfll(t) =fll(t;s,T)=Fll(t;s,T)\nFll(0;s,T), (1)\nwhere\nFll(t;s,T)=1\n2TZs+T\ns\u0000Tpl0(t)pl0(t+t)dt. (2)\nHere, tis the delay time [s], sis the running step [s], 2 Tis the integration interval [s]\nandpl0(t) is the sound in the left channel at time tafter passing through an A-weighted\nnetwork. The ACF parameters were the (1) delay time of the maximum peak ( \u001c1), (2)\namplitude of the ﬁrst maximum peak ( \u001e1) and (3) width of the peak at \u001c= 0 (W F(0)),\ncalculated by doubling the delay time at which the normalized ACF becomes 0.5 times\nthat at the origin of the delay (Figure 1a). Additionally, \u001c1and\u001e1are related to the pitch\n(high or low) and pitch strength (clear or ambiguous) perceived in the periodical part of\nthe sound. The spectral centroid is equivalent to W F(0), and a sound with greater W F(0)is\nthus perceived as having a lower pitch in the noisy part.\nAudiol. Res.  2022 , 12, FOR PEER REVIEW  4 \n \n Table 1. Human- and nature-generated sounds and ca lculated Zwicker’s and ACF/IACF parame-\nters. \nSound Source Zwicke r’s Parameters ACF/IACF Parameters \n Short Title Contents Loud-\nness \n[sone] Sharp-\nness \n[acum] Rough-\nness \n[asper] Fluctua-\ntion \nStrength \n[vacil] τ1 [ms] ϕ1 WΦ(0) \n[ms] IACC \nHu-\nman-\ngener-\nated \nsound Cutting Cutting vegetable 6.20 1.63 0.07 1.31 2.52 0.20 0.26 0.58 \nFizzwater Stirring carbonated water 4.15 3.25 0.06 0.02 0.22 0.29 0.06 0.09 \nTyping Typing a keyboard 5.75 2.22  0.10 0.59 0.86 0.15 0.09 0.19 \nHeels Footsteps of high heels 5.58 1.58 0.05 0.43 1.56 0.19 0.36 0.37 \nBook Flipping a book 6.01 1.94 0.07 0.06 1.40 0.13 0.13 0.23 \nBrush Brushing something 6.79 1.78 0.07 0.05 1.99 0.15 0.14 0.49 \nShampoo Washing hair with shampoo 5.67 2.33 0.08 0.33 1.92 0.04 0.10 0.05 \nHair Cutting hair 6.34 2.17 0.01 0.39 0.93 0.42 0.09 0.33 \nPen Writing with pen 6.08 2.54 0. 01 0.39 0.42 0.29 0.06 0.29 \nEarpick Earpick 6.86 1.30 0.11 0.74 6.45 0.05 0.40 0.02 \nNa-\nture-\ngener-\nated \nsound Fire Building a fire 7.28 1.88 0. 13 0.03 3.32 0.11 0.12 0.86 \nBubble Bubbles under water 6.23 0.70 0.06 0.07 6.74 0.21 0.77 0.40 \nBrook Murmur of a brook 5.43 1.87 0.11 0.07 1.70 0.13 0.15 0.12 \nWaves Sound of waves 5.83 1.43 0.05 0.06 3.63 0.05 0.30 0.38 \nRain Sound of rain 5.92 2.11 0. 06 0.10 3.63 0.05 0.30 0.58 \nLava Lava flowing 5.90 2.53 0.15 0.02 0.68 0.09 0.07 0.72 \nCricket Bell-ringing cricket 3.78 3.19  0.06 0.02 0.48 0.84 0.07 0.76 \nCicada Evening cicada 2.77 2.69 0.02 0.02 0.28 0.95 0.09 0.93 \nVolcano Bubbles of mud volcano 7.11 1. 46 0.12 0.29 1.65 0.15 0.22 0.07 \nBamboo Wind through bamboo forest 4.98 3.13 0.07 0.06 3.76 0.02 0.06 0.26 \n \nFigure 1. (a) Normalized ACF of Cicada  as a nature-generated sound and ( b) normalized IACF of \nCutting  as a human-generated so und. The definitions of τ1, ϕ1, WΦ(0) and the IACC are included. \n2.2. Participants \nWe recruited 26 participants (20 men and 6 women; age: 21.7 ± 0.4 years) who had \nnormal hearing. All participants self-repor ted that they knew of the ASMR through \nwatching Japanese YouTube channels. The in stitutional ethics co mmittee approved the \nexperimental protocol (approval code: R3-19).  \n2.3. Tasks and Procedures \nAfter listening to the ASMR trigger (50 s) through headphones  (HD598, Sennheiser, \nWedemark, Germany) binaurally, the participants  were instructed to provide scores on a \nFigure 1. (a) Normalized ACF of Cicada as a nature-generated sound and ( b) normalized IACF of\nCutting as a human-generated sound. The deﬁnitions of \u001c1,\u001e1, WF(0)and the IACC are included.\nThe IACC was calculated from the normalized IACF:\nflr(t) =flr(t) =Flr(t;s,T)p\nFll(0;s,T)Frr(0;s,T), (3)\nwhere\nFlr=1\n2TZs+T\ns\u0000Tpl0(t)pr0(t+t)dt. (4)\nHere,Frris the ACF for the right channel and pr0(t) is the A-weighted sound in the\nright channel. The IACC is the maximum peak amplitude of the IACF whose delay time\nis within\u00061 ms (Figure 1b). The IACC is related to the subjective sound diffuseness\nmentioned in the Introduction. The integration interval (2 T) and running step ( s) were,\nrespectively, 1 and 0.5 s for the both ACF and IACF calculations, and Table 1 lists average\nvalues of the time-varying parameters."
    },
    {
      "section": "Page 5",
      "page_number": 5,
      "text": "Audiol. Res. 2022 ,12 578\n2.2. Participants\nWe recruited 26 participants (20 men and 6 women; age: 21.7 \u00060.4 years) who\nhad normal hearing. All participants self-reported that they knew of the ASMR through\nwatching Japanese YouTube channels. The institutional ethics committee approved the\nexperimental protocol (approval code: R3-19).\n2.3. Tasks and Procedures\nAfter listening to the ASMR trigger (50 s) through headphones (HD598, Sennheiser,\nWedemark, Germany) binaurally, the participants were instructed to provide scores on a\nﬁve-point Likert scale in the subsequent 10 s. The LAeqat the ear positions was adjusted to\n45 dBA. After mounting the headphones on a head and torso simulator (type 4128; Brüel\n& Kjær, Naerum, Denmark), the output level was adjusted to the 45 dBA in the average\nof the left and right channels. The participants were asked to give scores ( \u00002,\u00001, 0, 1 or\n2) for the degree of perceived loudness (from \u00002: not so loud to 2: very loud), perceived\npitch (from\u00002: very low to 2: very high), comfort (from \u00002: not so comfortable to 2: very\ncomfortable), perceived closeness to the sound image (from \u00002: very far to 2: very close)\nand ASMR (from \u00002: not feeling an ASMR to 2: feeling a strong ASMR) on the question\nsheet. The order of presentation of the AMSR triggers was randomized. The experiment\nwas conducted in an anechoic chamber ( LAeqof the background noise below 30 dB) at\nOsaka University, Japan. The Matlab was used to calculate the statistical values in the\nfollowing section.\n3. Results\nFigure 2 shows the average scores of the subjective loudness, pitch, comfort, closeness,\nand ASMR for the human- (black symbols) and nature-generated (gray symbols) sounds.\nThe subjective loudness, closeness, and ASMR scores tended to be higher for the human-\ngenerated sounds than for the nature-generated sounds. According to a t-test of the total\nscores of the human- (260 = 10 ASMR triggers \u000226 participants) and nature-generated\n(260) sounds, there were signiﬁcant differences in the subjective loudness ( t338= 3.65,\np< 0.01) , closeness ( t338= 8.69, p< 0.01), and ASMR ( t338= 7.84 ,p< 0.01). In contrast, the\ncomfort was higher for the nature-generated sounds ( t338= 6.28, p< 0.01) and there was\nno signiﬁcant difference in the perceived pitch between the nature- and human-generated\nsounds ( t338= 0.28 ,p= 0.78). The three sounds with the highest ASMR values were Earpick,\nShampoo, and Book for the human-generated sounds and Volcano, Lava, and Bubble for the\nnature-generated sounds, and they were commonly perceived to be close. The three sounds\nwith the lowest ASMR values were Cutting, Heels, and Brush for the human-generated\nsound and Cicada, Bamboo, and Rain for the nature-generated sounds, and they were\ncommonly perceived to be far."
    },
    {
      "section": "Page 6",
      "page_number": 6,
      "text": "Audiol. Res. 2022 ,12 579\nAudiol. Res.  2022 , 12, FOR PEER REVIEW  6 \n \n  \nFigure 2. Average scores for ( a) loudness, ( b) pitch, ( c) comfort, ( d) closeness, and ( d) the ASMR. \nBlack and gray symbols are results for human- an d nature-generated sounds , respectively. The bar \non each symbol shows standard deviations. The bl ack and gray horizontal dot lines are total aver-\naged scores for human- and nature -generated sounds, respectively. \nFigure 2. Average scores for ( a) loudness, ( b) pitch, ( c) comfort, ( d) closeness, and ( e) the ASMR.\nBlack and gray symbols are results for human- and nature-generated sounds, respectively. The bar on\neach symbol shows standard deviations. The black and gray horizontal dot lines are total averaged\nscores for human- and nature-generated sounds, respectively.\nTable 2 shows the Pearson correlation coefﬁcients of the ASMR scores with the sound\nquality parameters that had normal distributions. The ASMR scores of the nature-generated\nsounds were strongly correlated with loudness and roughness among the Zwicker parame-"
    },
    {
      "section": "Page 7",
      "page_number": 7,
      "text": "Audiol. Res. 2022 ,12 580\nters. Meanwhile, the ASMR scores of the human-generated sounds were strongly correlated\nwith the IACC among the ACF/IACF parameters. Figure 3 shows the ASMR scores as func-\ntions of loudness, roughness, and IACC which showed high Pearson correlation coefﬁcients.\nThe strong negative relationship could be observed in the IACC for the human-generated\nsounds, while the positive relationships could be observed in the loudness and roughness\nfor the nature-generated sounds. Table 2 lists the correlation coefﬁcients of the ASMR\nscores with the scores of the other psychological judgements, too. The subjective loudness\nhad a high correlation with the ASMR generated by the nature-generated sounds. Addi-\ntionally, closeness had a high correlation with the ASMR generated by both human- and\nnature-generated sounds.\nTable 2. Correlation coefﬁcients of the ASMR scores among Zwicker’s parameters, ACF/IACF\nparameters and subjective judgements (**: p< 0.01, *: p< 0.05).\nZwicker’s Parameters ACF/IACF Parameters Subjective Judgements\nLoudness Sharpness RoughnessFluctuation\nStrength\u001c1\u001e1 WF(0) IACCSubjective\nLoudnessPitch Comfort Closeness\nASMR (Total) 0.42 \u00000.21 0.27 0.15 0.12 \u00000.36 0.06\u00000.67 ** 0.64 ** \u00000.29\u00000.38 0.93 **\nASMR (Human) 0.04 0.11 0.32 \u00000.30 0.39 \u00000.32\u00000.04\u00000.89 ** 0.38 \u00000.20 0.02 0.93 **\nASMR (Nature) 0.73 * \u00000.61 0.77 ** 0.47 0.14 \u00000.46 0.34\u00000.41 0.92 ** \u00000.53\u00000.17 0.96 **\nAudiol. Res.  2022 , 12, FOR PEER REVIEW  7 \n \n  \nFigure 3. Relationships of the ASMR scores with loudness, roughness, and IACC for ( a) human-\ngenerated sounds (black symbols) and ( b) nature-generated sounds (gray symbols). \nTable 2. Correlation coefficients of the ASMR scores among Zwicker’s parameters, \nACF/IACF parameters and subjective judgements (**: p < 0.01, *: p < 0.05). \n Zwicke r’s Parameters ACF/IACF Parameters Subjective Judgements \n Loudness SharpnessRoughnessFluctuation \nstrength τ1 ϕ1 WΦ(0) IACC Subjective Loud-\nness PitchComfort Close-\nness \nASMR (Total) 0.42 −0.21 0.27 0.15 0.12 −0.36 0.06 −0.67 ** 0.64 ** −0.29 −0.38 0.93 ** \nASMR (Human) 0.04 0.11 0.32 −0.30 0.39 −0.32 −0.04 −0.89 ** 0.38 −0.20 0.02 0.93 ** \nASMR (Nature) 0.73 * −0.61 0.77 ** 0.47 0.14 −0.46 0.34 −0.41  0.92 ** −0.53 −0.17 0.96 ** \n4. Discussion \nThe primary reason why the ASMR scores of the human-generate d sounds were sig-\nnificantly higher than the nature-generated sounds may be the distance from the sound \nsource to the receiver. In fact, the perceived closeness was strongly related to the ASMR \nsensation (Table 2). The human-generated sounds  were recorded at a position close to the \nbinaural devices whereas the nature-generated sounds were recorded at a certain distance \nfrom the sound source. Additionally, the ASMR triggers used in previous studies (e.g., \nwhisper voice, personal attention, and crisp sounds) were recorded close to the binaural \nmicrophone [1,9–13]. In these triggers, the personal attention refers to role-play videos \nthat concentrate on the viewer, so that it is not just an ASMR trigger but the scenario/con-text in which the triggers occur. To examine acoustical aspects in the triggers, sounds in-\ncluding the scenario/context (e.g., speech) were removed from the triggers used in this \nstudy. However, the Earpick,  Shampoo, and Hair sounds that had high ASMR scores \nmade the participants imagine to be acted upon  themselves. It seems undeniable that such \nunintended personal attention might help the ASMR sensations for these triggers, and the \nvery closed triggers to the participants are li kely to induce the pseudo-personal attention. \nFigure 3. Relationships of the ASMR scores with loudness, roughness, and IACC for ( a) human-\ngenerated sounds (black symbols) and ( b) nature-generated sounds (gray symbols)."
    },
    {
      "section": "Page 8",
      "page_number": 8,
      "text": "Audiol. Res. 2022 ,12 581\n4. Discussion\nThe primary reason why the ASMR scores of the human-generated sounds were\nsigniﬁcantly higher than the nature-generated sounds may be the distance from the sound\nsource to the receiver. In fact, the perceived closeness was strongly related to the ASMR\nsensation (Table 2). The human-generated sounds were recorded at a position close to the\nbinaural devices whereas the nature-generated sounds were recorded at a certain distance\nfrom the sound source. Additionally, the ASMR triggers used in previous studies (e.g.,\nwhisper voice, personal attention, and crisp sounds) were recorded close to the binaural\nmicrophone [ 1,9–13]. In these triggers, the personal attention refers to role-play videos that\nconcentrate on the viewer, so that it is not just an ASMR trigger but the scenario/context in\nwhich the triggers occur. To examine acoustical aspects in the triggers, sounds including the\nscenario/context (e.g., speech) were removed from the triggers used in this study. However,\nthe Earpick, Shampoo, and Hair sounds that had high ASMR scores made the participants\nimagine to be acted upon themselves. It seems undeniable that such unintended personal\nattention might help the ASMR sensations for these triggers, and the very closed triggers\nto the participants are likely to induce the pseudo-personal attention.\nFor nature-generated sounds, sound qualities relating to higher loudness and rough-\nness induced the ASMR experience (Figure 3). These parameters also had high correlations\nwith the closeness scores (loudness: r= 0.73, p< 0.05, roughness: r= 0.77, p< 0.01). The\nnearby sounds produce the ASMR, whereas some listeners are annoyed by sounds close to\ntheir ears. Therefore, the comfort scores were signiﬁcantly lower for the human-generated\nsounds (Figure 2c). Although it is well known that people who experience ASMRs report\nfeeling relaxed and sleepy after watching and listening to ASMR content, some people feel\nannoyance from the triggers [ 4]. The hypersensitivity for the auditory perception is the\nsame origin for the ASMR and misophonia; however, higher-order cognitive processing\nmay divide expressed emotions into the preference for the ASMR or annoyance for the\nmisophonia [ 3]. The very closed sound makes the listeners imagine either the positive\npersonal attention or negative invasion of territory. Separation at the cognitive processing\nmay be related to the different interpretation of the closeness. If this study contains speech\nsignals addressing the participants, the comfort scores for the human-generated sounds\nmay be improved.\nAlthough a previous ASMR study reported that sounds with a lower pitch were\nmore likely to produce an intense ASMR sensation [ 9], the pitch scores and ACF/IACF\nparameters relating to pitch (i.e., \u001c1,\u001e1and W F(0)) did not affect the ASMR score (Figure 2b\nand Table 2). The bass or low-frequency response is higher when a sound source is close\nto a directional or cardioid microphone (in what is known as the acoustical proximity\neffect) [ 31]. In this study, the acoustical proximity effect might occur to the same degree for\nany human-generated sound that is sufﬁciently close to the binaural microphones.\nThe human-generated sounds with a lower IACC produced a stronger ASMR sensation\n(Figure 3). The IACC is related to the spatial characteristics of a sound ﬁeld, and it can thus\ncontrol the location of a sound image. In concert halls (having a diffused sound ﬁeld), the\nIACC is lower when the distance between the sound source and receiver is greater [ 32],\nbecause the direct sound that tends to increase the IACC is weakened relative to reﬂections\nand reverberations. In contrast, in laboratory experiments, the IACC can be controlled by\nchanging the interchannel phase difference of stereo loudspeakers in front of the listener,\nand a sound with lower IACC can generate a sound image closer to the listener (in what is\nreferred to as auditory distance rendering) [ 33–37]. This phenomenon observed in auditory\ndistance rendering agrees with the results of the present study. However, the binaural phase\nof the ASMR triggers used in this study was not manipulated digitally; therefore, there\nmay be another explanation in this case. The IACC indicates the similarity of time-varied\nsounds entering the left and right ears. It is thus expected that sound near one ear (e.g., the\nsound heard when using an earpick) has low similarity (low IACC) between the ears, and\nwe thus have to separate the relationships between the IACC and the distance from the\nsound image into near and far ﬁelds centering around the listener’s head."
    },
    {
      "section": "Page 9",
      "page_number": 9,
      "text": "Audiol. Res. 2022 ,12 582\nFinally, we discuss the possible applications of these ﬁndings in clinical treatments\nfor misophonia. The most successfully used treatment at the clinical scene is cognitive\nbehavioral therapy (CBT) [ 38–42]. The CBT protocol constitutes four different techniques:\ntask concentration exercises, counterconditioning, stimulus manipulation, and relaxation\nexercises. Following treatment, 48% of the patients showed a signiﬁcant reduction of miso-\nphonia symptoms [ 43]. In a session of stimulus manipulation, the patients are instructed to\nchange the pitch and time interval of sound triggers by using an audio-editing software,\nand this manipulation initiates a sense of control over their personal misophonic trigger\nsounds. In this study, the IACC is the most effective factor to control the ASMR sensation,\nso the change of IACC (e.g., convolution with binaural impulse responses) may be effective\nto let the patients know the misophonic trigger sounds under their control.\n5. Conclusions\nThe following conclusions are drawn from the results of the study.\n(1) Human-generated sounds are more likely to trigger stronger ASMRs than nature-\ngenerated sounds.\n(2) Among possible ASMR auditory triggers, sounds perceived to be close to the listener\nare more likely to evoke the ASMR sensation.\n(3) In the case of nature-generated sounds, the ASMR triggers with higher loudness and\nroughness among Zwicker parameters are more likely to evoke the ASMR sensation.\n(4) In the case of human-generated sounds, the ASMR triggers with a lower IACC among\nthe ACF/IACF parameters are more likely to evoke the ASMR sensation.\nFunding: This research was supported by a Grant-in-Aid for Science Research (B) from the Japan\nSociety for the Promotion of Science (18H03560).\nInstitutional Review Board Statement: The institutional ethics committee in Osaka University\napproved the experimental protocol (approval code: R3-19).\nInformed Consent Statement: Informed consent was obtained from all subjects involved in the\nstudy. Written informed consent has been obtained from the patients to publish this paper.\nData Availability Statement: Not applicable.\nAcknowledgments: The author thanks the participants for their cooperation during the experiment,\nYoshiki Konosu for helping with the experiment.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\nReferences\n1. Barratt, E.L.; Davis, N.J. Autonomous Sensory Meridian Response (ASMR): A ﬂow-like mental state. PeerJ 2015 ,3, e851. [CrossRef]\n[PubMed]\n2. McErlean, A.B.J.; Banissy, M.J. Increased misophonia in self-reported Autonomous Sensory Meridian Response. PeerJ 2018 ,6,\ne5351. [CrossRef] [PubMed]\n3. McGeoch, P .D.; Rouw, R. How everyday sounds can trigger strong emotion: ASMR, misophonia and the feeling of wellbeing.\nBioEssays. 2020 ,42, 2000099. [CrossRef] [PubMed]\n4. Tada, K.; Hasegawa, R.; Kondo, H. Sensitivity to everyday sounds: ASMR, misophonia, and autistic traits. Jpn. J. Psychol. 2022 ,93,\n263–269. [CrossRef]\n5. Jastreboff, M.M.; Jastreboff, P .J. Components of decreased sound tolerance: Hyperacusis, misophonia, phonophobia. ITHS News\nLett. 2001 ,2, 5–7.\n6. Jastreboff, P .J.; Jastreboff, M.M. Treatments for decreased sound tolerance (hyperacusis and misophonia). In Seminars in Hearing ;\nThieme Medical Publishers: New York, NY, USA, 2014; Volume 35, pp. 105–120.\n7. Møller, A.R. Misophonia, phonophobia, and ‘exploding head’ syndrome. In Textbook of Tinnitus ; Møller, A.R., Langguth, B.,\nDeRidder, D., Kleinjung, T., Eds.; Springer: New York, NY, USA, 2011; pp. 25–27.\n8. Wu, M.S.; Lewin, A.B.; Murphy, T.K.; Storch, E.A. Misophonia: Incidence, phenomenology, and clinical correlates in an\nundergraduate student sample. J. Clin. Psychol. 2014 ,70, 994–1007. [CrossRef]\n9. Barratt, E.L.; Spence, C.; Davis, N.J. Sensory determinants of the autonomous sensory meridian response (ASMR): Understanding\nthe triggers. PeerJ. 2017 ,5, e3846. [CrossRef]"
    },
    {
      "section": "Page 10",
      "page_number": 10,
      "text": "Audiol. Res. 2022 ,12 583\n10. Fredborg, B.; Clark, J.; Smith, S.D. An examination of personality traits associated with autonomous sensory meridian response\n(ASMR). Front. Psychol. 2017 ,8, 247. [CrossRef]\n11. Poerio, G.L.; Blakey, E.; Hostler, T.J.; Veltri, T. More than a feeling: Autonomous sensory meridian response (ASMR) in\ncharacterized by reliable changes in affect and physiology. PLoS ONE 2018 ,13, e0196645. [CrossRef]\n12. Smith, S.D.; Fredborg, B.; Kornelsen, J. Functional connectivity associated with different categories of autonomous sensory\nmeridian response (ASMR) triggers. Conscious. Cogn. 2020 ,85, 103021. [CrossRef]\n13. Swart, T.R.; Bowling, N.C.; Banissy, M.J. ASMR-experience questionnaire (AEQ): A data-driven step towards accurately classifying\nASMR responders. Br. J. Psychol. 2022 ,113, 68–83. [CrossRef]\n14. Zwicker, E.; Fastl, H. Psychoacoustics: Facts and Models ; Springer: Berlin/Heidelberg, Germany, 1999.\n15. ISO 532-1 ; Acoustics—Methods for Calculating Loudness—Part 1: Zwicker Method. International Organization for Standardiza-\ntion: Geneva, Switzerland, 2017.\n16. DIN 45692 ; Measurement Technique for the Simulation of the Auditory Sensation of Sharpness. German Institute for Standardiza-\ntion: Berlin, Germany, 2009.\n17. Ando, Y. 5. Prediction of subjective preference in concert halls. In Concert Hall Acoustics ; Springer: Berlin/Heidelberg, Germany,\n1995; pp. 70–88.\n18. Cariani, P .A.; Delgutte, B. Neural correlates of the pitch of complex tones. I. Pitch and pitch salience. J. Neurophysiol. 1996 ,76,\n1698–1716. [CrossRef] [PubMed]\n19. Cariani, P .A.; Delgutte, B. Neural correlates of the pitch of complex tones. II. Pitch shift, pitch ambiguity, phase invariance, pitch\ncircularity, rate pitch, and the dominance. J. Neurophysiol. 1996 ,76, 1717–1734. [CrossRef] [PubMed]\n20. Sato, S.; You, J.; Jeon, J.Y. Sound quality characteristics of refrigerator noise in real living environments with relation to\npsychoacoustical and autocorrelation function parameters. J. Acoust. Soc. Am. 2007 ,122, 314–325. [CrossRef] [PubMed]\n21. Soeta, Y.; Shimokura, R. Sound quality evaluation of air-conditioner noise based on factors of the autocorrelation function. Appl.\nAcoust. 2017 ,124, 11–19. [CrossRef]\n22. Ando, Y. Autocorrelation-based features for speech representation. Acta Acust. United Acust. 2015 ,101, 145–154. [CrossRef]\n23. Shimokura, R.; Akasaka, S.; Nishimura, T.; Hosoi, H.; Matsui, T. Autocorrelation factors and intelligibility of Japanese monosylla-\nbles in individuals with sensorineural hearing loss. J. Acoust. Soc. Am. 2017 ,141, 1065. [CrossRef]\n24. Kitamura, T.; Shimokura, R.; Sato, S.; Ando, Y. Measurement of temporal and spatial factors of a ﬂushing toilet noise in a\ndownstairs bedroom. J. Temp. Des. Archit. Environ. 2002 ,2, 13–19.\n25. Fujii, K.; Soeta, Y.; Ando, Y. Acoustical properties of aircraft noise measured by temporal and spatial factors. J. Sound Vib. 2001 ,\n241, 69–78. [CrossRef]\n26. Fujii, K.; Atagi, J.; Ando, Y. Temporal and spatial factors of trafﬁc noise and its annoyance. J. Temp. Des. Archit. Environ. 2002 ,2,\n33–41.\n27. Soeta, Y.; Shimokura, R. Survey of interior noise characteristics in various types of trains. Appl. Acoust. 2013 ,74, 1160–1166.\n[CrossRef]\n28. Smith, S.D.; Fredborg, B.K.; Kornelsen, J. An examination of the default mode network in individuals with autonomous sensory\nmeridian response (AMSR). Soc. Neurosci. 2017 ,12, 361–365. [CrossRef] [PubMed]\n29. Lochte, B.C.; Guillory, S.A.; Richard, C.A.H.; Kelly, W.M. An fMRI investigation of neural correlates underlying the autonomous\nsensory median response (ASMR). BioImpacts 2018 ,8, 295–304. [CrossRef]\n30. Audio Toolbox. Available online: https://jp.mathworks.com/help/audio/index.html?s_tid=CRUX_lftnav (accessed on 23\nSeptember 2022).\n31. Nikolov, M.E.; Blagoeva, M.E. Proximity effect frequency characteristics of directional microphones. In Proceedings of the Audio\nEngineering Society Convention 108, Paris, French, 19–22 February 2000.\n32. Fujii, K.; Hotehama, T.; Kato, K.; Shimokura, R.; Okamoto, Y.; Suzumura, Y.; Ando, Y. Spatial distribution of acoustical parameters\nin concert halls: Comparison of different scattered reﬂections. J. Temp. Des. Archit. Environ. 2004 ,4, 59–68.\n33. Kurozumi, K.; Ohgushi, K. The relationship between the cross correlation coefﬁcient of two-channel acoustic signals and sound\nimage quality. J. Acoust. Soc. Am. 1983 ,74, 1726–1733. [CrossRef]\n34. Gerzon, M.A. Signal processing for simulating realistic stereo images. In Proceedings of the Audio Engineering Society Convention\n93, San Francisco, CA, USA, 1–4 October 1992.\n35. Kendall, G.S. The decorrelation of audio signals and its impact on spatial imagery. Comput. Music J. 1995 ,19, 71–87. [CrossRef]\n36. Koyama, S.; Furuya, K.; Hiwasaki, Y.; Haneda, Y. Reproducing virtual sound sources in front of a loudspeaker array using inverse\nwave propagator. IEEE Trans. Audio Speech Lang. Process. 2012 ,20, 1746–1758. [CrossRef]\n37. Jeon, S.W.; Park, Y.C.; Youn, D.H. Auditory distance rendering based on ICPD control for stereophonic 3D audio system. IEEE\nSignal Process. Lett. 2015 ,22, 529–533. [CrossRef]\n38. Bernstein, R.E.; Angell, K.L.; Dehle, C.M. A brief course of cognitive behavioral therapy for the treatment of misophonia: A case\nexample. Cogn. Behav. Ther. 2013 ,6, e10. [CrossRef]\n39. Dozier, T.H. Counterconditioning treatment for misophonia. Clin. Case Stud. 2015 ,14, 374–387. [CrossRef]\n40. Dozier, T.H. Treating the initial physical reﬂex of misophonia with the neural repatterning technique: A counterconditioning\nprocedure. Psychol. Thought 2015 ,8, 189–210. [CrossRef]"
    },
    {
      "section": "Page 11",
      "page_number": 11,
      "text": "Audiol. Res. 2022 ,12 584\n41. McGuire, J.F.; Wu, M.S.; Storch, E.A. Cognitive-behavioral therapy for 2 youths with Misophonia. J. Clin. Psychiatry 2015 ,76,\n573–574. [CrossRef] [PubMed]\n42. Reid, A.M.; Guzick, A.G.; Gernand, A.; Olsen, B. Intensive cognitive-behavioral therapy for comorbid misophonic and obsessive-\ncompulsive symptoms: A systematic case study. J. Obsessive Compuls. Relat. Disord. 2016 ,10, 1–9. [CrossRef]\n43. Schröder, A.E.; Vulink, N.C.; van Loon, A.J.; Denys, D.A. Cognitive behavioral therapy is effective in misophonia: An open trial. J.\nAffect. Disord. 2017 ,217, 289–294. [CrossRef] [PubMed]"
    }
  ]
}