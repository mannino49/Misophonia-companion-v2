{
  "doc_type": "scientific paper",
  "title": "What sound sources trigger misophonia? Not just chewing and breathing",
  "authors": [
    "Heather A. Hansen",
    "Andrew B. Leber",
    "Zeynep M. Saygin"
  ],
  "year": 2024,
  "journal": "Psychology, The Ohio State University",
  "doi": null,
  "abstract": "Misophonia is a highly prevalent yet understudied condition characterized by aversion toward particular environmental sounds. Oral/nasal sounds (e.g., chewing, breathing) have been the focus of research, but variable experiences warrant an objective investigation. Experiment 1 asked whether human-produced oral/nasal sounds were more aversive than human-produced non-oral/nasal sounds and nonhuman/nature sounds. Experiment 2 additionally asked whether machine-learning algorithms could predict the presence and severity of misophonia. Sounds were presented to individuals with misophonia (Exp.1: N=48, Exp.2: N=45) and members of the general population (Exp.1: N=39, Exp.2: N=61). Aversiveness ratings to each sound were self-reported. Sounds from all three source categories - not just oral/nasal sounds - were rated as significantly more aversive to individuals with misophonia than controls. Further, modeling all sources classified misophonia with 89% accuracy and significantly predicted misophonia severity (r=0.75). Misophonia should be conceptualized as more than an aversion to oral/nasal sounds, which has implications for future diagnostics and experimental consistency moving forward.",
  "keywords": [
    "misophonia",
    "diagnosis",
    "sound sensitivity",
    "sound aversion",
    "source categories",
    "machine learning"
  ],
  "research_topics": [
    "misophonia",
    "sound source categories",
    "environmental sound aversion",
    "machine learning classification",
    "psychiatric disorder diagnosis",
    "auditory stimulus response",
    "oral/nasal sounds",
    "non-oral/nasal human sounds",
    "nonhuman/nature sounds",
    "neuroimaging in misophonia",
    "experimental psychology",
    "sound sensitivity measurement"
  ],
  "created_at": "2025-05-05T01:47:10.570974Z",
  "source_pdf": "documents/research/Global/Hansen 2021 What Sound sources trigger misophonia not just chewing and breathing.pdf",
  "sections": [
    {
      "section": "Page 1",
      "page_number": 1,
      "text": "Running Head: MISOPHONIA SOURCE CATEGORIES \nWhat sound sources trigger misophonia? Not just chewing and breathing\nHeather A. Hansen1, Andrew B. Leber1, Zeynep M. Saygin1 \n1Department of Psychology, The Ohio State University \nPlease address correspondence to:\nHeather A. Hansen, Department of Psychology, The Ohio State University, 225 Psychology \nBuilding, 1835 Neil Avenue, Columbus, OH, 43210. E-mail: hansen.508@osu.edu\nWord Count: 8466 (entire manuscript)1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n12\n13\n14\n15\n16\n17\n18\n19\n20"
    },
    {
      "section": "Page 2",
      "page_number": 2,
      "text": "2\nMISOPHONIA SOURCE CATEGORIES \nAbstract\nObjectives: \nMisophonia is a highly prevalent yet understudied condition characterized by aversion toward \nparticular environmental sounds. Oral/nasal sounds (e.g., chewing, breathing) have been the \nfocus of research, but variable experiences warrant an objective investigation. Experiment 1 \nasked whether human-produced oral/nasal sounds were more aversive than human-produced \nnon-oral/nasal sounds and nonhuman/nature sounds. Experiment 2 additionally asked whether \nmachine-learning algorithms could predict the presence and severity of misophonia.\nMethod:\nSounds were presented to individuals with misophonia (Exp.1: N=48, Exp.2: N=45) and \nmembers of the general population (Exp.1: N=39, Exp.2: N=61). Aversiveness ratings to each \nsound were self-reported.\nResults: \nSounds from all three source categories – not just oral/nasal sounds – were rated as significantly \nmore aversive to individuals with misophonia than controls. Further, modeling all sources \nclassified misophonia with 89% accuracy and significantly predicted misophonia severity \n(r=0.75). \nConclusions:\nMisophonia should be conceptualized as more than an aversion to oral/nasal sounds, which has \nimplications for future diagnostics and experimental consistency moving forward.\nKeywords: misophonia, diagnosis, sound sensitivity, sound aversion, source categories, machine \nlearning21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43"
    },
    {
      "section": "Page 3",
      "page_number": 3,
      "text": "3\nMISOPHONIA SOURCE CATEGORIES \nIntroduction\nWhen nails scrape against a chalkboard or someone screams, most people have an \nimmediate adverse reaction to the sound: their attention is instantly captured, they might wince \nor be irritated by it, and they look to make it stop. These generally aversive sounds are often \nloud, rough, and high frequency, and are thought to elicit a negative reaction to aid \nsurvival(Halpern et al., 1986) . Some individuals, however, experience similar discomfort to \ncertain seemingly innocuous soft sounds in the environment. For instance, sounds like chewing, \nbreathing, or tapping may evoke similar feelings of anxiety, panic, anger, or even rage in these \nindividuals. Individuals with these experiences are said to have “misophonia”, a term coined by \nJastreboff and Jastreboff who described the condition as involving negative reactions to specific \nsounds and/or sounds that occur in specific contexts, but otherwise normal tolerance for other \nsounds(Jastreboff & Jastreboff, 2002) . These experiences are not uncommon and can be quite \nsevere; it has been estimated that misophonic impairments may exist in about 20% of the general\npopulation(Wu et al., 2014; Zhou et al., 2017) , with one in five sufferers indicating thoughts of \nsuicide because of the sounds (Rouw & Erfanian, 2017) .\nDespite its apparent prevalence, misophonia research is still in its nascency (see Brout et al.,\n2018 for a review), and misophonia is not currently listed as a mental health disorder in the \nAmerican Psychiatric Association’s Diagnostic and Statistical Manual of Mental Disorders  \n(DSM-5; American Psychiatric Association, 2013). Researchers who have explored symptom \npatterns and comorbidity of sufferers suggest misophonia be considered a discrete psychiatric \ndisorder(Rouw & Erfanian, 2017; Schröder et al., 2013) . Schröder and colleagues (Schröder et \nal., 2013) went so far as to propose their own diagnostic criteria, including the stipulation that \n“the presence or anticipation of a specific sound, produced by a human being (e.g., eating 44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66"
    },
    {
      "section": "Page 4",
      "page_number": 4,
      "text": "4\nMISOPHONIA SOURCE CATEGORIES \nsounds, breathing sounds), provokes an impulsive aversive physical reaction which starts with \nirritation or disgust that instantaneously becomes anger”.  Although a valuable stepping stone, \nwe argue that the scope of this definition should be reconsidered, in part because of its limited \nconception of sounds that qualify as triggering.  For example, case studies suggest that \nindividuals with misophonia express annoyance for a variety of sounds, not all produced by a \nhuman (e.g., dogs barking, glasses clinking, nail picking, door slamming, specific songs, etc.)\n(Ferreira et al., 2013; Hadjipavlou et al., 2008; P. L. Johnson et al., 2013; Neal & Cavanna, 2013;\nWebber et al., 2014) .  Larger questionnaires and interviews show that frequently reported trigger \nsounds include eating sounds (e.g., chewing), breathing noises (e.g., sniffling), sounds made by \nthe body (e.g., shuffling feet), and sounds made by objects (e.g., clock ticking) (Edelstein et al., \n2013). Some psychiatric interviews have concluded that all trigger sounds are oral or nasal \nsounds produced by humans (Schröder et al., 2013)  – a point supported by a meta-analysis of \nclinical case studies (Taylor, 2017) – but other larger clinical evaluations describe a plethora of \nnonhuman trigger sounds reported by patients (e.g., school bells, refrigerator \nhumming(Jastreboff & Jastreboff, 2014) . Because vast individual differences seem to exist in the\ntypes of stimuli that individuals with misophonia find aversive, Dozier, Lopenz, and \nPearson(Dozier et al., 2017)  suggest updating the criteria proposed by Schröder and \ncolleagues(Schröder et al., 2013) . However, thus far there has been no experimental evidence \nsupporting whether or not sounds need to be produced by a human being (or be oral or nasal) to \nbe bothersome.\nSince these results are discordant, further research is necessary.  Moreover, these results are \ndrawn from interviews; few studies have experimentally presented auditory stimuli to individuals\nwith misophonia to investigate the types of stimuli that are aversive. Of those studies that have, 67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89"
    },
    {
      "section": "Page 5",
      "page_number": 5,
      "text": "5\nMISOPHONIA SOURCE CATEGORIES \nwe have learned that participants with misophonia find auditory stimuli more bothersome than \nvisual stimuli(Edelstein et al., 2013) , individuals with higher misophonic sensitivity have \ndecreased cognitive performance in the presence of gum chewing (Seaborne & Fiorella, 2018) , \nand individuals with misophonia have higher activity in the anterior insular cortex (Kumar et al., \n2017), anterior cingulate cortex and superior temporal cortex (Schröder et al., 2019)  when \nlistening to trigger sounds. These physiological and neuroimaging experiments are valuable steps\nforward in investigating the mechanisms of misophonia. However, these experiments were not \ndesigned to determine what particular sounds trigger misophonia and mainly used human-\nproduced oral/nasal sounds as their triggering stimuli.\nGiven the vagueness by which misophonia is defined and the reliance on interviews  to \nunderstand the nuances of a seemingly prevalent condition, an empirical exploration into the \ntypes of sounds that are triggering to individuals with misophonia is necessary. A consensus on \nwhat sounds constitute misophonia would help in future diagnosis, as well as lay a foundation for\nappropriate stimuli to use in experiments moving forward. Does the source of a sound matter in \ndetermining whether it will bother an individual with misophonia? That is, does the sound need \nto be produced by a human being or be oral/nasal in order to be triggering? The present study \naims to address these questions. Experiment 1 presents self-described individuals with \nmisophonia and healthy controls with 30 everyday sounds from three different source categories:\n1) human-produced oral/nasal sounds (e.g., chewing gum), 2) human-produced non-oral/nasal \nsounds (e.g., clicking a pen), and 3) nonhuman/nature sounds (e.g., clock ticking). Self-report \nbehavioral measures were obtained. Experiment 2 replicates and generalizes Experiment 1 to a \nnovel and larger online sound bank, using 125 sounds in total. Additionally, Experiment 2 uses \nmachine learning methods to generate independent predictions of i) misophonia level and ii) 90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112"
    },
    {
      "section": "Page 6",
      "page_number": 6,
      "text": "6\nMISOPHONIA SOURCE CATEGORIES \nmisophonia classification for each individual based only on their discomfort ratings to the sound \nbank.\nExperiment 1\nMethod\nStimuli\n30 auditory clips were used as stimuli in this experiment. The clips were pulled from \nfreesound.org and an online stimulus set (Norman-Haignere et al., 2015) . Stimuli were chosen \nbased off sounds commonly reported in the literature to be triggering to individuals with \nmisophonia, as well as other everyday background sounds that retained the same soft, repetitive \nnature as commonly reported triggers. Sounds were vetted and sorted into the three source \ncategories by a majority consensus of five independent raters. Human-produced oral/nasal \nsounds (hereafter “Source 1”) included crunching chips, breathing, coughing, chewing gum, \nslurping, sneezing, sniffling, snoring, swallowing, and throat clearing. Human-produced non-\noral/nasal sounds (hereafter “Source 2”) included bouncing a basketball, chopping vegetables, \nhammering, walking in heels, clicking a mouse, clipping nails, clicking a pen, swinging on a \nswingset, typing, and writing. Nonhuman/nature sounds (hereafter “Source 3”) included a bird \nchirping, clock ticking, crow cawing, dog drinking water, frog croaking, printer, water dripping, \nwind howling, wind chimes, and windshield wipers.\nAll sounds were 15s in duration, stereophonic, and matched for amplitude using RMS in \nAdobe Audition CC (v10.0.0.130, Adobe Systems Incorporated, 2017). Minimal edits were \nmade (e.g., noise reduction, slowing, cropping, or looping) using Audition and Audacity® \n(v2.1.3, Audacity Team, 2017).113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134"
    },
    {
      "section": "Page 7",
      "page_number": 7,
      "text": "7\nMISOPHONIA SOURCE CATEGORIES \nSurveys\nEach participant’s misophonia level was determined via three misophonia assessment \nsurveys. All participants completed the Misophonia Activation Scale (MAS-1;  Fitzmaurice, \n2014), the Misophonia Assessment Questionnaire (MAQ-2;  Johnson & Dozier, 2013) , and the \nAmsterdam Misophonia Scale (A-MISO-S; Schröder et al., 2013) . \nAdditionally, to probe any comorbid effects with other psychiatric conditions, all \nparticipants completed the Obsessive Compulsive Inventory-Revised (OCI-R; Foa et al., 2002) \nand Depression Anxiety Stress Scale-21 (DASS-21; Lovibond & Lovibond, 1995) .\nParticipants\nMisophonia. 48 individuals (28 females, 20 males, mean age = 33.2 years) with self-\ndiagnosed misophonia were included in this experiment. Participants with misophonia needed to \nself-report an average response greater than or equal to a 4 out of 10 on the MAS-1 to be eligible \nfor the study. According to a composite score that equally weighted the three misophonic \nassessment surveys, in which higher scores denote worse misophonia, the group with misophonia\nhad a mean misophonia level of 59.4 out of 100 (range = 28.5-83.4). All individuals with \nmisophonia self-reported normal or corrected-to-normal vision and hearing (i.e., no hearing \nloss). Individuals were recruited via online misophonia support groups on Facebook and Reddit, \nand volunteered to participate. \nControl. 39 individuals (23 females, 16 males, mean age = 19.6 years) from the general \npopulation were also included in this experiment. All individuals self-reported normal or \ncorrected-to-normal vision and hearing (i.e., no hearing loss). All individuals were recruited from\nthe Psychology undergraduate research pool at The Ohio State University and received course \ncredit for their participation. 136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158"
    },
    {
      "section": "Page 8",
      "page_number": 8,
      "text": "8\nMISOPHONIA SOURCE CATEGORIES \nThe entire group of 39 individuals had a mean composite misophonia level of 19.0 (range\n= 4.0-65.8). Given that individuals with misophonia likely exist in the general population, the \nopposite criterion as above was used to establish a control group: only individuals with an \naverage self-reported response less than a 4 out of 10 on the MAS-1 were kept in the analyses \n(hereafter referred to as “controls”).  The control group consisted of 32 individuals (19 females, \n13 males, mean age = 19.6 years) with a mean misophonia level of 15.4 (range = 4.0-52.8). It is \nworth noting that 17.9% (7/39) of participants were excluded, supporting previous findings that \nmisophonia exists in about 20% of the general population (Wu et al., 2014; Zhou et al., 2017) . \nParticipant scores on the individual misophonia assessments, including a distinction of \nwhich participants from the general population were included as controls, can be found in \nSupplement S1.\nAll experimental methods were approved by The Ohio State University Institutional \nReview Board, and all participants gave informed consent to participate.   \nProcedure\nAll participants received a link to the online experiment through Qualtrics, a secure \nadministration software (Qualtrics, Provo, UT). Participants were required to take the experiment\nwearing headphones from a desktop or laptop. To verify this, participants were given a brief \nheadphone check(Woods et al., 2017)  after giving consent to participate, and told to adjust \nvolume to a comfortable level; only participants who passed the headphone check and had a \nbrowser that enabled Adobe Flash Player could proceed. \nFor the actual experiment, participants were presented with each of the 30 auditory clips \none at a time for 15s each. While listening to the sound, participants viewed the word “Listen” \nand were required to listen to the entirety of the 15s sound before the screen automatically 159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181"
    },
    {
      "section": "Page 9",
      "page_number": 9,
      "text": "9\nMISOPHONIA SOURCE CATEGORIES \nadvanced to a response screen. Participants were asked to identify the previous sound by typing \ninto a textbox (see Figure S1 and Supplement 2 for discussion of these results), and asked to rate \nthe sound’s aversiveness to questions including “How pleasant was the sound?” and “How much \ndiscomfort did you feel during the sound?”, by clicking one response on a 5-point ordinal scale. \nThe unpleasantness rating is scaled from “extremely pleasant” (1) to “extremely unpleasant” (5), \nwith labeled steps in between; the unpleasantness rating aimed to capture typical sound \naversiveness. The discomfort rating is scaled from “none at all” (1) to “an extreme amount” (5), \nwith labeled steps in between; the discomfort rating aimed to capture the evoked misophonic \nreaction. Participants were required to spend a minimum of 5s on the response page before they \ncould submit it, with no maximum time cutoff. After clicking to submit their responses, the next \ntrial began with a new auditory clip. Presentation of the 30 sounds was randomized for each \nparticipant. \nUpon completion of all 30 sounds, participants viewed a webpage debriefing them on the \nexperiment, explaining what misophonia is, and defining misophonic terms (e.g., “triggers”) \npresent in the assessment scales since the items are geared toward a misophonic audience. At the \nend, they completed the surveys listed above and reported demographic information. The surveys\nwere done last to avoid demand characteristics with sound ratings. The entire experiment took \n30-60min to complete. \nAnalyses\nWe used mixed ANOVAs, Student’s t-tests, and Pearson’s correlations to assess the \ndifferences in source categories between individuals with misophonia and controls. For analyses \nin which multiple comparisons were conducted, we used the Holm-Bonferroni method (Holm, \n1979) to control the familywise Type I error rate (corrected p-values are denoted by pHB).182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204"
    },
    {
      "section": "Page 10",
      "page_number": 10,
      "text": "10\nMISOPHONIA SOURCE CATEGORIES \nResults\nSource Categories\nFirst, we explored whether individuals with misophonia were more bothered by sounds \nfrom certain source categories than others, and how this compared to control individuals (without\nmisophonia). Using a 2 (group: misophonia vs. control, between-subjects) x 3 (source category: \n1 [human oral/nasal] vs. 2 [human non-oral/nasal] vs. 3 [nonhuman/nature], within-subjects) \nmixed ANOVA, a group x source category interaction was assessed separately for both the \nunpleasantness and discomfort ratings (Figure 1). For both ratings, there were signficant main \neffects of group (unpleasantness: ( F(1,78) = 21.280, pHB = 3.0 x 10-5, ηp2 = 0.214, discomfort: \nF(1,78) = 15.295, pHB = 3.9 x 10-4, ηp2 = 0.164) and source category (unpleasantness: F(2,77) = \n269.279, pHB = 5.250 x 10-35, ηp2 = 0.875, discomfort: F(2,77) = 135.487, pHB = 1.809 x 10-25, ηp2 =\n0.779). Likewise, the interactions were significant for both the unpleasantness rating ( F(2,77) = \n3.998, pHB = 0.022, ηp2 = 0.094) and the discomfort rating ( F(2,77) = 5.327, pHB = 0.005, ηp2 = \n0.122). Compared to controls, individuals with misophonia rated more unpleasantness and felt \nmore discomfort when listening to human-produced oral/nasal sounds (unpleasantness: t(78) = \n3.665, pHB = 8.980 x 10-4, discomfort: t(78) = 3.940, pHB = 4.447 x 10-4) and human-produced non-\noral/nasal sounds (unpleasantness: t(78) = 4.766, pHB = 2.561 x 10-5, discomfort: t(78) = 3.303, \npHB = 0.002), with a smaller but still significant difference when listening to nonhuman/nature \nsounds (unpleasantness: t(78) = 2.219, pHB = 0.029, discomfort: t(78) = 2.455, pHB = 0.016).\nFurther, within each sample, there were differences in aversiveness ratings for sounds \nfrom different sources. Individuals with misophonia rated Source 1 sounds as significantly more \nunpleasant (t(47) = 9.671, pHB = 1.9 x 10-12) and evoking more discomfort ( t(47) = 10.884, pHB = \n3.9 x 10-14 than Source 2 sounds, which in turn were more unpleasant ( t(47) = 9.122, pHB = 5.7 x 205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227"
    },
    {
      "section": "Page 11",
      "page_number": 11,
      "text": "11\nMISOPHONIA SOURCE CATEGORIES \n10-12) and evoked more discomfort ( t(47) = 4.127, pHB = 1.5 x 10-4) than Source 3 sounds. \nAccordingly, Source 1 sounds were rated by individuals with misophonia as significantly more \nunpleasant (t(47) = 17.173, pHB = 2.1 x 10-21)  and evoking more discomfort ( t(47) = 13.957, pHB =\n7.4 x 10-18)  than Source 3 sounds. Controls likewise rated Source 1 sounds as significantly more \nunpleasant (t(31) = 12.505, pHB = 2.4 x 10-13) and evoking more discomfort ( t(31) = 9.988, pHB = \n6.6 x 10-11) than Source 2 sounds, as well as being more unpleasant ( t(31) = 17.650, pHB = 2.3 x \n10-17) and evoking more discomfort ( t(31) = 10.722, pHB = 1.8 x 10-11) than Source 3 sounds. \nHowever, Source 2 sounds were only rated as more unpleasant than Source 3 sounds ( t(31) = \n5.777, pHB = 2.3 x 10-6); the difference in evoked discomfort did not reach significance ( t(31) = \n1.895, pHB = 0.068) between them. Thus, individuals with misophonia show clearly differentiated\ndiscomfort to different sound sources, whereas controls only find Source 1 sounds particularly \nbothersome. For a depiction of how each individual with misophonia rated sounds from all three \nsources on average, see Supplement 3 (Figure S2).  \nRatings based on Misophonia level\nThere are vast individual differences in the specific triggers that bother individuals with \nmisophonia, and the present experiment had samples with a wide range of misophonia levels. \nAdditionally, given that misophonia may exist on a spectrum and be present to some extent in \nthe general population, we wanted to look at how ratings to each of the three sources were \ninfluenced by misophonia level in our entire sample of participants – both individuals with \nmisophonia (N=48) and from the general population (N=39) – not just the misophonia group vs. \nthe control group. Figure 2 depicts the correlations between average discomfort rating for each \nsource category and composite misophonia level. The correlations are significant for both Source\n1 and Source 2 ratings, for both samples collapsed (Source 1: r = 0.576, pHB < 1 x 10-5, Source 2: 228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250"
    },
    {
      "section": "Page 12",
      "page_number": 12,
      "text": "12\nMISOPHONIA SOURCE CATEGORIES \nr = 0.473, pHB < 1 x 10-5) and for each sample separately (Source 1 – Misophonia: r = 0.595, pHB \n< 1 x 10-5, General population: r = 0.331, pHB = 0.040; Source 2 – Misophonia: r = 0.324, pHB = \n0.025, General population: r = 0.373, pHB = 0.039). However, using Source 3, the correlation is \nonly significant with samples collapsed ( r = 0.322, pHB = 0.007), not within the samples \nseparately (Misophonia: r = 0.281, pHB = 0.105, General population: r = 0.066, pHB = 0.689).  \nThis suggests that the extent to which an individual has misophonia maps onto how bothersome \nthey find sounds from all three source categories, with particularly robust effects for Source 1 \nand Source 2 sounds. \nDiscussion\nWe asked whether sounds from different source categories would evoke different ratings \nof unpleasantness or discomfort between individuals with misophonia and controls. As \nevidenced by Figure 1, although controls experience some discomfort and acknowledge some \nsounds as unpleasant, individuals with misophonia are bothered to a more extreme extent. \nFurther, this difference seems reliant on source category: individuals with misophonia did not \ndiffer from the general population in their reaction towards nonhuman/nature sounds nearly as \nmuch as they did for human-produced sounds. This is reflected in correlation with total \nmisophonia level collapsed across samples, since discomfort for nonhuman/nature sounds did not\ncorrelate with misophonia level as much as discomfort for human-produced sounds did.\nSince between-group differences may be influenced by factors unrelated to the present \nstudy, exploring within-sample differences sheds even more light. Individuals with misophonia \nfind human-produced oral/nasal sounds the most bothersome, as suggested by case studies. \nHowever, aversion is not exclusive to this source; human-produced non-oral/nasal sounds were \nalso significantly more bothersome than nonhuman/nature sounds. Notably, this difference was 251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273"
    },
    {
      "section": "Page 13",
      "page_number": 13,
      "text": "13\nMISOPHONIA SOURCE CATEGORIES \nabsent in controls. This suggests that controls also acknowledge oral/nasal sounds as bothersome \n(but to a lesser extent than individuals with misophonia), but diverge from individuals with \nmisophonia in their response to human non-oral/nasal sounds. Thus, consideration of these \nhuman non-oral/nasal sounds may be of specific interest in diagnosing and distinguishing \nindividuals with misophonia from healthy individuals.  \nExperiment 2\nExperiment 1 clearly suggested that individuals with misophonia feel different aversion \nto sounds, depending on the source. Is this finding reliable, and would it extend to different \nstimuli? And if so, can we identify an individual as having misophonia (and the severity of their \nmisophonia) based only on discomfort ratings to these sounds? Experiment 2 had three main \ngoals: 1) replicate the effects found in Experiment 1 on a larger set of stimuli, 2) classify if an \nindividual has misophonia or not using discomfort ratings rather than self-report questionnaires, \nand 3) predict the level of misophonia severity using these discomfort ratings. Are the set of \nsounds that are the most informative for predicting misophonia (or its severity) only oral/nasal \nsounds, or do they include sounds that are human-produced non-oral/nasal or nonhuman/nature \nsounds as well? \nTo address these questions, we perform a) an ANOVA on this larger set of sounds on an \nindependent set of participants from Experiment 1, b) classifier models to predict misophonia vs.\ncontrol participants based on their discomfort ratings to these sounds, and c) regression models \nto predict misophonia severity. For parts b and c, we additionally identified the most predictive \nsounds and their source categories.\nMethod\nStimuli274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296"
    },
    {
      "section": "Page 14",
      "page_number": 14,
      "text": "14\nMISOPHONIA SOURCE CATEGORIES \nIn addition to the 30 auditory stimuli used in Experiment 1, 95 everyday sounds were \ndrawn from the Google SSML Sound Library \n(https://developers.google.com/actions/tools/sound-library/). Sounds from this sound bank were \nintentionally unedited, and thus varied in stimulus duration ( M = 35s, SD = 65s, range = 5-499s),\nand low-level sound properties. Three independent raters sorted these 95 sounds into the three \nbroader source categories used in Experiment 1; four of these sounds of ambience (coffee shop, \ncrowd talking, kids playing in a gym, and carnival atmosphere) were not agreed to cleanly fit \ninto one category and were thus left out of source category analyses.\nSurveys\nThe same surveys were used as in Experiment 1, in addition to the IPIP Big-Five \npersonality scale(Goldberg, 1999; Goldberg et al., 2006) . Also, participants were directly asked \nif they believed they had misophonia, with the options “Yes”, “No,” and “Maybe/Somewhat”.\nParticipants\nMisophonia. 45 individuals (32 females, 13 males, mean age = 34.2 years) with self-\ndiagnosed misophonia were included in this experiment. As in Experiment 1, participants with \nmisophonia needed to self-report an average response greater than or equal to a 4 out of 10 on \nthe MAS-1 to be eligible for the study. Of the 45, only one participant reported also participating\nin Experiment 1. According to a composite score that equally weighted the three misophonic \nassessment surveys, the group with misophonia had a mean misophonia level of 58.5 out of 100 \n(range = 31.1-83.9). All individuals with misophonia self-reported normal or corrected-to-normal\nvision and hearing (i.e., no hearing loss). Individuals were recruited via online misophonia \nsupport groups on Facebook, Reddit, and Yahoo!, and volunteered to participate. 297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318"
    },
    {
      "section": "Page 15",
      "page_number": 15,
      "text": "15\nMISOPHONIA SOURCE CATEGORIES \nControl. 62 individuals (24 females, 37 males, 1 non-binary, mean age = 19.9 years) \nfrom the general population also completed the experiment. One individual was excluded for not \nfaithfully responding to the surveys, leaving a sample of 61 (24 females, 36 males, 1 non-binary, \nmean age = 19.9 years). All individuals self-reported normal or corrected-to-normal vision and \nhearing (i.e., no hearing loss). All individuals were recruited from the Psychology undergraduate \nresearch pool at The Ohio State University and received course credit for their participation; \nnone of the 61 individuals participated in Experiment 1.  \nThe entire group of 61 individuals had a mean composite misophonia level of 13.9 (range\n= 0-81.3). Given that individuals with misophonia likely exist in the general population, \nparticipants were again excluded from the control analyses if they had an average self-reported \nresponse greater than or equal to a 4 out of 10 on the MAS-1 or if they answered “yes” to \nbelieving they had misophonia; the participants that remained are hereafter referred to as \n“controls”.    The control group consisted of 50 individuals (19 females, 30 males, 1 non-binary, \nmean age = 19.9 years) with a mean misophonia level of 9.5 (range = 0-26.8). Similar to \nExperiment 1, it is worth noting that 18.0% (11/61) of participants were excluded for \nexperiencing misophonia, supporting previous findings that misophonia exists in about 20% of \nthe general population (Wu et al., 2014; Zhou et al., 2017) .\nParticipant scores on the individual misophonia assessments, including a distinction of \nwhich participants from the general population were included as controls, can be found in \nSupplement S1.\nAll experimental methods were approved by The Ohio State University Institutional \nReview Board, and all participants gave informed consent to participate.   \nProcedure319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341"
    },
    {
      "section": "Page 16",
      "page_number": 16,
      "text": "16\nMISOPHONIA SOURCE CATEGORIES \nThe procedure for Experiment 2 was identical to that of Experiment 1, except for sound \npresentation and ratings questions. First, since Google provided short labels describing each \nsound, these labels were presented to participants on screen instead of the word “Listen”. This \nchange was made to eliminate the confound of participants not identifying the sounds \nappropriately, given that incorrect identification in Experiment 1 inadvertently shaped \naversiveness ratings and caused differing effects depending on the sound source category (see \nSupplement 3 and Figure S2). The labels appeared concurrently with the sound and thus \npreserved some measure of ecological validity, since individuals with misophonia normally have\nsome sort of environmental context to enable them to discern the identity of a triggering \nstimulus. Additionally, stimuli from the Google sound bank were presented using the provided \nlinks from https://developers.google.com/actions/tools/sound-library/ , which allowed for \nstop/start controls instead of the webpage automatically playing and advancing when the sound \nwas finished. Also, given that these stimuli varied in duration, participants were not forced to \nlisten to the entirety of each stimulus. Instead, participants were instructed “You do not need to \nlisten to the full sound, but please listen to enough of it that you can accurately answer both \nquestions.” To ensure participants actually played each sound, a catch sound (which did not \nmatch the labeled description) was randomly inserted three times throughout the experiment. \nParticipants were familiarized with this sound before the experiment began, and told whenever \nthey heard it to leave their ratings blank. Participants were not aware how many catch sounds \nthere were or what the corresponding labels would be. Only participants who correctly followed \nthese directions, indicating they played through each sound, were included in the analyses (N = \n45 misophonia, 61 control). 342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363"
    },
    {
      "section": "Page 17",
      "page_number": 17,
      "text": "17\nMISOPHONIA SOURCE CATEGORIES \nSecond, since sound labels were presented, participants were no longer asked to identify \nthe sound. They only gave aversiveness ratings, which included “How much discomfort did you \nfeel during the sound?” (like Experiment 1) and “How tolerable is this sound to you?”, which \nwas scaled from “extremely intolerable” (1) to “extremely tolerable” (5), with labeled steps in \nbetween. The discomfort rating from Experiment 1 better captured the aversiveness associated \nwith misophonia and distinguished individuals with misophonia from controls than the \nunpleasantness rating did, and we sought to see if sound tolerance would provide any other \nuseful distinction. However, the tolerance rating also did not meaningfully distinguish \nindividuals with misophonia from controls, suggesting the tolerance rating likewise captured \ngeneral sound aversiveness like the unpleasantness rating did. Additionally, the discomfort rating\nwas pre-registered as our main measure of interest (see below); as such, only the discomfort \nrating will be further reported here (see Figure S5 for tolerance rating results).\nPre-registration and Analyses\nMethods and analyses for Experiment 2 were pre-registered after data collection and prior to data\nanalysis on the Open Science Framework website (https://osf.io/rzgbs/). Any post hoc analyses \npresented here are clearly labeled as post hoc. A pre-registered analysis using frequency ranges \nto explain principal components of sound discomfort ratings will not be discussed here because \nresults were not easily interpretable and ultimately unrelated to the scope of the present paper; \nnevertheless, all pre-registered results can be found at https://osf.io/rzgbs/.  Predicting \nmisophonia level with source category discomfort ratings (Experiment 2 parts b and c below) \nwas a post hoc version of the pre-registered regression analyses, but was ultimately a better fit to \ninvestigate the questions of the present paper than the pre-registered analysis of regressing out \nsound frequencies.364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386"
    },
    {
      "section": "Page 18",
      "page_number": 18,
      "text": "18\nMISOPHONIA SOURCE CATEGORIES \nExperiment 2 part a.  As in Experiment 1, we used mixed ANOVAs, Student’s t-tests, \nand Pearson’s correlations to assess the differences in source categories between individuals with\nmisophonia and controls, and implemented the Holm-Bonferroni method to control the \nfamilywise Type I error rate (corrected p-values are denoted by pHB). Additionally, we used \nlinear classification and general linear regression to make predictions about group membership \nand misophonic severity given discomfort ratings.\nExperiment 2 part b: Linear Classification. For this analysis, we sought to \ndiscriminate between individuals with and without misophonia; thus, we grouped our \nparticipants into those with misophonia (N=45) vs. controls (N=50).  We randomly partitioned \nthe subjects into a training set (N=48) and a test set (N=47), with the constraint that individuals \nfrom the misophonia and control samples were evenly distributed between the two sets. We built \nfour models, using 1) all 125 sounds, 2) only sounds from Source 1 (n=28), 3) only sounds from \nSource 2 (n=64), and 4) only sounds from Source 3 (n=29). Discomfort ratings for each sound \nwere standardized first using z-scores for each model, as is standard in machine-learning. Each \nmodel used a support vector machine learning algorithm and lasso regularization, and was \nconstructed by implementing k-fold cross validation with 5 folds in the training set. The model \nthat had the smallest mean squared error was chosen as the final model and was subsequently \napplied to the independent test set of participants. This process was repeated four times using the\ndifferent sound sources, and each of these four final models was used to classify group \nmembership in the left-out sample. Model accuracy was determined by averaging how many \nindividuals in the test set were correctly labeled. Sensitivity was determined by dividing true \npositives (i.e., misophonia correctly identified) by total positives (i.e., true positives + false \nnegatives [misophonia identified as control]). Specificity was determined by dividing true 387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409"
    },
    {
      "section": "Page 19",
      "page_number": 19,
      "text": "19\nMISOPHONIA SOURCE CATEGORIES \nnegatives (i.e., control correctly identified) by total negatives (i.e., true negatives + false \npositives [control identified as misophonia]).\nExperiment 2 part c: General Linear Regression.  For the regression analyses, we \nsought to predict total misophonia level, and thus combined data from both individuals with \nmisophonia and the general population (total N=106) to obtain the widest variability in \nmisophonia scores. We first used stepwise regression, with each individual’s total misophonia \nlevel as the response variable and each individual’s discomfort rating of the 125 sounds as \npredictor variables. Both the predictor and the response variables were standardized using z-\nscores. We used a criterion of including only predictors that minimized the sum of squared error \nin each model. To cross-validate the models, each linear regression model was constructed using \nN-1 participants. The model was then used to predict the total misophonia level of the left-out \nparticipant, given that participant’s discomfort ratings. Thus, we used cross-validation \nprocedures to expand generalizability of these predictions and deduce the most common sound \npredictors retained across models. A final model was then built to identify the most predictive \nsounds using all 106 participants, after taking into account individual differences in demographic\nmeasures (i.e., gender and age) and clinical scores (i.e., from OCI-R and DASS-21).\nResults\nExperiment 2 part a:  Using a 2 (group: misophonia vs. control, between-subjects) x 3 \n(source category: 1 [human oral/nasal] vs. 2 [human non-oral/nasal] vs. 3 [nonhuman/nature], \nwithin-subjects) mixed ANOVA, a group x source category interaction was assessed for the \ndiscomfort rating of interest. Analysis comprising just the 30 sounds previously used in \nExperiment 1 replicated the results of Experiment 1, as did extension of the analysis to include 410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431"
    },
    {
      "section": "Page 20",
      "page_number": 20,
      "text": "20\nMISOPHONIA SOURCE CATEGORIES \nall sounds categorized from the sound bank. Statistics and figures from these analyses can be \nfound in Supplements 4-5.\nOn average, individuals with misophonia rate sounds from Source 1 or Source 2 as \nevoking more discomfort than do controls. Can knowing how an individual rates a particular \nsound (or group of sounds) be used to predict misophonia or how severe an individual’s \nmisophonia is? \nExperiment 2 part b:  Linear classification.  We sought to explore whether we could \nclassify an individual as being from the misophonia or control group, based off their discomfort \nratings for particular sounds. We constructed four classification models on a training set of \nparticipants and applied the final models to each individual of the test set to get a predicted \nclassification. Classification accuracy was 0.89 using all sounds (sensitivity: 0.77, specificity: \n1.0),  0.81 using Source 1 sounds only (sensitivity: 0.73, specificity: 0.88), 0.77 using Source 2 \nsounds only (sensitivity: 0.68, specificity: 0.84), and 0.81 using Source 3 sounds only \n(sensitivity: 0.82, specificity: 0.80). To further probe the significance of these results, we used \npermutation testing, randomly shuffling group membership labels 1000 times and calculating \nclassification accuracy each time. The null distributions for each model can be found in \nSupplement 6A (Figure S9). Compared to the null distributions, each model could significantly \nclassify individuals with misophonia from controls (all sounds: p < 0.001, Source 1 sounds: p < \n0.001, Source 2 sounds: p < 0.001, Source 3 sounds: < 0.001). The top five most informative \nsounds in discriminating individuals with misophonia versus controls are depicted in Table 1. \nFor an illustration of the top fifty sounds and a ranking of how each group rated the individual \nsounds on average, see Figure S10 and Figure S8A, respectively. 432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453"
    },
    {
      "section": "Page 21",
      "page_number": 21,
      "text": "21\nMISOPHONIA SOURCE CATEGORIES \nExperiment 2 part c: Generalized Linear Regression. In addition to binary \nclassification, we asked if we could predict an individual’s severity of misophonia (i.e., their \nmean misophonia level from the three assessment surveys). First, we used a linear regression \nmodel with stepwise regression and discomfort ratings to all 125 sounds individually as predictor\nvariables in a leave-one-out cross-validation process. Results of the predictions are shown in \nFigure 3. Misophonia level was significantly predicted using a subset of individual sounds (r = \n0.401, p = 2.01 x 10-5). Although each cross-validation model was slightly different (because it \nincluded data from a slightly different group of participants), certain sounds were consistently \nincluded in over 80% of the models (Table 2). \nTo better understand the contribution of each predictor, we generated a final model using \nall subjects. Additionally, to account for demographic and clinical differences between the \nsubjects, we included 6 nuisance regressors: age, gender, OCD level (from OCI-R), and levels of\ndepression, anxiety, and stress (from DASS-21). A stepwise regression model built on the 125 \nsounds significantly predicted the residual misophonia level after regressing out the nuisance \nvariables (model fit: R2 = 0.872, F(22,74) = 22.9, p = 1.2 x 10-24). Again using a criterion to \nminimize the sum of squared error, 16 sounds were retained in the model (Table 3). It is \nimportant to note that we explored the collinearity of these sounds in two ways (see Supplement \n5B, Figures S7 and S8B) and found that discomfort ratings to sounds within a source category \nwere more correlated with each other than with sounds across source categories, particularly for \nsounds in Source 1 and Source 2. Interestingly, however, our results show that the stepwise \nregression models consistently chose sounds across all three source categories rather than simply\nhuman oral/nasal sounds, indicating that significant variance was explained by incorporating \nhuman non-oral/nasal and nonhuman/nature sounds too. Thus, we offer 16 sounds that could be 454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476"
    },
    {
      "section": "Page 22",
      "page_number": 22,
      "text": "22\nMISOPHONIA SOURCE CATEGORIES \nused to predict misophonia but acknowledge that another subset of sounds could be used as long \nas they span all three source categories.\nFor additional linear regression models built using average discomfort rating to sounds of\neach source as a single predictor or in separate models with more stringent criteria, which \nshowed similar results as the analyses above, see Supplement 6B (Figure S11) and Supplement \n6C (Figures S12-S13).\nDiscussion\nExperiment 2 sought to replicate the effects found in Experiment 1 and extend the \nfindings to a larger sound bank, distinguish between individuals with misophonia vs. controls \nusing discomfort ratings, and predict misophonia severity using machine learning. In a separate \ngroup of individuals with misophonia and controls, the same 30 sounds used in Experiment 1 led\nto the same group differences between each of the three sources categories as well as the same \ncorrelations between discomfort and misophonia level, and a larger sound bank additionally \nsupported these observed effects (see Supplement 4). \nFurther, machine learning approaches using multiple different methods showed that all \nsound sources could be used to significantly predict an individual’s misophonia; incorporating \ninformation from all three sources produced the best-fitting prediction models as determined by \nindependent test sets. In addition, stepwise regressions consistently chose sounds from all three \nsources as the most informative predictors of misophonia. Although certain human oral/nasal or \nhuman non-oral/nasal sounds may have been left out of the final models due to collinearity, these\nanalyses give confidence that the most predictive subset of sounds broadly contains sounds from \nall sources. Thus, narrowing our interpretation of misophonia to discomfort for just oral/nasal \nsounds is insufficient, as the condition seems to extend farther than just oral/nasal sounds. 477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499"
    },
    {
      "section": "Page 23",
      "page_number": 23,
      "text": "23\nMISOPHONIA SOURCE CATEGORIES \nPerhaps this large database of sounds can be tailored to individuals in future experiments who \nexperience diverse misophonic triggers (as suggested by Schröder et al., 2019), as long as the \ninclusion of sounds spanning all three source categories is prioritized.\nImportantly, discomfort ratings to each of the sound sources explains unique variance in \nan individual’s overall misophonia level, distinct from the variance that demographic or clinical \nmeasures can account for alone. In other words, if an individual’s experience of misophonia was \nprimarily driven by their level of OCD or their age, for instance, then using the sound discomfort\nratings as predictors after demographic and clinical measures were regressed out would not have \nyielded significant results. This finding may have a few theoretical implications about the \nexperience of misophonia: 1) it is likely clinically distinct from that of OCD, depression, anxiety,\nor stress; and 2) it cannot be fully explained by age or gender. \nAdditionally, given that misophonia is a sound-based disorder and research is still \nsomewhat nascent, investigating different sound categories seems highly relevant. As detailed in \nSupplement 5A, we assigned each of the 125 sounds to one sub-category that it best fit (i.e., \nambiences, animals, babies, footsteps, household, kitchen, metal, nature, office, oral/nasal, \noutdoors, paper/plastic, rubbing/wiping, water) and explored average discomfort ratings across \nparticipants to each sub-category (Figure S6). After splitting the sounds into finer-grained \ncategory labels, the discomfort ratings we obtained in this experiment objectively corroborate, \nfor the first time to our knowledge, self-reported sound triggers from anecdotal case studies and \nquestionnaires. For instance, individuals with misophonia often report being bothered by pen-\nclicking or glasses clinking (e.g., Edelstein et al., 2013; Taylor, 2017) , and this analysis showed \nsignificantly more discomfort for sounds classically heard in an office or kitchen, such as these. \nIt is important to note that individuals with misophonia do not have generalized higher 500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522"
    },
    {
      "section": "Page 24",
      "page_number": 24,
      "text": "24\nMISOPHONIA SOURCE CATEGORIES \ndiscomfort for all sounds; if so, all categories would have showed significant differences \nbetween individuals with misophonia and controls. However, when correcting for multiple \ncomparisons, individuals with misophonia were no different from controls in their responses to \nanimals or babies, which upholds self-reported responses from previous literature that sounds \nfrom animals and babies are not as bothersome as the same sounds from adult humans (Edelstein \net al., 2013). Likewise, nature sounds didn’t bother individuals with misophonia, suggesting \nthere might be something more specific about the repetitiveness of the sound, or the need to \nattribute the sound to a culpable human, that produces the negative reaction.\nGeneral Discussion\nThe present experiments investigated whether constraining misophonia to aversion for \nhuman-produced oral/nasal sounds is an empirically justified stipulation. More specifically, we \nused two independent samples of individuals with misophonia and controls – as well as two \nunique stimulus sets – to show that the discomfort that individuals with misophonia felt differed \nfrom that of controls for both human-produced non-oral/nasal sounds and even nonhuman/nature\nsounds. Additionally, to the best of our knowledge, these experiments are the first to objectively \nshow differences in discomfort to sounds from finer grained categories, including sounds from \nthe office, kitchen, or general household as well as paper/plastic, water, and metal sounds. These \nfindings not only corroborate case studies anecdotally describing nonhuman and/or \nnon-oral/nasal sounds as bothersome, but, given that not all categories showed differences \nbetween individuals with misophonia and controls, also emphasize that misophonia is \ncharacterized by specific source category or sound aversions – not general sound annoyance. \nWhereas prior case studies have explored the relationship between misophonia and eating \ndisorders as a way to put oral/nasal triggers into context (Kluckow et al., 2014) , our results 523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545"
    },
    {
      "section": "Page 25",
      "page_number": 25,
      "text": "25\nMISOPHONIA SOURCE CATEGORIES \nsuggest additional information may be gleaned about an individual’s misophonia onset or \ntriggers by probing their experiences in other contexts, such as office or home life.  \nFurther, perhaps more convincingly, Experiment 2 introduces machine learning methods to \nparse what the most influential predictors of misophonia classification and/or severity are. \nSource 1 (human-produced oral/nasal) and Source 2 (human-produced non-oral/nasal) sounds \nconsistently provided significant predictions of misophonia, and Source 3 (nonhuman/nature) \nsounds also significantly contributed to predictions using separate training and test sets. \nClassification accuracy was significant and comparably high when incorporating discomfort for \nall sounds, as well as each sound source separately. Finally, a model constructed on all subjects \nand sounds shed light on the 16 most influential sound predictors in this data set – majority of \nwhich are not Source 1 sounds. Taken together, these analyses make it clear that sounds from all \nsource types can be used to identify misophonia, and constraining the condition to primarily \nhuman-produced oral/nasal sounds misses out on important distinctions between individuals with\nmisophonia and healthy individuals for other types of sounds. \nHowever, thus far, most experimental investigations into misophonia have done just that \n– designed paradigms that seemingly constrain misophonia to human-produced oral/nasal \nsounds. For instance, although Seaborne and Fiorella (Seaborne & Fiorella, 2018)  objectively \ndemonstrated daily impairments that individuals with misophonia face, which is beneficial, the \nexperiment only presented one possible misophonic trigger – gum chewing – ignoring the effects\nthat other background sounds in the study environment (e.g., writing, pen clicking, papers \nrustling, etc.) might have had. Further, Kumar and colleagues (Kumar et al., 2017)  published \ngroundbreaking findings using a combination of neuroimaging, physiological measurements, and\nbehavioral ratings to probe aversion to misophonia triggering sounds, generally unpleasant 546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568"
    },
    {
      "section": "Page 26",
      "page_number": 26,
      "text": "26\nMISOPHONIA SOURCE CATEGORIES \nsounds, and neutral sounds. However, this study specifically recruited only individuals with \naversion to eating, breathing, and chewing sounds, which inherently limits the generalizability to \nindividuals who have primarily other triggers. Similarly, a neuroimaging study from Schröder \nand colleagues(Schröder et al., 2019)  presented misophonic video clips, generally aversive clips \n(i.e., segments of violent or loathsome scenes from commercial films), and neutral clips (i.e., a \nmale actor performing soundless activities) to their participants while in the scanner. Although \nusing video stimuli is more ecologically valid, their misophonic sounds were likewise mainly \noral/nasal and compared against a neutral baseline that lacked an auditory component, \nconfounding comparisons and making conclusions about brain regions associated with \nmisophonia (i.e., auditory cortex) uncertain.   Lastly, although Edelstein and \ncolleagues(Edelstein et al., 2013)  avoided the human-produced oral/nasal constraint in their \nexploratory investigation of misophonia, the chosen auditory stimuli intentionally covered a wide\nrange of content (i.e., more than just commonly reported trigger sounds, e.g., birds singing, \nchildren laughing, whale song), and conclusions are drawn combining all sounds together.\n As a whole, previous work has suggested that the primary deficit in misophonia is an \naversion to human-produced oral/nasal sounds. Here, we propose that a) individuals with \nmisophonia can include those with aversions to other types of sounds and these individuals \nshould be included in future misophonia studies, and b) future experiments include a wider range\nof auditory stimuli that include other types of sounds (i.e., not only oral/nasal sounds).  \nConstraining misophonia to certain sounds limits generalizability of experiments, and minimizes \nexperiences of individuals who do not identify with these triggers. This can have negative \nconsequences, such as failing to diagnose individuals who do not fit one’s narrow guidelines of 569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590"
    },
    {
      "section": "Page 27",
      "page_number": 27,
      "text": "27\nMISOPHONIA SOURCE CATEGORIES \nwhat misophonia is; should misophonia be added to the DSM, its diagnostic criteria should not \nrequire human-produced oral/nasal sounds be the only and/or most prominent trigger. \nNevertheless, this work has a few limitations. First, our two samples of participants are not \nage-matched. Recruitment was approached differently for individuals with misophonia versus \ncontrols, and thus produced samples that varied in age. Although it is unlikely that the results \nfrom these experiments are solely due to the sample with misophonia merely being older, age \ncannot be ruled out as a contributing factor. Additionally, data presented in this paper are drawn \nfrom self-report behavioral ratings, which are inherently prone to response bias. One might \nworry that individuals with misophonia may rate all sounds (or oral/nasal sounds) as higher out \nof obligation; however, a handful of individuals commented in follow-up questions that they \nconsidered themselves to have severe misophonia but were not bothered by the particular stimuli \nof the experiment, and thus rated them low on aversiveness accordingly. Individuals with \nmisophonia also self-reported their misophonic severity; given that they were recruited and \ntested online, they could not be clinically assessed. Still, three misophonic assessments were \nused, each with different questions and probing misophonic experiences in different ways, so we \nbelieve that the aggregated misophonia severity levels are as authentic as can be obtained via \nself-report. Granted, determining how to objectively quantify what counts as misophonia, via \nself-reported assessments or clinical diagnoses, is crucial. How should we determine who has \nmisophonia, and what types of sounds qualify as misophonic trigger sounds? With a narrow \ndefinition of what sounds are triggering, fewer individuals will be classified as having \nmisophonia, and vice versa. However, we believe a broader view of the types of sounds that \ncould be triggering is necessary for research moving forward.591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612"
    },
    {
      "section": "Page 28",
      "page_number": 28,
      "text": "28\nMISOPHONIA SOURCE CATEGORIES \nOverall, results from the data presented here help emphasize more generally the vast \ndifferences that exist in experienced discomfort to all types of sound stimuli in individuals with \nmisophonia, compared to controls. This helps validate the disorder quantitatively, offers \nsupporting evidence for the inclusion of misophonia as a legitimate disorder, and emphasizes the \nneed to expand our definition of misophonic trigger sounds.\nCompeting interests\nThe authors declare no competing interests.\nFinancial support\nThe authors received no funding from an external source.\nOpen Practices Statement \nData are available upon request. Methods and analyses for Experiment 2 were pre-registered, and\nall pre-registered analyses not included in the present manuscript can be found at \nhttps://osf.io/rzgbs/.  613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625"
    },
    {
      "section": "Page 29",
      "page_number": 29,
      "text": "29\nMISOPHONIA SOURCE CATEGORIES \nReferences\nAmerican Psychiatric Association. (2013).  Diagnostic and statistical manual of mental \ndisorders (5th ed.). https://doi.org/10.1176/appi.books.9780890425596\nBrout, J. J., Edelstein, M., Erfanian, M., Mannino, M., Miller, L. J., Rouw, R., Kumar, S., & \nRosenthal, M. Z. (2018). Investigating misophonia: A review of the empirical literature, \nclinical implications, and a research agenda. Frontiers in Neuroscience , 12. \nhttps://doi.org/10.3389/fnins.2018.00036\nDozier, T. H., Lopez, M., & Pearson, C. (2017). Proposed Diagnostic Criteria for Misophonia: A\nMultisensory Conditioned Aversive Reflex Disorder. Frontiers in Psychology , 8, 1–3. \nhttps://doi.org/10.3389/fpsyg.2017.01975\nEdelstein, M., Brang, D., Rouw, R., & Ramachandran, V. S. (2013). Misophonia: physiological \ninvestigations and case descriptions. Frontiers in Human Neuroscience , 7(June), 296. \nhttps://doi.org/10.3389/fnhum.2013.00296\nFerreira, G. M., Harrison, B. J., & Fontenelle, L. F. (2013). Hatred of sounds: Misophonic \ndisorder or just an underreported psychiatric symptom? Annals of Clinical Psychiatry , \n25(4), 271–274.\nFitzmaurice, G. (2014). Misophonia Activation Scale .\nFoa, E. B., Huppert, J. D., Leiberg, S., Langner, R., Kichic, R., Hajcak, G., & Salkovskis, P. M. \n(2002). The obsessive-compulsive inventory: Development and validation of a short \nversion. Psychological Assessment , 14(4), 485–496. https://doi.org/10.1037/1040-\n3590.14.4.485\nGoldberg. (1999). A broad-bandwidth, public domain, personality inventory. In Personality \nPsychology in Europe  (Vol. 7, pp. 7–28).\nGoldberg, L. R., Johnson, J. A., Eber, H. W., Hogan, R., Ashton, M. C., Cloninger, C. R., & \nGough, H. G. (2006). The international personality item pool and the future of public-\ndomain personality measures. Journal of Research in Personality , 40(1), 84–96. \nhttps://doi.org/10.1016/j.jrp.2005.08.007\nHadjipavlou, G., Baer, S., Lau, A., & Howard, A. (2008). Selective sound intolerance and \nemotional distress: what every clinician should hear. Psychosomatic Medicine , 70, 739–740.\nhttps://doi.org/10.1097/PSY.0b013e318180edc2\nHalpern, D. L., Blake, R., & Hillenbrand, J. (1986). Psychoacoustics of a chilling sound. \nPerception & Psychophysics , 39(2), 77–80. https://doi.org/10.3758/BF03211488\nHolm, S. (1979). A Simple Sequentially Rejective Multiple Test Procedure. Scandinavian \nJournal of Statistics , 6(2), 65–70.\nJastreboff, M. M., & Jastreboff, P. J. (2002). Decreased sound tolerance and tinnitus retraining \ntherapy (TRT). Australian and New Zealand Journal of Audiology , 24(2), 74–84. \nhttps://doi.org/10.1375/audi.24.2.74.31105\nJastreboff, P. J., & Jastreboff, M. M. (2014). Treatments for decreased sound tolerance \n(hyperacusis and misophonia). Seminars in Hearing , 35(2), 105–120. \nhttps://doi.org/10.1055/s-0034-1372527\nJohnson, M., & Dozier, T. (2013). Misophonia Assessment Questionnaire (MAQ) .\nJohnson, P. L., Webber, T. A., Wu, M. S., Lewin, A. B., & Murphy, T. K. (2013). When \nselective audiovisual stimuli become unbearable: a case series on pediatric misophonia. \nNeuropsychiatry, 3(6), 569–575.\nKluckow, H., Telfer, J., & Abraham, S. (2014). Should we screen for misophonia in patients with626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670"
    },
    {
      "section": "Page 30",
      "page_number": 30,
      "text": "30\nMISOPHONIA SOURCE CATEGORIES \neating disorders? A report of three cases. International Journal of Eating Disorders , 47(5), \n558–561. https://doi.org/10.1002/eat.22245\nKumar, S., Tansley-Hancock, O., Sedley, W., Winston, J. S., Callaghan, M. F., Allen, M., Cope, \nT. E., Gander, P. E., Bamiou, D. E., & Griffiths, T. D. (2017). The Brain Basis for \nMisophonia. Current Biology, 27(4), 527–533. https://doi.org/10.1016/j.cub.2016.12.048\nLovibond, P. F., & Lovibond, S. H. (1995). The structure of negative emotional states: \nComparison of the Depression Anxiety Stress Scales (DASS) with the Beck depression and \nanxiety inventories. Behavioral Research and Therapy , 33(3), 335–343. \nhttps://doi.org/10.1007/BF02511245\nNeal, M., & Cavanna, A. E. (2013). Selective sound sensitivity syndrome (misophonia) in a \npatient with Tourette syndrome. The Journal of Neuropsychiatry and Clinical \nNeurosciences, 25(1), E01. https://doi.org/10.1176/appi.neuropsych.11100235\nNorman-Haignere, S., Kanwisher, N. G., & McDermott, J. H. (2015). Distinct Cortical \nPathways for Music and Speech Revealed by Hypothesis-Free Voxel Decomposition . 88(6), \n1281–1296. https://doi.org/10.1016/j.neuron.2015.11.035.Distinct\nRouw, R., & Erfanian, M. (2017). A Large-Scale Study of Misophonia. Journal of Clinical \nPsychology, 0(0), 1–27. https://doi.org/10.1002/jclp.22500\nSchröder, A., van Wingen, G., Eijsker, N., San, R., Vulink, N. C., Turbyne, C., & Denys, D. \n(2019). Misophonia is associated with altered brain activity in the auditory cortex and \nsalience network. Scientific Reports, 9(7542), 1–9. https://doi.org/10.1038/s41598-019-\n44084-8\nSchröder, A., Vulink, N., & Denys, D. (2013). Misophonia - Diagnostic Criteria for a New \nPsychiatric Disorder. PLoS ONE, 8(1). https://doi.org/10.1371/ journal.pone.0054706\nSeaborne, A., & Fiorella, L. (2018). Effects of background chewing sounds on learning: The role\nof misophonia sensitivity. Applied Cognitive Psychology , 32(2), 264–269. \nhttps://doi.org/10.1002/acp.3387\nTaylor, S. (2017). Misophonia: A new mental disorder? Medical Hypotheses , 103, 109–117. \nhttps://doi.org/10.1016/j.mehy.2017.05.003\nWebber, T. A., Johnson, P. L., & Storch, E. A. (2014). Pediatric misophonia with comorbid \nobsessive-compulsive spectrum disorders. General Hospital Psychiatry , 36(2), 231.e1-\n231.e2. https://doi.org/10.1016/j.genhosppsych.2013.10.018\nWoods, K. J. P., Siegel, M. H., Traer, J., & McDermott, J. H. (2017). Headphone screening to \nfacilitate web-based auditory experiments. Attention, Perception, & Psychophysics . \nhttps://doi.org/10.3758/s13414-017-1361-2\nWu, M. S., Lewin, A. B., Murphy, T. K., & Storch, E. A. (2014). Misophonia: Incidence, \nphenomenology, and clinical correlates in an undergraduate student sample. Journal of \nClinical Psychology , 70(10), 994–1007. https://doi.org/10.1002/jclp.22098\nZhou, X., Wu, M. S., & Storch, E. A. (2017). Misophonia symptoms among Chinese university \nstudents: Incidence, associated impairment, and clinical correlates. Journal of Obsessive-\nCompulsive and Related Disorders , 14, 7–12. https://doi.org/10.1016/j.jocrd.2017.05.001671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712"
    },
    {
      "section": "Page 31",
      "page_number": 31,
      "text": "31\nMISOPHONIA SOURCE CATEGORIES \nList of Figures:\nFigure 1. Mean Aversiveness Ratings by Source Category . Blue = Source 1, yellow = Source 2,\ngreen = Source 3. Dark bars = individuals with misophonia, light bars = controls. Error bars \ndepict standard error of the mean. Significance only shown for between group differences. \n*=<0.05, **=<0.01,  ***=<0.001\nFigure 2. Average discomfort rating for each source category compared to total misophonia \nlevel. Scatterplots show individual participants from the misophonia sample (red, N=48) and \ngeneral population (gray, N=39). Red solid line shows lines of best fit among individuals with \nmisophonia only, gray solid line shows line of best fit among general population only, and black \ndashed line shows line of best fit collapsed across all subjects (N=87).\nFigure 3. Actual Misophonia Level vs. Predicted Misophonia Level.  Scatterplots show \nindividual participants from the misophonia sample (red, N=45) and general population (gray, \nN=61). Black solid line shows line of best fit collapsed across all subjects (N=106). Black \ndashed lines represent a 95% confidence interval. Regressors included ratings for all 125 \nsounds individually, keeping only the predictors that were significant. Predictions made using a \ncross-validated approach. Significant predictors differed for each model. 713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729"
    }
  ]
}