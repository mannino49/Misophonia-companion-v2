{
  "doc_type": "scientific paper",
  "title": "Bottom up and top down factors differentially influence stimuls representations across large scale attentional networks",
  "authors": [
    "Long"
  ],
  "year": 2018,
  "journal": "Psychology, University of Oregon, Eugene, Oregon 97402",
  "doi": "10.1523/JNEUROSCI.2724-17.2018",
  "abstract": null,
  "keywords": [
    "attention",
    "cognitive control",
    "decoding",
    "parietal cortex",
    "prefrontal cortex",
    "resting-state networks"
  ],
  "research_topics": [
    "attention",
    "cognitive control",
    "decoding",
    "parietal cortex",
    "prefrontal cortex",
    "resting-state networks"
  ],
  "created_at": "2025-05-05T02:16:03.847781Z",
  "source_pdf": "documents/research/Global/Long 2018 Bottom up and top down factors differentially influence stimuls representations across large scale attentional networks.pdf",
  "sections": [
    {
      "section": "Page 1",
      "page_number": 1,
      "text": "Behavioral/Cognitive\nBottom-Up and Top-Down Factors Differentially Influence\nStimulus Representations Across Large-Scale AttentionalNetworks\nX\nNicole M. Long and X\nBrice A. Kuhl\nDepartment of Psychology, University of Oregon, Eugene, Oregon 97402\nVisual attention is thought to be supported by three large-scale frontoparietal networks: the frontoparietal control network (FPCN), the\ndorsal attention network (DAN), and the ventral attention network (VAN). The traditional view is that these networks support visualattention by biasing and evaluating sensory representations in visual cortical regions. However, recent evidence suggests that frontopa-rietal regions actively represent perceptual stimuli. Here, we assessed how perceptual stimuli are represented across large-scale fronto-parietal and visual networks. Specifically, we tested whether representations of stimulus features across these networks are differentiallysensitive to bottom-up and top-down factors. In a pair of pattern-based fMRI studies, male and female human subjects made perceptualdecisions about face images that varied along two independent dimensions: gender and affect. Across studies, we interrupted bottom-upvisual input using backward masks. Within studies, we manipulated which stimulus features were goal relevant (i.e., whether gender oraffect was relevant) and task switching (i.e., whether the goal on the current trial matched the goal on the prior trial). We found thatstimulus features could be reliably decoded from all four networks and, importantly, that subregions within each attentional networkmaintained coherent representations. Critically, the different attentional manipulations (interruption, goal relevance, and task switch-ing) differentially influenced feature representations across networks. Whereas visual interruption had a relatively greater influence onrepresentations in visual regions, goal relevance and task switching had a relatively greater influence on representations in frontoparietalnetworks. Therefore, large-scale brain networks can be dissociated according to how attention influences the feature representations thatthey maintain.\nKey words: attention; cognitive control; decoding; parietal cortex; prefrontal cortex; resting-state networks\nIntroduction\nVisual attention is thought to be supported by several frontopa-\nrietal networks ( Posner and Petersen, 1990 ;Corbetta and Shul-man, 2002 ;Dosenbach et al., 2008 ). The idea that the brain is\ncomprised of multiple functional networks has been inspired andelaborated by resting-state analyses of human fMRI data (\nYeo et\nal., 2011 ), which reveal three networks of particular importance\nto attentional control: the frontoparietal control network(FPCN), the dorsal attention network (DAN), and the ventral\nReceived Sept. 20, 2017; revised Jan. 18, 2018; accepted Jan. 24, 2018.\nAuthor contributions: N.M.L. and B.A.K. designed research; N.M.L. performed research; N.M.L. analyzed data;\nN.M.L. and B.A.K. wrote the paper.\nThis work was supported by the Lewis Family Endowment to the University of Oregon, which supports the Robert\nand Beverly Lewis Center for NeuroImaging and by the National Institute of Neurological Disorders and Stroke–National Institutes of Health (Grant 1RO1NS089729 to B.A.K.). We thank Rosalie Samide and Sarah Sweigart forassistance with data collection.\nThe authors declare no competing financial interests.Correspondence should be addressed to either Nicole Long or Brice Kuhl, Department of Psychology, 1227 Uni-\nversity of Oregon Eugene, OR 97403. E-mail:\nniclong@uoregon.edu orbkuhl@uoregon.edu .\nDOI:10.1523/JNEUROSCI.2724-17.2018\nCopyright © 2018 the authors 0270-6474/18/382495-10$15.00/0Significance Statement\nVisual attention is supported by multiple frontoparietal attentional networks. However, it remains unclear how stimulus features\nare represented within these networks and how they are influenced by attention. Here, we assessed feature representations in fourlarge-scale networks using a perceptual decision-making paradigm in which we manipulated top-down and bottom-up factors.We found that top-down manipulations such as goal relevance and task switching modulated feature representations in atten-tional networks, whereas bottom-up manipulations such as interruption of visual processing had a relatively stronger influence onfeature representations in visual regions. Together, these findings indicate that attentional networks actively represent stimulusfeatures and that representations within different large-scale networks are influenced by different forms of attention.The Journal of Neuroscience, March 7, 2018 •38(10):2495–2504 • 2495"
    },
    {
      "section": "Page 2",
      "page_number": 2,
      "text": "attention network (VAN). Traditionally, these networks have\nbeen thought to support visual attention by biasing and evaluat-ing sensory representations within visual cortical areas (\nDesi-\nmone and Duncan, 1995 ;Egner and Hirsch, 2005 ;Serences and\nYantis, 2006 ;Gazzaley and Nobre, 2012 ). However, recent evi-\ndence from pattern-based fMRI studies has blurred the distinc-tion between sensory representations in visual cortical areas andcontrol processes in frontoparietal regions. Namely, there is ac-cumulating evidence that frontoparietal regions actively repre-sent stimulus features during visual attention and workingmemory (\nEster et al., 2015 ;Lee and Kuhl, 2016 ;Xu, 2017 ). These\nfindings suggest a potentially transformative approach for under-standing the functional role of frontoparietal networks in visualattention: that frontoparietal networks can be characterized –and dissociated from visual cortical regions – in terms of howthey represent stimuli in relation to bottom-up and top-downfactors.\nAttention manipulations that may dissociate stimulus repre-\nsentations in frontal and parietal cortices from those in visualcortex include robustness to interruption of visual processing,goal relevance, and task switching. For example, working mem-ory representations in the intraparietal sulcus (\nBettencourt and\nXu, 2016 ) and prefrontal cortex ( Miller et al., 1996 ) are more\nrobust to distraction than are representations in visual corticalareas. Likewise, multiple frontal and parietal regions preferen-tially represent goal-relevant stimulus information, as shown viaelectrophysiological recordings in monkeys (\nRainer et al., 1998 ;\nSwaminathan and Freedman, 2012 ;Roy et al., 2014 ;Sarma et al.,\n2016 ) and pattern-based fMRI in humans ( Kuhl et al., 2013 ;\nSreenivasan et al., 2014 ;Ester et al., 2015 ;Bracci et al., 2017 ).\nFinally, dorsal frontal and parietal regions show increased uni-variate activation on trials when goals change (task switching)(\nBraver et al., 2003 ;Monsell, 2003 ;Yeung et al., 2006 ;Ravizza and\nCarter, 2008 ;Bode and Haynes, 2009 ;Esterman et al., 2009 ).\nCollectively, these findings suggest that different forms of atten-tion potentially differentiate stimulus representations acrossfrontoparietal and visual regions. However, there has been lim-ited application of “representation-based” analyses to large-scalenetworks. Do frontoparietal networks actively and coherentlyrepresent stimulus information? Are representations in differentnetworks influenced by different forms of attention?\nHere, we conducted a pair of pattern-based fMRI studies to\ndetermine how various attention-related manipulations (inter-ruption of visual processing, goal relevance, and task switching)influence feature representations in frontoparietal attentionalnetworks (FPCN, DAN, and VAN) and, as a comparison, withina network of visual regions (VisN). In both studies, subjectsviewed faces that varied along two independent dimensions: gen-der (male vs female) and affect (happy vs grumpy). On each trial,subjects made a perceptual decision related to a cued face feature(e.g., “Male?”). Using pattern classification analyses, we firsttested for representation of stimulus features within each net-work and, importantly, tested whether frontoparietal regionswithin a common network maintained “coherent” representa-tions. Next, we determined how each attention manipulationinfluenced feature representations across networks. Visual inter-ruption was manipulated across studies: in Study 1, stimuli werefollowed by a visual mask; in Study 2, there was a longer stimulusduration and no mask. Goal relevance was manipulated by vary-ing the dimension (gender/affect) that was currently relevant.Finally, task switching was manipulated by alternating betweengoals on a trial-by-trial manner, resulting in trials in which goalsrepeated (stay trials) and trials in which goals changed (switchtrials). We predicted that frontoparietal networks would ac-\ntively and coherently represent stimulus features and thatfrontoparietal representations would be relatively more sensitive\nto top-down manipulations (goals, task switching), whereas rep-resentations in VisN would be relatively more sensitive tobottom-up manipulations (interruption).\nMaterials and Methods\nSubjects\nThirty-two (19 female; mean age /H1100522 years) right-handed, native Eng-\nlish speakers from the University of Oregon community participated inthe fMRI studies. Sixteen subjects participated in Experiment 1 and 16participated in Experiment 2. Four total subjects were excluded. In Ex-periment 1, one subject was excluded for poor task performance (65%accuracy, which was /H110224 SDs away from the average performance of 95%)\nand one was excluded for excessive head motion. In Experiment 2, twosubjects were excluded for exiting the scanner before task completion(one subject complained of nausea and the other began coughing repeat-edly; each missing two of six runs). Therefore, for each experiment, therewas a final set of 14 subjects included for analyses. All subjects had nor-mal or corrected-to-normal vision. Informed consent was obtained inaccordance with the University of Oregon Institutional Review Board.The raw, de-identified data and the associated experimental and analysiscodes used in this study can be accessed via the Kuhl laboratory website(\nhttp://kuhllab.com/publications/ ).\nAn additional 14 (5 female; mean age /H1100519) right-handed, native\nEnglish speakers from the University of Oregon community participatedin a follow-up behavioral study. All subjects had normal or corrected-to-normal vision. Informed consent was obtained in accordance with theUniversity of Oregon Institutional Review Board and subjects receivedcourse credit for participating.\nMaterials\nStimuli consisted of 88 face images drawn from various internet sources.From this pool, 16 faces were designated as “target” faces, 24 as “filler”faces, and 48 as “localizer” faces. Within each set (target, filler, localizer)gender and affect were balanced: 1/4 of the faces were “happy, males”; 1/4were “happy, females”; 1/4 were “grumpy, males”; and 1/4 were“grumpy, females.” Gender and affect were determined, with unanimousagreement, by two independent raters.\nExperimental design and statistical analysis\nProcedure and design . Each trial began with the presentation of one of\nfour “goal cues” or questions: “Male?,” “Female?,” “Happy?,” or“Grumpy?” The goal was presented for 1400 ms (\nFig. 1 A) and was im-\nmediately followed by a face stimulus. In Experiment 1, the face waspresented for 100 ms and immediately followed by a visual mask com-posed of scrambled face parts for 500 ms. In Experiment 2, the face waspresented for 600 ms with no mask. After the mask (Experiment 1) orface (Experiment 2), there wa sa6s interstimulus interval (ISI) during\nwhich a fixation cross was shown. On each trial, the subject’s task was torespond “Yes” or “No” via button box as to whether the face matched thegoal/question. Subjects could respond at any point after the onset of theface, including during the ISI.\nTrials were organized into blocks. Each block contained 17 trials, with\nthe first trial in each block representing a “filler” face. The designation ofthe first trial as a filler face was particularly important for analyses of“stay” versus “switch” trials because the first trial cannot be designated aseither a stay or switch trial (see below). The stimuli for the remaining 16trials in each block were target faces. Each of the 16 target faces appearedexactly once per block. Within each block, only two of the four goalswere presented (either “Male?” or “Female?” or either “Happy?” or“Grumpy?”). The goals alternated in an A-A-B-B-A-A-B-B manner (\nFig.\n1C). Therefore, there was a constant alternation between switch trials\n(goal change) and stay trials (goal repeat) .A3sg e t ready screen at the\nstart of each block informed subjects which two goals would occur withinthat block.\nEach of the six scan runs contained four blocks. The four blocks in each\nrun comprised each of the four possible combinations of goals (Male/2496 •J. Neurosci., March 7, 2018 •38(10):2495–2504 Long and Kuhl •Stimulus Features in Attentional Networks"
    },
    {
      "section": "Page 3",
      "page_number": 3,
      "text": "Grumpy, Female/Grumpy, Male/Happy, and Female/Happy). Block or-\nder was randomized across runs and across subjects. The order of the 16target faces was also randomized in each block. Across the six scan runs,there was a grand total of 408 trials (including filler trials). To familiarizesubjects with the task, before entering the scanner, subjects completed a24-trial practice round. Subjects practiced each goal combination (e.g.,“Male?/Happy?”) and the stimuli used were the 24 filler faces.\nAfter the main experiment, subjects completed two localizer runs.\nHowever, because data from the localizer scan are not reported here,details of the task are not included.\nThe design of the behavioral follow-up study was identical to Experi-\nment 1, with the exception of the ISI (3 s instead of 6 s) and the inclusionof a post-task questionnaire, in which subjects were asked increasinglyspecific questions as to whether they noticed the task switching structure.We first asked subjects if they noticed any patterns throughout the task.We then noted that there was a pattern to the goal cues and asked if theywere aware of such a pattern. Finally, we explicitly asked subjects if theynoticed the A-A-B-B-A-A-B-B structure of the goal cues.\nfMRI data acquisition\nImaging data were collected on a Siemen s 3 T Skyra scanner at the Robert\nand Beverly Lewis Center for NeuroImaging at the University of Oregon.Before the functional imaging, a whole-brain high-resolution anatomicalimage was collected for each subject using a T1-weighted protocol (gridsize 256 /H11003256; 176 sagittal slices; voxel size 1 /H110031/H110031 mm). Whole-brain\nfunctional images were collected using a T2*-weighted multiband accel-erated EPI sequence (TR /H110052s ;T E /H1100525 ms; flip angle /H1100590°; 72 hori-\nzontal slices; grid size 104 /H11003104; voxel size 2 /H110032/H110032 mm). For the main\nexperiment, six functional scans were collected, each consisting of 280volumes. For the localizer task, two functional scans were collected eachconsisting of 225 volumes.fMRI data preprocessing\nPreprocessing of the functional data was conducted using FSL 5.0\n(FMRIB Software Library, http://www.fmrib.ox.ac.uk/fsl ;Smith et al.,\n2004 ) and custom scripts. Images were first corrected for head motion\nusing MCFLIRT ( Jenkinson et al., 2002 ). Motion-corrected images were\nsmoothed with a Gaussian kernel with 1.7 mm SD ( /H110114 mm FWHM).\nNetwork selection\nWe assessed feature representations in four resting-state networks de-\nfined from a large, independent sample of subjects ( Yeo et al., 2011 ): the\nFPCN, DAN, VAN, and VisN ( Fig. 1 D). The resting-state networks were\ngenerated for each subject using their high-resolution anatomical imageand the FreeSurfer cortical parcellation scheme (\nhttp://surfer.nmr.mgh.\nharvard.edu ). The networks were then coregistered to the functional\ndata.\nUnivariate analyses\nUnivariate data analyses were conducted under the assumptions of thegeneral linear model (GLM) using SPM12 (\nhttp://www.fil.ion.ucl.ac.uk/\nspm). To test for univariate effects of switch versus stay trials, we defined\na model with separate regressors for switch and stay trials. The model alsoincluded regressors for scan run and six motion parameters for each run.Switch versus stay trials were contrasted using paired-samples ttests\nresulting in subject-specific statistical parametric maps. These tvalues\nwere then averaged within network, resulting in a single mean tstatistic\nper network and subject. For each network, one-sample ttests were then\napplied across subjects to test for effects of switch versus stay trials at thegroup level.\nPattern classification analyses\nFor pattern classification analyses, functional data were detrended, high-pass filtered (0.01 Hz), and z-scored within scan (mean response of each\nFigure 1. Experimental design. A, In a given trial, a goal cue was presented for 1400 ms. There were one of four possible goal cues (“Male?,” “Female?,” “Happy?,” or “Grumpy?”). After th e goal,\na face was presented for 100 ms. In Experiment 1, the face was followed by a mask image for 500 ms. In Experiment 2, the face was presented for an additional 500 ms. A fixation cross was then\npresented for 6000 ms. Subjects responded “Yes” or “No” as to whether the face matched the goal. B, Faces varied along two dimensions: gender and affect. Note that faces shown here were not\nthose used in the experiment. Permission was given for these faces to be published. C, Each block followed the same general structure. At the start of each block, subjects were cued as to which goals\nwould be relevant for that block. Only two goals were relevant in each block (one from each dimension) and these goals alternated in a fixed AABBAABB sch edule. This enforced a constant alternation\nbetween switch and stay trials. D, Four independently defined resting-state networks of a priori interest ( Yeo et al., 2011 ).Long and Kuhl •Stimulus Features in Attentional Networks J. Neurosci., March 7, 2018 •38(10):2495–2504 • 2497"
    },
    {
      "section": "Page 4",
      "page_number": 4,
      "text": "voxel within each scan /H110050). Next, data were temporally compressed via\na weighted averaged of TRs 3, 4, and 5 (40%, 40%, 20%, respectively)relative to trial onset (representing 4–10 s after the goal was presented).TRs 3 and 4 were given greater weight because the hemodynamic re-sponse tends to peak at these TRs. The temporally compressed data re-sulted in a single spatial pattern of activity for each trial. Before patternclassification analyses were performed, an additional round of z-scoring\nwas applied across voxels to the trial-specific spatial patterns. This finalround of z-scoring resulted in each trial-specific spatial volume having a\nmean activation equal to 0. Therefore, mean univariate activity wasmatched precisely across all conditions and trial types (\nKuhl and Chun,\n2014 ;Long et al., 2016 ). Pattern classification analyses were performed\nusing penalized (L2) logistic regression (penalty parameter /H110051) imple-\nmented via the Liblinear toolbox ( Fan et al., 2008 ) and custom MATLAB\n(RRID:SCR 001622) code. Classifier performance was assessed in twoways. “Classification accuracy” represented a binary coding of whetherthe classifier successfully guessed the queried feature of the face. We used\nclassification accuracy for general assessment of classifier performance(i.e., whether features could be decoded). “Classifier evidence” was acontinuous value reflecting the logit-transformed probability that theclassifier assigned the correct feature each trial. Classifier evidence wasused as a trial-specific, continuous measure of feature information,which was used to assess trial-level correlations in feature representationswithin and between networks (see below).\nFor each subject, four separate classifiers were trained to decode stim-\nulus features. A given classifier was trained to discriminate either malefrom female faces (gender classifier) or happy from grumpy faces (affectclassifier). Additionally, separate gender classifiers were applied for trialsin which the goal was either “Male?” or “Female?” (i.e., trials in whichgender was relevant) versus trials in which the goal was either “Happy?”or “Grumpy?” (i.e., trials in which affect was relevant). Likewise, separateaffect classifiers were applied for trials in which affect was the relevantdimension versus trials in which gender was the relevant dimension.Goal-relevant feature representations were indexed by performance ofthe gender classifier on gender-relevant trials and performance of theaffect classifier on affect-relevant trials. Goal-irrelevant feature represen-tations were indexed by performance of the gender classifier on affect-relevant trials and performance of the affect classifier on gender-relevanttrials. All classification analyses were performed using leave-one-run-outcross-validation. A critical element of our design and implementation ofclassification analyses is that we deliberately orthogonalized feature in-formation from behavioral responses. In other words, there was no con-sistent mapping between feature information and motor response. As anexample, during trials in which gender was relevant, 50% of the time, afemale face would require a “Yes” response (i.e., on “Female?” trials) and,50% of the time, a female face would require a “No” response (i.e., on“Male?” trials). Therefore, successful decoding of goal-relevant featureinformation cannot be attributed to decoding of the planned or executedmotor responses. A second critical element of our design is that, acrosstrials, goals and features were not always matched. In fact, they wereindependent. That is, when presented with a goal cue of “Male?,” thesubsequently presented stimulus was equally likely to be a male or femaleface. This design feature critically allowed us to deconfound goal andfeature information. In other words, decoding of stimulus features can-not be driven exclusively by goal information.\nTo decode top-down goals, an additional four classifiers were applied.\nThe four classifiers corresponded to the four different possible combina-tions of goals in a given block: “Male?” versus “Grumpy?,” “Male?” versus\n“Happy?,” “Female?” versus “Grumpy?,” and “Female?” versus “Happy?”\nClassification was performed using leave-one-run-out cross-validation.Classification accuracy was averaged across all four classifiers to providea single measure of goal decoding accuracy. Because goal decoding couldpotentially be driven by low-level information such as visual word formor subvocal articulation, we also tested whether goal classifiers general-ized to different goals corresponding to the same dimensions. For exam-ple, a classifier trained to discriminate between “Male?” versus “Happy?”faces can also be thought of as a classifier that is discriminating betweenthe gender versus affect dimensions. If so, then a classifier trained on“Male?” versus “Happy?” may successfully transfer to the discriminationof “Female?” versus “Grumpy?” Successful transfer would suggest that\nthe goal representation is, at least in part, a representation of the relevantdimension as opposed to the specific word cue per se. Therefore, for eachof the four goal classifiers described above, we tested for transfer to the“complementary” pair of goals; i.e., goal pairs that corresponded to dis-crimination between the same dimensions. This dimension decodinganalysis was again performed using leave-one-run-out cross-validationso as to match the goal decoding analyses in terms of statistical power.Classification accuracy was averaged across all four transfer tests to pro-vide a single aggregate value of dimension decoding accuracy.\nRepresentational coherence analysis\nTo assess the coherence of representations within and between networks,we decomposed each of the three attentional networks into separatefrontal and parietal ROIs: frontal-FPCN, parietal-FPCN, frontal-DAN,parietal-DAN, frontal-VAN, and parietal-VAN. None of the voxels fromthe frontal ROIs were contiguous with voxels from the parietal ROIs.These ROIs were generated using the average subject brain in Free Surfer.The ROIs were then projected from this volume space to surface spaceand then converted from average surface space to subject space. Becausethe number of voxels in these regions varied both within and acrosssubjects and differences in ROI size are likely to influence classifier per-formance, classification analyses were performed by randomly subsam-pling 500 voxels from each of the six frontal and parietal ROIs. Thisprocess was repeated for 100 iterations for each ROI and subject, witheach iteration involving a different random sample of 500 voxels. Classi-fication of goal-relevant and goal-irrelevant features was performed us-ing the same approach described above. Here, however, classifierevidence (a continuous value reflecting the strength of classifier informa-tion) was the critical dependent measure. For each subject, trial-by-trialmeasures of classifier evidence were correlated within network (e.g., acorrelation between classifier evidence from frontal-FPCN and parietal-FPCN) and between network (e.g., frontal-FPCN and parietal-DAN).The correlation analyses were separately performed for relevant and ir-relevant feature evidence. In total, we obtained 50,400\n/H9267values: 100 iter-\nations/H110039 correlations /H1100328 subjects /H110032 relevance conditions. All /H9267\nvalues were Fisher- ztransformed before averaging across iterations and\ngoal relevance. Finally, z-/H9267values were averaged across the three “within”\nnetwork correlations (e.g., correlations between frontal-FPCN andparietal-FPCN) and the six “between” network correlations (e.g., corre-lations between frontal-FPCN and parietal-DAN), resulting in two z-\n/H9267\nvalues for each subject, mean within network correlation, and mean\nbetween network correlation.\nStatistical analyses\nOne-sample ttests were used to compare representational coherence\nmeasures ( z-/H9267values) to zero. Paired-sample ttests were used to compare\nclassification accuracy across subjects to chance decoding accuracy, asdetermined by permutation procedures. Namely, for each subject andnetwork, we shuffled the condition labels of interest (e.g., “male” and“female” for the gender feature classifier) and then calculated classifica-tion accuracy. We repeated this procedure 1000 times for each networkand subject and then averaged the 1000 shuffled accuracy values for eachnetwork and subject. These mean values were used as network- andsubject-specific empirically derived measures of chance accuracy. Pairedsamples ttests compared the true (unshuffled) accuracy values to the\nshuffled accuracy values. For these paired-sample ttests, we report un-\ncorrected p-values; however, all of the p-values exceeded the threshold\nfor significance after Bonferroni correction; that is, after adjusting for thefact that we tested effects across four networks (i.e., a threshold of p/H11005\n0.0125). Mixed-effects ANOVAs were used to compare conditionsand/or networks; experiment was always included as a factor when datafrom both experiments were included.\nResults\nLarge-scale networks represent stimulus features\nMotivated by recent evidence that activity patterns in frontopa-rietal regions represent stimulus features (\nSwaminathan and\nFreedman, 2012 ;Ester et al., 2015 ;Lee and Kuhl, 2016 ;Bracci et2498 •J. Neurosci., March 7, 2018 •38(10):2495–2504 Long and Kuhl •Stimulus Features in Attentional Networks"
    },
    {
      "section": "Page 5",
      "page_number": 5,
      "text": "al., 2017 ), we first tested for representations of face features (gen-\nder, affect) within each of the three attentional networks (FPCN,DAN, VAN) and, for comparison, within VisN. Importantly, andas described in the Materials and Methods, our procedure delib-erately deconfounded feature information from behavioral re-sponses, so decoding accuracy cannot be explained by decodingof response preparation or execution. Averaging across the gen-der and affect classifiers (producing a single value per subjectreflecting feature decoding), accuracy was above chance in eachof the four networks (FPCN, t\n(27)/H110055.82, p/H110210.001; DAN, t(27)/H11005\n5.37, p/H110210.001; VAN, t(27)/H110055.42, p/H110210.001; VisN, t(27)/H110054.34,\np/H110210.001; Figure 2 A), confirming the sensitivity of these large-\nscale networks to feature information. There were no significantdifferences in decoding accuracy for the gender versus affect clas-sifiers for any of the four networks ( t/H110211,p/H110220.35). A mixed-\neffects ANOVA with factors of network (FPCN, DAN, VAN,VisN) and experiment (1, 2) did not reveal a main effect of net-work ( F\n(3,78)/H110050.49, p/H110050.69), indicating that overall feature\ndecoding was comparable across networks. The effects of exper-\niment are considered below.\nRepresentational coherence within networks\nThe preceding results demonstrate that activity patterns inresting-state networks represent stimulus features. However, theuse of large-scale networks as ROIs is predicated on the notionthat individual components (brain regions) within these net-works are acting together in a functionally relevant way to sup-port behavior. To test for coherence of representations within theattentional networks, we divided each of the three attentionalnetworks into frontal and parietal subregions (see Materials andMethods and ROIs from a sample subject in\nFig. 2 B) and corre-\nlated trial-by-trial feature evidence within and between each net-work (\nFig. 2 C). Correlations were significantly /H110220 both within\nnetworks (M /H110050.15, SD /H110050.03; t(27)/H1100524.7, p/H110210.001) and\nbetween networks (M /H110050.14, SD /H110050.03; t(27)/H1100527.4, p/H110210.001;\nFig. 2 D) .A2/H110032 mixed-effects ANOVA with factors of network\npairing (within, between) and experiment (1, 2) revealed a maineffect of network pairing ( F\n(1,26)/H1100523.9, p/H110210.001), with stronger\ncorrelations within networks than between networks. This\nmain effect of network pairing did not interact with experi-ment ( F\n(1,26)/H110050.03, p/H110050.86). Therefore, although feature rep -\nresentations were present across all of the attentional networks,there was greater representational coherence within networksthan between networks, validating the use of these networks aslarge-scale ROIs for pattern-based analyses.\nRobustness of feature representations to visual interruption\nWe next tested the relative sensitivity of feature representations ineach network to the interruption of visual processing by compar-ing decoding accuracy across experiments. Sensitivity to visual\ninterruption would be reflected by relatively lower decoding ac-curacy in Experiment 1 (100 ms stimulus /H11001500 ms visual mask)\nthan in Experiment 2 (600 ms stimulus /H11001no visual mask;\nFig. 3 )\nmixed-effects ANOVA with factors of experiment (1, 2) and net-work (FPCN, DAN, VAN, VisN) revealed a main effect of exper-iment ( F\n(1,26)/H1100526.5, p/H110210.001), with lower decoding accuracy in\nExperiment 1 than Experiment 2, consistent with a disruptive\ninfluence of interruption. Relative sensitivity to visual interrup-tion markedly varied across networks, as reflected by a significantexperiment by network interaction ( F\n(3,78)/H110056.2,p/H110210.001).\nPost hoc independent sample ttests revealed no reliable differ-\nences between experiments in FPCN ( t(26)/H110051.1,p/H110050.29), a\ntrend in DAN ( t(26)/H110052.0,p/H110050.06), and reliably greater feature\ndecoding in Experiment 2 than Experiment 1 in VAN ( t(26)/H110053.2,\np/H110050.003) and VisN ( t(26)/H110055.8,p/H110210.001). Therefore, the\nexperimental manipulation of including a brief visual mask (Ex-\nperiment 1) versus extended stimulus presentation (Experiment2), the only difference between the two experiments, robustlydiminished feature representations in VisN and VAN, but hadrelatively less influence on feature representations in DAN andFPCN.\nGoal relevance influences feature representations\nWe next tested the relative sensitivity of feature representations ineach network to trial-specific behavioral goals; that is, whethergoal-relevant feature information was stronger than goal-irrelevant feature information (\nKuhl et al., 2013 ;Roy et al., 2014 ;\nSarma et al., 2016 ;Bracci et al., 2017 ). For example, on trials in\nwhich gender was the relevant dimension (“Male?” or “Female?”\nFigure 2. Feature decoding across networks. A, Feature decoding averaged across experiments and goal relevance. Feature decoding was reliably above chance in all networks. B–D, Represen-\ntational coherence within and between networks. B, Each attentional network was divided into discontiguous frontal and parietal subregions (here shown for a single subject). C, Correlations were\ncomputed for trial-level measures of goal-relevant feature evidence for each pair of frontal and parietal subregions. Within-network correlation s (dark gray) corresponded to correlations between\npairs of subregions from the same network (e.g., frontal-FPCN and parietal-FPCN). Between-network correlations (light gray) corresponded to corr elations between pairs of subregions from\ndifferent networks (e.g., frontal-FPCN and parietal-DAN). This analysis was then repeated for goal-irrelevant feature evidence and Fisher Z-transformed /H9267values were then averaged across the\ngoal-relevant and goal-irrelevant correlation analyses. D, Within-network correlations were reliably stronger than between-network correlations. *** p/H110210.001.\nFigure 3. Feature decoding as a function of interruption. Interruption of visual processing\nwas manipulated across experiments. In Experiment 1 (clear circles), stimuli were followed by avisual mask. In Experiment 2 (filled circles), there was a longer stimulus duration and no mask.There was a significant network-by-experiment interaction ( p/H110210.001). Significant differ-\nences between experiments were observed in VAN and VisN. Error bars indicate SEM. /H11011p/H11021\n0.10, ** p/H110210.01, *** p/H110210.001.Long and Kuhl •Stimulus Features in Attentional Networks J. Neurosci., March 7, 2018 •38(10):2495–2504 • 2499"
    },
    {
      "section": "Page 6",
      "page_number": 6,
      "text": "trials), goal-relevant feature information was indexed by accu-\nracy of the gender classifier and goal-irrelevant feature informa-tion was indexed by accuracy of the affect classifier. To be clear,for this analysis, decoding accuracy refers only to the featureinformation in the actual face image. Therefore, if gender was therelevant dimension and the face image was a female face, then theclassifier was accurate if it guessed “Female” regardless of whetherthe current goal was “Male?” or “Female?” This approach en-sured that feature information was not confounded with goalinformation. Separate classifiers were trained/tested for relevantand irrelevant feature representations. That is, one set of classifi-ers was specifically trained and tested on goal-relevant featureinformation and a separate set was trained and tested on goal-irrelevant feature information. Separate classifiers were used forgoal-relevant and goal-irrelevant feature decoding so that therewas no assumption that goal-relevant and goal-irrelevant featurerepresentations are encoded in a common (generalizable) way.\nA mixed-effects ANOVA with factors of experiment, goal rel-\nevance (relevant, irrelevant), and network (FPCN, DAN, VAN,VisN) revealed a marginally significant main effect of relevance(F\n(1,26)/H110053.93, p/H110050.06), reflecting a trend for stronger decoding\nof goal-relevant than goal-irrelevant feature information ( Fig. 4 ).\nThe effect of relevance did not vary by experiment ( F(1,26)/H110050.07,\np/H110050.80). More critically, there was a significant interaction\nbetween network and goal relevance ( F(3,78)/H110053.12, p/H110050.03),\nindicating that the sensitivity of feature representations to top-\ndown goals varied across the networks. This interaction betweennetwork and relevance did not vary by experiment ( F\n(3,78)/H110051.24,\np/H110050.30).\nFor each network, we ran follow-up mixed-effects ANOVAs\nwith factors of goal relevance (relevant, irrelevant) and experi-ment to test for effects of goal relevance within each network. Asignificant main effect of goal relevance (relevant /H11022irrelevant)\nwas observed in VAN ( F\n(1,26)/H1100513.9, p/H110210.001), with a similar\ntrend in FPCN ( F(1,26)/H110053.7,p/H110050.07). There was no effect of\nrelevance in DAN ( F(1,26)/H110050.03, p/H110050.87) or VisN ( F(1,26)/H11005\n0.12, p/H110050.73). None of the main effects of relevance interacted\nwith experiment ( F/H110212.5,p/H11022.1).\nTask switches influence feature representations\nIn the preceding section, we considered whether goal-relevantfeatures were more strongly represented than goal-irrelevant fea-tures. However, goal cues (and, therefore, the relevant featuredimension) changed every two trials, creating both switch (goalchange) and stay (goal repeat) trials. To measure the influence ofgoal cue switches, we first probed the behavioral data for effects of\ntask switching. Reaction times (RTs) were compared as a func-tion of trial type (switch, stay) and experiment in a mixed-effectsANOVA. As expected, RTs were significantly greater for switchtrials (M /H110051423 ms, SD /H11005259 ms) than stay trials (M /H110051398 ms,\nSD/H11005269 ms; F\n(1,26)/H1100512.7, p/H110050.001; Fig. 5 A). This effect did\nnot interact with experiment ( F(1,26)/H110053.2,p/H110050.08). Accuracy\ndid not significantly differ across switch and stay trials (switch,\nM/H1100596.92%, SD /H110053.17%; stay, M /H1100597.52%, SD /H110052.17%;\nt(27)/H11005/H11002 1.5,p/H110050.13). There was a trend toward an interaction\nbetween switch/stay and experiment ( F(1,26)/H110054.03, p/H110050.06),\nreflecting a relatively greater effect of accuracy (stay /H11022switch) in\nExperiment 2.\nTo determine whether the predictable nature of goal cues (i.e.,\nthe A-A-B-B-A-A-B-B alternation) was detected by subjectsand/or influenced switch costs (\nRogers and Monsell, 1995 ;Mon-\nsell, 2003 ), we ran an independent behavioral study in which 14\nsubjects completed an experiment virtually identical to Experi-ment 1 (see Materials and Methods). Critically, however, thisbehavioral experiment also included a postexperiment question-naire to assess whether subjects became aware of the alternatingpattern of goal cues. Across all subjects, RTs were significantlygreater for switch trials (M /H110051298 ms, SD /H11005227 ms) than stay\ntrials (M /H110051257 ms, SD /H11005204 ms; t\n(13)/H110052.58, p/H110050.02),\nconsistent with the data from Experiments 1 and 2. Of the 14\nsubjects, four were able to explicitly describe the task structure(i.e., they were aware of the pattern of alternation). Among thesefour subjects, switch costs were numerically greater (M /H1100559 ms)\nrelative to subjects who were unaware of the task structure (M /H11005\n33 ms). These data indicate that, despite the highly structuredtask alternation, most subjects did not become aware of thisstructure. In addition, at least in this small sample of subjects,there was no evidence that awareness of the structure reducedswitch costs. Therefore, it is unlikely that explicit awareness of thetask structure had a major influence on fMRI-based effects of taskswitching.\nPrior univariate fMRI studies of task switching have consis-\ntently reported increased activation during switch versus stay tri-als in dorsal frontoparietal regions (overlapping with FPCN andDAN), whereas more ventral regions (overlapping with VAN) aregenerally insensitive to switch effects (\nKimberg et al., 2000 ;Rush-\nworth et al., 2002 ;Braver et al., 2003 ;Brass and Von Cramon,\n2004 ;Corbetta et al., 2008 ). In light of these prior studies, and as\na comparison point for our decoding analyses, we tested for uni-variate effects of task switching in each of the four resting-statenetworks. Consistent with prior findings, we found reliablygreater activation for switch than stay trials in FPCN ( t\n(27)/H110054.0,\np/H110210.001) and DAN ( t(27)/H110052.5,p/H110050.02) and no effect in VAN\n(t(27)/H110051.2,p/H110050.23) or VisN ( t(27)/H110050.23, p/H110050.82; Fig. 5 B).Post\nhocindependent-samples ttests within each network did not\nreveal any between-experiment differences in switch effects inFPCN, DAN, or VAN ( t/H110211.5,p/H11022.1). In VisN, there was a\nreliable difference between experiment ( t\n(26)/H11005/H11002 2.3,p/H110050.03),\nalthough neither experiment alone showed a reliable difference\nbetween switch and stay trials. In Experiment 1, activation wasrelatively greater for stay than switch trials ( t\n(13)/H11005/H11002 1.7,p/H11005\n0.12), whereas in Experiment 2, activation was relatively greater\nfor switch than stay trials ( t(13)/H110051.6,p/H110050.14).\nNext, we considered the novel question of whether task\nswitches influenced the strength of feature representations. Forthis analysis, we used the same classifiers described above andthen back-sorted trial-specific accuracy values as a function oftrial type (switch vs stay). Decoding accuracy was entered into a\nFigure 4. Feature decoding as a function of goal relevance. Orange indicates goal-relevant\nfeature decoding (e.g., decoding whether a face was male vs female when the goal was “Male?”or “Female?”). Blue indicates goal-irrelevant feature decoding (e.g., decoding whether a facewas male vs female when the goal was “Happy?” or “Grumpy?”). There was a significant inter-action between network and goal relevance ( p/H110050.03). Goal-relevant feature decoding was\nreliably greater than goal-irrelevant feature decoding in VAN, with a similar trend in FPCN./H11011p/H110210.10, *** p/H110210.001.2500 •J. Neurosci., March 7, 2018 •38(10):2495–2504 Long and Kuhl •Stimulus Features in Attentional Networks"
    },
    {
      "section": "Page 7",
      "page_number": 7,
      "text": "mixed-effects ANOVA with factors of task switch (switch, stay),\nrelevance (relevant, irrelevant), network, and experiment. Themain effect of task switch was not significant ( F\n(1,26)/H110050.09, p/H11005\n0.77). However, there was a robust 3-way interaction between\nthe effects of task switch, relevance, and network ( F(3,78)/H110054.50,\np/H110050.006). Considering goal-relevant feature information alone,\na mixed-effects ANOVA with factors of task switch, network, andexperiment revealed a significant interaction between task switchand network ( F\n(3,78)/H110054.87, p/H110050.004). For goal-irrelevant fea -\nture information, a similar mixed-effects ANOVA did not revealan interaction between task switch and network ( F\n(3,78)/H110050.60,\np/H110050.62). Therefore, the influence of task switches on feature\nrepresentations varied across the networks, but only when con-sidering goal-relevant feature representations.\nThe interaction between task switch and network for goal-\nrelevant feature representations was driven by relatively strongerrepresentations on switch versus stay trials in DAN and VAN andan opposite pattern in FPCN. This interaction did not signifi-cantly vary by experiment ( F\n(3,78)/H110052.25, p/H110050.09). Follow-up\nmixed-effects ANOVAs with factors of task switch (switch, stay)\nand experiment revealed that, within FPCN, there was a signifi-cant effect of task switch ( F\n(1,26)/H110056.3,p/H110050.02), reflecting rela -\ntively lower feature information on switch than stay trials. Incontrast, there were opposite trends (greater feature informationon switch than stay trials) in DAN ( F\n(1,26)/H110054.2,p/H110050.05) and\nVAN ( F(1,26)/H110054.2,p/H110050.05). In VisN, there was no effect of task\nswitch ( F(1,26)/H110051.1,p/H110050.31). The effect of task switch inter -\nacted with experiment in FPCN ( F(1,26)/H110056.6,p/H110050.02), but not\nin DAN, VAN, or VisN ( F/H110212.2,p/H11022.15). For FPCN, the inter-\naction between experiment and task switch reflected a greaterdecoding advantage on stay versus switch trials in Experiment 2compared with Experiment 1. Collectively, these data indicatethat task switches had an effect on goal-relevant feature informa-tion, but this effect varied across networks and was most evidentin the attentional networks.\nNetwork representations of task goals\nAlthough our central aim was to assess feature representationsacross networks, a complementary question is how/whetherthese networks represent top-down behavioral goals. Previousresearch has revealed top-down goal effects within multiple fron-toparietal regions (\nEsterman et al., 2009 ;Greenberg et al., 2010 ;\nLiu and Hou, 2013 ;Waskom et al., 2014 ;Waskom and Wagner,\n2017 ). Here, to test for goal representations, we trained and tested\nfour separate pairwise classifiers to distinguish each combinationof gender and affect goals. For example, one classifier was trainedto dissociate “Male?” versus “Grumpy?” goal cues. Averagingacross the pairwise classifiers, goal decod-\ning was significantly above chance in eachof the four networks (FPCN, t\n(27)/H110058.8,\np/H110210.001; DAN, t(27)/H1100510.6, p/H110210.001;\nVAN, t(27)/H110055.9,p/H110210.001; VisN, t(27)/H11005\n10.8, p/H110210.001; Fig. 6 ). A mixed-effects\nANOVA with factors of experiment andnetwork revealed no main effect of exper-iment ( F\n(1,26)/H110050.76, p/H110050.39), a main\neffect of network ( F(3,78)/H110058.53, p/H11021\n0.001), and no interaction between net-\nwork and experiment ( F(3,78)/H110050.31, p/H11005\n0.81). Adding task switch (switch, stay) as\na factor ( Waskom et al., 2014 ) revealed no\nmain effect of switch on goal decoding(F\n(1,26)/H110050.01, p/H110050.92), nor was there\nan interaction between the factors of task switch and network\n(F(3,78)/H110050.70, p/H110050.56).\nBecause goals were communicated to subjects via word\ncues, successful goal decoding potentially reflects multiple in-fluences: (1) “true” abstract goal representations, (2) represen-\ntations of the visual word form of each goal cue, and/or (3)subvocal rehearsal of the word cues. Therefore, as a complemen-tary measure, we also assessed “dimension” decoding; that is,decoding of the dimension (gender or affect) that was relevant oneach trial. Dimension decoding was assessed by testing for trans-fer of classifiers across goal cues but within dimensions. Forexample, a classifier trained to discriminate “Male?” versus“Grumpy?” goals would be tested on “Female?” versus “Happy?”goals. Successful transfer would occur if the representation of the“Male?” goal is similar to the representation of the “Female?” goaland, likewise, if “Grumpy?” is similar to “Happy?” Although thisanalysis does not entirely rule out effects of visual word form orsubvocal rehearsal, it at least reduces these concerns. One-samplettests revealed reliable dimension decoding in all four networks\n(FPCN, t\n(27)/H110057.9,p/H110210.001; DAN, t(27)/H110057.5,p/H110210.001; VAN,\nt(27)/H110055.1,p/H110210.001; VisN, t(27)/H1100510.0, p/H110210.001). A mixed-\neffects ANOVA with factors of experiment and network revealed\nno main effect of experiment ( F(1,26)/H110051.09, p/H110050.31), a main\neffect of network ( F(3,78)/H110055.53, p/H110050.002), and no reliable\ninteraction between network and experiment ( F(3,78)/H110050.17,\np/H110050.92).\nFinally, to compare goal decoding and dimension decoding\ndirectly, we ran a mixed-effects ANOVA with factors of goal level(goal decoding vs dimension decoding), network, and experi-ment. There was a significant main effect of goal level ( F\n(1,26)/H11005\n25.86, p/H110210.001), reflecting lower dimension decoding accuracy\nthan goal decoding accuracy, which is not surprising given thatdimension decoding requires transfer of the classifier to non-identical goal cues. There was a significant main effect of network\n(F\n(3,78)/H110058.32, p/H110210.001), reflecting (as with the separate\nANOVAs for goal decoding and dimension decoding) relatively\nlower decoding accuracy in VAN than in the other networks ( Fig.\n6). The interaction between goal level and network was also sig-\nnificant ( F(3,78)/H110052.95, p/H110050.04), indicating that the “cost” of\ntransferring across goal cues (but within dimensions) varied\nacross networks. Namely, the difference between goal decodingand dimension decoding was most pronounced in VisN (\nFig. 6 ).\nDiscussion\nHere, using pattern-based fMRI methods and a perceptualdecision-making task, we compared representations of stimulusfeatures across multiple resting-state networks. We specifically\nFigure 5. Effects of task switching. A, Difference in RTs (ms) between switch and stay trials. RTs were significantly slower for\nswitch trials than stay trials. B, Univariate contrast of switch versus stay trials. Activation was significantly greater for switch than\nstay trials in both FPCN and DAN. C, Goal-relevant feature decoding separated by switch and stay trials. There was a significant\ninteraction between switch/stay and network, reflecting greater goal-relevant feature decoding on stay than switch trials in FPCN;the opposite trends were true for DAN and VAN. /H11011p/H110210.10, * p/H110210.05, ** p/H110210.01, *** p/H110210.001.Long and Kuhl •Stimulus Features in Attentional Networks J. Neurosci., March 7, 2018 •38(10):2495–2504 • 2501"
    },
    {
      "section": "Page 8",
      "page_number": 8,
      "text": "targeted networks that are thought to contribute to attention and\ncognitive control (FPCN, DAN, and VAN; Dosenbach et al.,\n2008 ;Cole et al., 2013 ;Sestieri et al., 2017 ) and, as an important\ncomparison, visual cortical networks. Although stimulus featureswere reliably decoded from each network, of critical interest washow feature representations in each network were influenced bythree attentional manipulations: (1) interruption of visual pro-cessing, (2) goal relevance, and (3) task switching. Whereasbottom-up manipulations (interruption) had a relatively greaterinfluence on feature representations within VisN, top-down ma-nipulations (goals and task switches) had a relatively greaterinfluence on representations in attentional networks. These\nfindings reveal an important dissociation between feature rep-resentations in higher-level attentional networks and featurerepresentations in visual cortex.\nWhereas most decoding studies use spatially contiguous ROIs\nor searchlight analyses, we decoded from large, spatially discon-tiguous networks identified from independent studies of resting-state connectivity (\nVincent et al., 2008 ;Yeo et al., 2011 ). We used\nthese resting-state networks because they have been linked spe-cifically to attentional processes (\nFox et al., 2006 ;Corbetta et al.,\n2008 ;Gratton et al., 2017 ). That said, these networks have been\ndefined based on correlations in univariate responses during restand it was an open question whether regions within thesenetworks would exhibit correlated feature representations, parti-cularly after mean univariate responses were removed (see Mate-rials and Methods). When specifically considering correlationsbetween frontal and parietal regions from each attentional net-work, we found that decoded feature representations were morecoherent within networks than between networks. Beyond vali-dating our approach, this finding provides novel evidence thatbrain regions within attentional networks represent commonstimulus information.\nThe traditional view of frontoparietal regions is that they bias\nand/or evaluate stimulus representations held in perceptual re-gions (\nKastner et al., 1999 ;Zanto et al., 2010 ;Liu et al., 2011 ;\nGazzaley and Nobre, 2012 ). However, our findings, along with\nother recent evidence ( Kuhl et al., 2013 ;Bettencourt and Xu,\n2016 ;Ester et al., 2016 ;Bracci et al., 2017 ), challenge this view by\ndemonstrating active stimulus representations within frontopa-rietal regions. This raises the critical question of how frontopari-etal representations differ from those in perceptual regions. Weshow that frontoparietal representations were sensitive to differ-ent forms of attention than representations in visual cortical ar-eas. The fact that frontoparietal representations were relatively\nmore sensitive to top-down factors helps to reconcile evidencethat frontoparietal regions represent stimulus information withthe putative role of these regions in top-down processing. Specif-ically, our results suggest a transformation of information fromperceptual regions to frontoparietal regions that selectively rep-resent and/or evaluate stimulus features. We next briefly considerthe pattern of results for each attentional manipulation.\nPrevious work has shown that visual distraction disrupts\nworking memory representations in visual cortex to a greaterdegree than representations in frontoparietal regions (\nMiller et\nal., 1996 ;Suzuki and Gottlieb, 2013 ;Woolgar et al., 2015 ;Betten-\ncourt and Xu, 2016 ). Based on these findings, we predicted that\nfeature representations in the attentional networks would be rel-atively less influenced by the across-experiment manipulation ofvisual interruption. Indeed, the effect of interruption (Experi-ment 1 vs Experiment 2) significantly differed across networks,with feature representations in VisN most strongly “suffering”from the interruption in bottom-up visual input (\nFig. 3 ). Among\nthe attentional networks, VAN showed a significant effect of in-terruption and there was a trend for DAN, whereas feature rep-resentations in FPCN were not influenced by interruption. Thefact that VAN feature representations were influenced by inter-ruption is consistent with the idea that VAN is involved inbottom-up attentional capture (\nCorbetta and Shulman, 2002 ).\nOne reason why frontoparietal regions may actively represent\nstimulus features is so that behaviorally relevant decisions can bemade. Indeed, several recent studies have found that stimulusrepresentations in frontoparietal regions are biased by task de-mands (\nSwaminathan and Freedman, 2012 ;Kuhl et al., 2013 ;\nEster et al., 2016 ;Sarma et al., 2016 ;Bracci et al., 2017 ). We\nspecifically designed our stimuli to be multidimensional so thatwe could test for flexible representation of individual features. Aswith visual interruption, we found that the influence of behav-ioral goals varied across networks (\nFig. 4 ). In VAN, there was\nsignificantly greater representation of goal-relevant than goal-irrelevant features, with a similar trend in FPCN. At first pass, theeffect of goal relevance in VAN seems at odds with the putativerole of VAN in bottom-up attentional orienting (\nCorbetta and\nShulman, 2002 ). For example, univariate responses in VAN in-\ncrease for oddball stimuli or targets that appear at invalid loca-tions (\nBledowski et al., 2004 ;Kincade et al., 2005 ). However,\nmore recent evidence suggests that VAN plays a role in compar-ing bottom-up input to top-down goals (\nVossel et al., 2014 ;Grat-\nton et al., 2017 ). In fact, a recent meta-analysis found greater\nVAN responses to task-relevant than task-irrelevant oddballs(\nKim, 2014 ), suggesting that VAN /H11032s response to exogenous stim-\nuli is moderated by top-down goals. Therefore, the present find-ing of preferential decoding of goal-relevant features in VAN isconsistent with the view that VAN plays a role in orienting atten-tion to task-relevant perceptual input.\nVisN feature representations were unaffected by goals. Al-\nthough other studies have clearly found that top-down factorscan influence stimulus representations in early visual areas (\nJehee\net al., 2011 ;Sprague and Serences, 2013 ;Ester et al., 2016 ), the\npresent findings suggest that frontoparietal regions can imposetheir own attentional biases to favor goal-relevant features asopposed to simply inheriting biases imposed in visual corticalregions. Potentially, the lack of attentional bias in VisN in thepresent study reflects the fact that we used stimuli (faces) that areprocessed holistically. With different stimulus types and/or at-tentional manipulations, it is likely easier to gate processing atearlier stages.\nFigure 6. Goal and dimension decoding. For goal decoding (teal), four different classifiers\nwere trained to dissociate each of the possible pairs of goals that appeared in a given block. Thegoal pairs always consisted of one goal from each dimension (e.g., “Male?” vs “Grumpy?” goals).For dimension decoding (lavender), the classifier training was identical to goal, but the classi-fiers were tested on different goal pairs corresponding to the same dimensions. For example, aclassifier trained to dissociate Male/Grumpy goals would be tested on Female/Happy goals. Allnetworks showed reliable goal and dimension decoding. * p/H110210.05, ** p/H110210.01, *** p/H11021\n0.001.2502 •J. Neurosci., March 7, 2018 •38(10):2495–2504 Long and Kuhl •Stimulus Features in Attentional Networks"
    },
    {
      "section": "Page 9",
      "page_number": 9,
      "text": "A large body of previous research indicates that switching\nbetween tasks (goals) is associated with increased univariate ac-tivity in dorsal frontoparietal regions (\nCorbetta et al., 2008 ). Our\nfinding of greater univariate activation for switch versus stay tri-als in FPCN and DAN, but not VAN or VisN, is consistent withthis literature. However, we are not aware of prior pattern-basedfMRI studies that have compared frontoparietal feature repre-sentations across switch versus stay trials, so our analysis of taskswitching effects on feature representations was necessarily moreexploratory. We found that the influence of task switching variedacross networks (\nFig. 5 ) and, in particular, that switching effects\nwere more apparent in the attentional networks than VisN. Giventhat task switching requires reconfiguration of top-down atten-tion, this finding is consistent with the argument that featurerepresentations in attentional networks are relatively more sen-sitive to top-down attention. Among the attentional networks,FPCN showed greater feature decoding on stay than switch trials,whereas DAN and VAN showed opposite trends. Although wedid not predict this pattern a priori, the tendency for featurerepresentations to be stronger on switch than stay trials (in DANand VAN) is reminiscent of recent evidence for greater decodingof top-down goals on switch than stay trials (\nWaskom et al.,\n2014 ). For FPCN, it is notable that switch trials were associated\nwith relatively greater univariate activity but relatively lower fea-ture decoding, raising the possibility that these measures reflectopposing processes. Given the exploratory nature of this analysis,we believe this question requires additional investigation.\nAlthough our primary focus was on frontoparietal represen-\ntations of stimulus features, several prior studies have reportedfrontoparietal representations of top-down goals (\nWaskom et al.,\n2014 ;Hanson and Chrysikou, 2017 ;Loose et al., 2017 ;Waskom\nand Wagner, 2017 ;Qiao et al., 2017 ) Consistent with these find-\nings, we observed significant goal decoding in all four networks.One open question is whether the strength of goal representa-tions is influenced by task switches. Although there is some evi-dence that goal representations are relatively stronger on switchtrials than stay trials (\nWaskom et al., 2014 ), others have failed to\nobserve switch-related effects in task representations ( Loose et\nal., 2017 ). Like Loose et al. (2017) , we did not observe a significant\ndifference in goal decoding as a function of task switching; how-ever, we did find task-switching effects in the decoding of goal-relevant features. Therefore, additional research will be needed tobetter understand how and when task switching influences thestrength of top-down goals and/or goal-relevant feature repre-sentations.\nGiven that classifiers trained on one pair of goals (e.g., “Male”\nvs “Grumpy” goals) reliably transferred to nonidentical goals thatshared the same dimensions (e.g., “Female” vs “Happy” goals),this suggests that goal representations reflected, at least in part,information about or attention to the goal-relevant stimulus di-mension (gender or affect). The fact that the “transfer cost” (goalvs dimension decoding) was relatively greatest in VisN is consis-tent with the idea that goal decoding in VisN was more closelyrelated to low-level properties of the goals (e.g., the visual wordform of the cue). Considering goal-decoding performance acrossnetworks also provides a useful comparison point for the featuredecoding results. For example, comparing overall goal decodingversus decoding of goal-relevant features across the three atten-tional networks (a 2 /H110033 ANOVA) revealed a highly significant\ninteraction ( F\n(2,52)/H110057.40, p/H110050.001), reflecting a dissociation\nbetween the networks that best represented goal-relevant features\n(VAN) versus the goals themselves (FPCN/DAN). Therefore,theoretical accounts of how these networks contribute to attentionwill benefit from considering, not only how feature representations\nvary across networks, but also the hierarchical organization of fea-ture and goal representations (\nKoechlin and Summerfield, 2007 ;\nBadre, 2008 ).\nIn summary, we show that resting-state networks implicated\nin attentional control actively and coherently represent stimulusfeatures and that network-based feature representations can bedissociated in terms of their sensitivity to various forms of atten-tion (interruption of visual processing, goal relevance, and taskswitching). Whereas feature representations in visual cortical ar-eas are sensitive to low-level manipulations (visual interruption),feature representations in attentional networks are sensitive tohigher-level manipulations (goal relevance and task switching).At a broad level, these findings indicate that multiple networksactively represent stimulus features, with the nature of these fea-ture representations providing insight into each network’s func-tional role.\nReferences\nBadre D (2008) Cognitive control, hierarchy, and the rostro-caudal organi-\nzation of the frontal lobes. Trends Cogn Sci 12:193–200. CrossRef\nMedline\nBettencourt KC, Xu Y (2016) Decoding the content of visual short-term\nmemory under distraction in occipital and parietal areas. Nat Neurosci19:150–157.\nCrossRef Medline\nBledowski C, Prvulovic D, Goebel R, Zanella FE, Linden DE (2004) Atten-\ntional systems in target and distractor processing: a combined ERP andfMRI study. Neuroimage 22:530–540.\nCrossRef Medline\nBode S, Haynes JD (2009) Decoding sequential stages of task preparation in\nthe human brain. Neuroimage 45:606–613. CrossRef Medline\nBracci S, Daniels N, Op de Beeck H (2017) Task context overrules object-\nand category-related representational content in the human parietal cor-tex. Cereb Cortex 27:310–321.\nCrossRef Medline\nBrass M, von Cramon DY (2004) Decomposing components of task prepa-\nration with functional magnetic resonance imaging. J Cogn Neurosci16:609–620.\nCrossRef Medline\nBraver TS, Reynolds JR, Donaldson DI (2003) Neural mechanisms of tran-\nsient and sustained cognitive control during task switching. Neuron 39:713–726.\nCrossRef Medline\nCole MW, Reynolds JR, Power JD, Repovs G, Anticevic A, Braver TS (2013)\nMulti-task connectivity reveals exible hubs for adaptive task control. NatNeurosci 16:1348–1355.\nCrossRef Medline\nCorbetta M, Shulman GL (2002) Control of goal-directed and stimulus-\ndriven attention in the brain. Nat Rev Neurosci 3:201–215. CrossRef\nMedline\nCorbetta M, Patel G, Shulman GL (2008) The reorienting system of the\nhuman brain: from environment to theory of mind. Neuron 58:306–324.\nCrossRef Medline\nDesimone R, Duncan J (1995) Neural mechanisms of selective visual atten-\ntion. Annu Rev Neurosci 18:193–222. CrossRef Medline\nDosenbach NU, Fair DA, Cohen AL, Schlaggar BL, Petersen SE (2008) A\ndual-networks architecture of top-down control. Trends Cogn Sci 12:99–105.\nCrossRef Medline\nEgner T, Hirsch J (2005) Cognitive control mechanisms resolve conict\nthrough cortical amplification of task-relevant information. Nat Neurosci8:1784–1790.\nCrossRef Medline\nEster EF, Sprague TC, Serences JT (2015) Parietal and frontal cortex encode\nstimulus-specific mnemonic representations during visual workingmemory. Neuron 87:893–905.\nCrossRef Medline\nEster EF, Sutterer DW, Serences JT, Awh E (2016) Feature-selective atten-\ntional modulations in human frontoparietal cortex. J Neurosci 36:8188–8199.\nCrossRef Medline\nEsterman M, Chiu YC, Tamber-Rosenau BJ, Yantis S (2009) Decoding cog-\nnitive control in human parietal cortex. Proc Natl Acad Sci U S A 106:17974–17979.\nCrossRef Medline\nFan RE, Chang KW, Hsieh CJ, Wang XR, Lin CJ (2008) Liblinear: a library\nfor large linear classification. The Journal of Machine Learning Research9:1871–1874.\nFox MD, Corbetta M, Snyder AZ, Vincent JL, Raichle ME (2006) Spontane-Long and Kuhl •Stimulus Features in Attentional Networks J. Neurosci., March 7, 2018\n•38(10):2495–2504 • 2503"
    },
    {
      "section": "Page 10",
      "page_number": 10,
      "text": "ous neuronal activity distinguishes human dorsal and ventral attention\nsystems. Proc Natl Acad Sci U S A 103:10046–10051. CrossRef Medline\nGazzaley A, Nobre AC (2012) Top-down modulation: bridging selective at-\ntention and working memory. Trends Cogn Sci 16:129–135. CrossRef\nMedline\nGratton C, Neta M, Sun H, Ploran EJ, Schlaggar BL, Wheeler ME, Petersen SE,\n**Nelson SM (2017) Distinct stages of moment-to-moment processingin the cinguloopercular and frontoparietal networks. Cereb Cortex 27:2403–2417.\nCrossRef Medline\nGreenberg AS, Esterman M, Wilson D, Serences JT, Yantis S (2010) Control\nof spatial and feature-based attention in frontoparietal cortex. J Neurosci30:14330–14339.\nCrossRef Medline\nHanson GK, Chrysikou EG (2017) Attention to distinct goal-relevant fea-\ntures differentially guides semantic knowledge retrieval. J Cogn Neurosci29:1178–1193.\nCrossRef Medline\nJehee JF, Brady DK, Tong F (2011) Attention improves encoding of task-\nrelevant features in the human visual cortex. J Neurosci 31:8210–8219.\nCrossRef Medline\nJenkinson M, Bannister P, Brady M, Smith S (2002) Improved optimization\nfor the robust and accurate linear registration and motion correction ofbrain images. Neuroimage 17:825–841.\nCrossRef Medline\nKastner S, Pinsk MA, De Weerd P, Desimone R, Ungerleider LG (1999)\nIncreased activity in human visual cortex during directed attention in theabsence of visual stimulation. Neuron 22:751–761.\nCrossRef Medline\nKim H (2014) Involvement of the dorsal and ventral attention networks in\noddball stimulus processing: a meta-analysis. Hum Brain Mapp 35:2265–2284.\nCrossRef Medline\nKimberg DY, Aguirre GK, D’Esposito M (2000) Modulation of task-related\nneural activity in task-switching: an fMRI study. Brain Res Cogn BrainRes 10:189–196.\nCrossRef Medline\nKincade JM, Abrams RA, Astafiev SV, Shulman GL, Corbetta M (2005) An\nevent-related functional magnetic resonance imaging study of voluntaryand stimulus-driven orienting of attention. J Neurosci 25:4593–4604.\nCrossRef Medline\nKoechlin E, Summerfield C (2007) An information theoretical approach to\nprefrontal executive function. Trends Cogn Sci 11:229–235. CrossRef\nMedline\nKuhl BA, Chun MM (2014) Successful remembering elicits event-specific\nactivity patterns in lateral parietal cortex. J Neurosci 34:8051–8060.\nCrossRef Medline\nKuhl BA, Johnson MK, Chun MM (2013) Dissociable neural mechanisms\nfor goal-directed versus incidental memory reactivation. J Neurosci 33:16099–16109.\nCrossRef Medline\nLee H, Kuhl BA (2016) Reconstructing perceived and retrieved faces from\nactivity patterns in lateral parietal cortex. J Neurosci 36:6069–6082.\nCrossRef Medline\nLiu T, Hou Y (2013) A hierarchy of attentional priority signals in human\nfrontoparietal cortex. J Neurosci 33:16606–16616. CrossRef Medline\nLiu T, Hospadaruk L, Zhu DC, Gardner JL (2011) Feature-specific atten-\ntional priority signals in human cortex. J Neurosci 31:4484–4495.\nCrossRef Medline\nLong NM, Lee H, Kuhl BA (2016) Hippocampal mismatch signals are mod-\nulated by the strength of neural predictions and their similarity to out-comes. J Neurosci 36:12677–12687.\nCrossRef Medline\nLoose LS, Wisniewski D, Rusconi M, Goschke T, Haynes JD (2017) Switch\nindependent task representations in frontal and parietal cortex. J Neuro-sci 37:8033–8042.\nCrossRef Medline\nMiller EK, Erickson CA, Desimone R (1996) Neural mechanisms of visual\nworking memory in prefontal cortex of the macaque. J Neurosci 16:5154–5167.\nMedline\nMonsell S (2003) Task switching. Trends Cogn Sci 7:134–140. CrossRef\nMedline\nPosner MI, Petersen SE (1990) The attention system of the human brain.\nAnnu Rev Neurosci 13:25–42. CrossRef Medline\nQiao L, Zhang L, Chen A, Egner T (2017) Dynamic trial-by-trial recoding of\ntask-set representations in the frontoparietal cortex mediates behavioralexibility. J Neurosci 37:11037–11050.\nMedline\nRainer G, Asaad WF, Miller EK (1998) Selective representation of relevantinformation by neurons in the primate prefrontal cortex. Nature 393:\n577–579. CrossRef Medline\nRavizza SM, Carter CS (2008) Shifting set about task switching: behavioral\nand neural evidence for distinct forms of cognitive exibility. Neuropsy-chologia 46:2924–2935.\nCrossRef Medline\nRogers RD, Monsell S (1995) Costs of a predictible switch between simple\ncognitive tasks. Journal of Experimental Psychology General 124:207.\nCrossRef\nRoy JE, Buschman TJ, Miller EK (2014) PFC neurons reect categorical de-\ncisions about ambiguous stimuli. J Cogn Neurosci 26:1283–1291.\nCrossRef Medline\nRushworth MF, Hadland KA, Paus T, Sipila PK (2002) Role of the human\nmedial frontal cortex in task switching: a combined fMRI and TMS study.J Neurophysiol 87:2577–2592.\nCrossRef Medline\nSarma A, Masse NY, Wang XJ, Freedman DJ (2016) Task-specific versus\ngeneralized mnemonic representations in parietal and prefrontal cortices.Nat Neurosci 19:143–149.\nCrossRef Medline\nSerences JT, Yantis S (2006) Selective visual attention and perceptual coher-\nence. Trends Cogn Sci 10:38–45. CrossRef Medline\nSestieri C, Shulman GL, Corbetta M (2017) The contribution of the human\nposterior parietal cortex to episodic memory. Nat Rev Neurosci 18:183–192.\nCrossRef Medline\nSmith SM, Jenkinson M, Woolrich MW, Beckmann CF, Behrens TE,\nJohansen-Berg H, Bannister PR, De Luca M, Drobnjak I, Flitney DE,Niazy RK, Saunders J, Vickers J, Zhang Y, De Stefano N, Brady JM, Mat-thews PM (2004) Advances in functional and structural MR image anal-ysis and implementation as FSL. Neuroimage 23:S208–S219.\nCrossRef\nMedline\nSprague TC, Serences JT (2013) Attention modulates spatial priority maps\nin the human occipital, parietal and frontal cortices. Nat Neurosci 16:1879–1887.\nCrossRef Medline\nSreenivasan KK, Vytlacil J, D’Esposito M (2014) Distributed and dynamic\nstorage of working memory stimulus information in extrastriate cortex.J Cogn Neurosci 26:1141–1153.\nCrossRef Medline\nSuzuki M, Gottlieb J (2013) Distinct neural mechanisms of distractor sup-\npression in the frontal and parietal lobe. Nat Neurosci 16:98–104.\nCrossRef Medline\nSwaminathan SK, Freedman DJ (2012) Preferential encoding of visual cat-\negories in parietal cortex compared with prefrontal cortex. Nat Neurosci15:315–320.\nCrossRef Medline\nVincent JL, Kahn I, Snyder AZ, Raichle ME, Buckner RL (2008) Evidence\nfor a frontoparietal control system revealed by intrinsic functional con-nectivity. J Neurophysiol 100:3328–3342.\nCrossRef Medline\nVossel S, Geng JJ, Fink GR (2014) Dorsal and ventral attention systems\ndistinct neural circuits but collaborative roles. Neuroscientist 20:150–159.\nCrossRef Medline\nWaskom ML, Kumaran D, Gordon AM, Rissman J, Wagner AD (2014)\nFrontoparietal representations of task context support the exible controlof goal-directed cognition. J Neurosci 34:10743–10755.\nCrossRef Medline\nWaskom ML, Wagner AD (2017) Distributed representation of context by\nintrinsic subnetworks in prefrontal cortex. Proc Natl Acad Sci U S A 114:2030–2035.\nCrossRef Medline\nWoolgar A, Williams MA, Rich AN (2015) Attention enhances multi-voxel\nrepresentation of novel objects in frontal, parietal and visual cortices.Neuroimage 109:429–437.\nCrossRef Medline\nXu Y (2017) Reevaluating the sensory account of visual working memory\nstorage. Trends Cogn Sci 21:794–815. CrossRef Medline\nYeo BT, Krienen FM, Sepulcre J, Sabuncu MR, Lashkari D, Hollinshead M,\nRoffman JL, Smoller JW, Zo ¨llei L, Polimeni JR, Fischl B, Liu H, Buckner\nRL (2011) The organization of the human cerebral cortex estimated by\nintrinsic functional connectivity. J Neurophysiol 106:1125–1165. CrossRef\nMedline\nYeung N, Nystrom LE, Aronson JA, Cohen JD (2006) Between-task compe-\ntition and cognitive control in task switching. J Neurosci 26:1429–1438.\nCrossRef Medline\nZanto TP, Rubens MT, Bollinger J, Gazzaley A (2010) Top-down modula-\ntion of visual feature processing: the role of the inferior frontal junction.Neuroimage 53:736–745.\nCrossRef Medline2504 •J. Neurosci., March 7, 2018 •38(10):2495–2504 Long and Kuhl •Stimulus Features in Attentional Networks"
    }
  ]
}