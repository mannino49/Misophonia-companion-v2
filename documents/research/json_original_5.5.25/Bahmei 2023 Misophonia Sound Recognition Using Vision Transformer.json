{
  "doc_type": "scientific paper",
  "title": "Misophonia Sound Recognition Using Vision Transformer",
  "authors": [
    "Bahmei"
  ],
  "year": 2023,
  "journal": "nature of the trigger sounds, and",
  "doi": null,
  "abstract": "— Misophonia is a condition characterized by an abnormal emotional response to specific sounds, suc h as eating, breathing, and clock ticking noises. Sound classifi cation for misophonia is an important area of research since i t can benefit in the development of interventions and therapies f or individuals affected by the condition. In the area of sound cla ssification, deep learning algorithms such as Convolutional Neur al",
  "keywords": [
    "—Misophonia",
    "Sound Classification"
  ],
  "research_topics": [
    "—Misophonia",
    "Sound Classification"
  ],
  "created_at": "2025-05-05T02:24:15.962972Z",
  "source_pdf": "documents/research/Global/Bahmei 2023 Misophonia Sound Recognition Using Vision Transformer.pdf",
  "sections": [
    {
      "section": "Page 1",
      "page_number": 1,
      "text": "Abstract — Misophonia is a condition characterized by an \nabnormal emotional response to specific sounds, suc h as eating, \nbreathing, and clock ticking noises. Sound classifi cation for \nmisophonia is an important area of research since i t can benefit \nin the development of interventions and therapies f or individuals \naffected by the condition. In the area of sound cla ssification, \ndeep learning algorithms such as Convolutional Neur al \nNetworks (CNNs) have achieved a high accuracy perfo rmance \nand proved their ability in feature extraction and modeling. \nRecently, transformer models have surpassed CNNs as  the \ndominant technology in the field of audio classific ation. In this \npaper, a transformer-based deep learning algorithm is proposed \nto automatically identify trigger sounds and the ch aracterization \nof these sounds using acoustic features. The experi mental results \ndemonstrate that the proposed algorithm can classif y trigger \nsounds with high accuracy and specificity. These fi ndings \nprovide a foundation for future research on the dev elopment of \ninterventions and therapies for misophonia. \n \nKeywords—Misophonia, Sound Classification, \nTransformer Models, Deep Learning \nI.  INTRODUCTION  \nMisophonia is a relatively new and understudied con dition \ncharacterized by an abnormal emotional response to specific \nsounds that are often repetitive and human-produced  (e.g., \nchewing, snoring, tapping, sniffing, etc.), eliciti ng excessive \nand inappropriate negative reactions even at low am plitudes \n[1], [2]. People who suffer from misophonia experie nce \nincreased sympathetic nervous system arousal, accom panied \nby emotional distress in response to these sounds, which are \nknown as trigger sounds. The condition can have a s ignificant \nimpact on an individual's daily life, leading to di fficulties in \nsocial and professional settings [2]. \nMost of the studies on misophonia have been case st udies \naiming to uncover the nature of the trigger sounds,  and \nphysical responses to those sounds [3]–[5]. However , there \nare few publications and studies to evaluate treatm ents for \nmisophonia [6]–[8]. In the absence of these studies , it is very \nchallenging for families and clinicians to provide appropriate \ncare to individuals who are suffering from misophon ia. \nIdentifying and categorizing sounds associated with  \nmisophonia can assist in creating treatments and th erapies for \npeople who have the condition [9]. Upon detecting t he trigger \nsounds, further considerations can be evaluated to ameliorate \nthe condition such as notifying of sound's presence , filtering \nout the sound, masking the sound, and so on. \n \nB. Bahmei (bbahmei@sfu.ca), School of Mechatronics System Engineering, \nSimon Fraser University, Surrey, BC, Canada \nE. Birmingham (elina_birmingham@sfu.ca), Faculty of  Education, Simon \nFraser University, Burnaby, BC, Canada \nS. Arzanpour (arzanpour@sfu.ca), School of Mechatro nics System \nEngineering, Simon Fraser University, Surrey, BC, C anada  In recent years, there has been an increasing inte rest in \nusing machine learning algorithms to automatically classify \nenvironmental sounds such as Support Vector Machine  \n(SVM) [10], and hidden Markov models (HMM) [11]. \nRecently, deep learning techniques have been introd uced to \nenhance the recognition performance of environmenta l \nsounds [12], [13]. Deep neural networks can automat ically \nlearn and extract features from the raw data, which  reduces \nthe need for manual feature engineering, and allows  the model \nto capture more complex patterns and dependencies i n the \ndata  [12]. Deep learning algorithms such as Convol utional \nNeural Networks (CNNs) [13], [14], Recurrent Neural  \nNetworks (RNNs) [15], [16], and their combination [ 17] have \nbeen shown to be highly effective in accurately ide ntifying \nenvironmental sounds, with high sensitivity and spe cificity. \nAlthough CNNs have been used to classify sounds, th ey are \nless successful at processing long sequences of aud io data.  \nLately, there has been a growing trend of utilizing  attention \nmechanisms to concentrate on the essential aspects of the \nsound being analyzed are designed to handle long se quences \nof data. Attention-based models [18], particularly those using \nTransformers, have been gaining popularity in recen t years. \nTransformers are a type of neural network that reli es solely \non attention mechanisms. This makes them well-suite d for \nparallel computations and the incorporation of glob al context, \nleading to more accurate results. As a result, they  have \nbecome a popular choice in various fields such as N atural \nLanguage Processing (NLP), computer vision, and mor e \nrecently, areas related to sound. \nIn the field of audio classification, there have be en a \nnumber of transformer models proposed [19]. Some of  the \nproposed models have investigated the benefits of u sing \nBidirectional Encoder Representations from Transfor mers \n(BERT) models [20]. The BERT models take a given to ken \nand the position embeddings as input, to address th e problem \nof sound classification at the edge. Similarly, an Audio \nSpectrogram Transformer (AST) is proposed in [21] t hat is \ncompletely based on attention-based models. In anot her \nstudy, the use of AST is explored with a Many-to-Ma ny \nAudio Spectrogram Transformer (M2M-AST), which can \noutput sequences with different resolutions for mul ti-channel \naudio inputs [22]. To simplify the training process , the drop \ntoken technique is introduced in combination with a  Video–\nAudio–Text Transformer (VATT) model, which achieved  \ncompetitive results [23]. All these studies achieve d significant \nperformance in the task of audio classification. \nThis paper proposed a sound classifier for misophon ia to \nrecognize trigger sounds in the environment. The pu rpose of \nthis study is to propose a trigger sound recognitio n system for \nthe sufferer of misophonia which can provide a foun dation for \nfuture research on the development of interventions  and Misophonia Sound Recognition Using Vision Transform er \nB. Bahmei, E. Birmingham, and S. Arzanpour"
    },
    {
      "section": "Page 2",
      "page_number": 2,
      "text": "therapies. In this paper, a standard transformer mo del which \ninitially is applied to images, named Vision Transf ormer \n(ViT) is considered and modified for the misophonia  sound \nclassification [24]. The model is trained and evalu ated on \nselected sounds from the ESC-50 dataset [25] which are \ncommonly reported as trigger sounds in misophonia. The \nexperimental results indicate that the proposed mod el has the \ncapability to accurately identify the target trigge r sounds in \nthe environment. To summarize, the main contributio ns of \nthis paper are as follows: \n1) A ViT model is considered and modified to achiev e a \nvery high-level of classification accuracy in sound  \nrecognition. \n 2) The classification model is applied on selected  trigger \nsounds for misophonia to introduce the first misoph onia \nsound recognition system. \nThe composition of the paper is as the following: I n Section \nII, the methods including the ViT, and dataset are discussed. \nSection III provides details about the experimental  results. \nFinally, the conclusions are presented in Section I V. \n \nII.  METHODS \nIn this section, the methods for ViT, and the datas et are \nexplained. \nA. Vision Transformer (ViT) \nThe transformer model, first introduced in 2017, us es an \nattention mechanism to generate representations of its inputs \nand outputs [18]. The transformer model has two mai n \ncomponents, an encoder, and a decoder. The encoder converts \nan input sequence of symbol representations into a sequence \nof continuous representations, and the decoder gene rates an \noutput sequence of symbols one at a time. Additiona lly, the \nmodel is autoregressive, meaning that it uses previ ously \ngenerated symbols as input when generating the next  one at \neach step. \nThe architecture used here is based on the ViT mode l \nproposed in [24]. This model breaks down an image i nto \nfixed-size patches, accurately embeds each one, and  \nincorporates positional embedding as input to the t ransformer \nencoder. The transformer encoder embeds information  \nglobally across the entire image, and during traini ng, the \nmodel learns to encode the relative location of the  image \npatches to rebuild the image's structure. Furthermo re, a \nclassification token is added to learn the informat ion extracted \nby the transformer encoder for the classification t ask.  \nA schematic view of the designed model in this pape r is \ndepicted in Figure 1. The input to the model is a 2 D \nspectrogram. A spectrogram is a logarithmic frequen cy scale \nand is considered one of the most common and effect ive \nfeatures for audio recognition [13]. In order to ex tract patches \nthat are required for a transformer, the raw audio signal will \nbe framed into 18 frames with a length of 32 millis econds. \nThe frames have a 50% overlap to avoid missing \ninformation at the edges. Afterward, the spectrogra ms of each \nframe are extracted, stacked together as the input patches, and \nfed to the transformer encoder as the input.  The o utput of the \ntransformer encoder known as encoded patches is fla ttened \nand fed to the fully connected layers for the class ification task.   \nFigure 1 Transformer model schematic view \n \nFigure 2 Transformer encoder block \n \nThe output of the model is the one-hot representati on of \nsound classes. \nThe transformer encoder block is presented in Figur e 2. \nThe transformer encoder includes: \n• The Multi-Head Self Attention (MSA) Layer, or also \nknown as Multi-Head Attention (MHA) is a key \ncomponent of the transformer encoder. It allows the  \nmodel to attend to different positions of the input  \nsequence simultaneously, by performing multiple \nself-attention operations with different weight \nmatrices, also known as heads. These attention head s \nhelp to train both local and global dependencies in  the \ninput. This allows the model to learn and capture \nmore complex patterns in the input data and improve  \nits performance. \n• The Multi-Layer Perceptron (MLP) Layer, also \nknown as the Position-wise Fully Connected Feed-"
    },
    {
      "section": "Page 3",
      "page_number": 3,
      "text": "Forward Network, is another key component of the \ntransformer encoder. It is a simple feed-forward \nneural network that is applied to each position of the \ninput sequence independently and in parallel. The \nrole of this layer is to learn and capture more com plex \npatterns in the input data that the self-attention layer \nmight have missed. \n• Layer Normalization (LN), also known as Layer \nNorm, is a technique used to normalize the \nactivations of the neurons in a neural network laye r. \nIt is typically applied before each block, such as the \nMHA and MLP layers, as it does not introduce any \nnew dependencies between the training images. This \nhelps to improve the training time and overall \nperformance of the model.  \nThe classification head in Figure 1 is implemented using \nMLP with four fully connected layers. In the classi fication \nhead, the ReLU activation function is used after ea ch layer \nexcept the last one. For the last layer, the SoftMa x activation \nfunction is applied. There is batch normalization a fter each \nlayer. The ADAM optimizer [26] is used in order to update \nnetwork weights.  \nB. Dataset \nThe ESC-50 dataset is a popular dataset for sound \nclassification which includes 50 classes consisting  of animal, \nhuman, natural, and urban sounds. Seven sounds are selected \nfrom this dataset as the commonly reported trigger sounds for \nmisophonia including breathing, snoring, drinking, keyboard \ntyping, clock ticking, mouse-clicking, and coughing . For each \nclass, there are 40 audio recordings, each lasting 4 seconds in \nduration. In this paper, as chewing sounds are comm only \nreported in the literature as a trigger sound for m isophonia, 40 \nsamples of chewing sounds were collected from frees ound.org \nand added to the dataset. In total, the dataset inc ludes eight \ntrigger sounds as the target output sounds. \n \nIII.  RESULTS  AND  DISCUSSION \nIn this section, the simulation results of the prop osed \ntechnique are presented. Considering that there is no specific \npublication about misophonia sound recognition, it is not \npossible to directly compare our work with others i n this \nspecific area. However, several experiments are con ducted to \nevaluate the performance and accuracy of the propos ed \nmethod.   \nThe model is trained over 100 epochs using a batch size of \n32. Since there are few misophonia sounds in the da taset, \napplying k-fold cross-validation would provide even  fewer \ntraining and validation sets, which might result in  overfitting \nand reduced generalization performance. Therefore, the \ndataset is split into 80% for training, 15% for val idation, and \n5% for testing. The data samples are shuffled befor e feeding \nto the model. Figure 3 shows the overall accuracy a nd loss of \nthe training and validation set of the model on eac h training \niteration. \nFor this study, the categorical cross-entropy loss is \nconsidered. This figure shows that the validation a ccuracy and \nloss of the model reach 92.29% and 0.1956, respecti vely which  \n \nFigure 3 . Overall accuracy and loss \n \n \nFigure 4 Confusion Matrix. The diagonal elements re present the \npercentage of instances for which the predicted lab el is equal to the \ntrue label (TP and TN), while off-diagonal elements  are those that \nare mislabeled by the classifier (FP an FN). \n \nshows the performance of the model to recognize tri gger \nsounds. It also indicates that the learning process  is quite \nconsistent and there is no bias or variance during the training. \nIn addition, a confusion matrix is also used to eva luate the \nperformance of the classifier presented in Figure 4 . It shows \nthe percentage of true positive (TP), false positiv e (FP), false"
    },
    {
      "section": "Page 4",
      "page_number": 4,
      "text": "negative (FN), and true negative (TN) predictions m ade by a \nmodel. It can be seen from Figure 4 that the most d ifficult \nclasses to classify were drinking, breathing, and m ouse-\nclicking. They have been misclassified as coughing in some \nsamples. However, it is noteworthy that the coughin g, chewing \nand clock ticking sounds are almost not misclassifi ed. From \nthe confusion matrix, some performance metrics incl uding \nprecision, recall and F1 score are computed and sho wn in \nTable 1 to evaluate the classification results. \n \nTable 1 Performance metrics for the model, showing precision, \nrecall, and F1 score for each class \n Precision Recall F1 score \nBreathing 1 0.97 0.98 \nCoughing 0.92 1 0.95 \nSnoring 1 0.98 0.99 \nDrinking 1 0.96 0.98 \nMouse Click 1 0.97 0.98 \nKeyboard Typing 1 0.98 0.99 \nClock tick 1 1 1 \n \nIV.  CONCLUSION \nIn this paper, a vision transformer-based deep lear ning \nmodel is evaluated for misophonia sound classificat ion. The \naccuracy of the model indicates that the system can  recognize \nthe trigger sounds accurately in the environment. I t is a \nfoundation and starting point for designing interve ntion \ntechniques and therapies for the sufferer of misoph onia. This \nis the first study conducted for detecting trigger sounds for \nmisophonia using artificial intelligence techniques . Further \ninvestigation can be conducted to use these results .  \nETHICS STATEMENT  \nThis paper does not include any experimental proced ures \ninvolving human subjects or animals. \nACKNOWLEDGMENT  \nThis project is supported by funding from the Kids Brain \nHealth Network (KBHN). \nREFERENCES  \n[1] N. E. Scheerer, T. Q. Boucher, B. Bahmei, G. Ia rocci, S. Arzanpour, \nand E. Birmingham, “Family Experiences of Decreased  Sound \nTolerance in ASD,” J. Autism Dev. Disord. , 2021, doi: \n10.1007/S10803-021-05282-4. \n[2] J. J. Brout et al. , “Investigating Misophonia: A Review of the \nEmpirical Literature, Clinical Implications, and a Research Agenda,” \nFront. Neurosci. , vol. 0, no. FEB, p. 36, Feb. 2018, doi: \n10.3389/FNINS.2018.00036. \n[3] M. Edelstein, D. Brang, R. Rouw, and V. S. Rama chandran, \n“Misophonia: Physiological investigations and case descriptions,” \nFront. Hum. Neurosci. , vol. 7, no. JUN, p. 296, Jun. 2013, doi: \n10.3389/FNHUM.2013.00296/BIBTEX. \n[4] H. Tinnitus, M. G. Editors, D. F. Duddy, . D Au , and L. A. Flowers, \n“Treatments for Decreased Sound Tolerance (Hyperacu sis and \nMisophonia),” Au.D. Semin Hear , vol. 35, pp. 105–120, 2014, doi: \n10.1055/s-0034-1372527. \n[5] E. Boucher, T. Q., Scheerer, N. E., Iarocci, G. , Bahmei, B., Arzanpour, \nS., & Birmingham, “Misophonia, hyperacusis, and the  relationship \nwith quality of life in autistic and non-autistic a dults,” 2021. \n[6] R. L. Schneider and J. J. Arch, “Case study: A novel application of \nmindfulness- and acceptance-based components to tre at misophonia,” J. Context. Behav. Sci. , vol. 6, no. 2, pp. 221–225, Apr. 2017, doi: \n10.1016/J.JCBS.2017.04.003. \n[7] A. E. Schröder, N. C. Vulink, A. J. van Loon, a nd D. A. Denys, \n“Cognitive behavioral therapy is effective in misop honia: An open \ntrial,” J. Affect. Disord., vol. 217, pp. 289–294, Aug. 2017, doi: \n10.1016/J.JAD.2017.04.017. \n[8] A. Schröder, N. Vulink, and D. Denys, “Misophon ia: Diagnostic \nCriteria for a New Psychiatric Disorder,” PLoS One,  vol. 8, no. 1, Jan. \n2013, doi: 10.1371/JOURNAL.PONE.0054706. \n[9] B. Birmingham, E., Arzanpour, S., Bahmei, “Syst em and Method for \nAmbient Noise Detection, Identification and Managem ent,” \nWO/2021/119806, 2021. \n[10] S. Sameh and Z. Lachiri, “Multiclass support v ector machines for \nenvironmental sounds classification in visual domai n based on log-\nGabor filters,” undefined, vol. 16, no. 2, pp. 203– 213, Jun. 2013, doi: \n10.1007/S10772-012-9174-0. \n[11] Y. T. Peng, C. Y. Lin, M. T. Sun, and K. C. Ts ai, “Healthcare audio \nevent classification using hidden Markov models and  hierarchical \nhidden Markov models,” Proc. - 2009 IEEE Int. Conf.  Multimed. \nExpo, ICME 2009, pp. 1218–1221, 2009, doi: \n10.1109/ICME.2009.5202720. \n[12] K. J. Piczak, “Environmental sound classificat ion with convolutional \nneural networks,” IEEE Int. Work. Mach. Learn. Sign al Process. \nMLSP, vol. 2015-November, Nov. 2015, doi: \n10.1109/MLSP.2015.7324337. \n[13] J. Salamon and J. P. Bello, “Deep Convolutiona l Neural Networks and \nData Augmentation for Environmental Sound Classific ation,” IEEE \nSignal Process. Lett., vol. 24, no. 3, pp. 279–283,  Mar. 2017, doi: \n10.1109/LSP.2017.2657381. \n[14] S. Adapa, “Urban Sound Tagging using Convoluti onal Neural \nNetworks,” pp. 5–9, Sep. 2019, doi: 10.33682/8axe-9 243. \n[15] Y. Aytar, C. Vondrick, and A. Torralba, “Sound Net: Learning Sound \nRepresentations from Unlabeled Video,” Adv. Neural Inf. Process. \nSyst., pp. 892–900, Oct. 2016, Accessed: Jan. 11, 2 022. [Online]. \nAvailable: https://arxiv.org/abs/1610.09001v1. \n[16] T. H. Vu and J.-C. Wang, “Acoustic Scene and E vent Recognition \nUsing Recurrent Neural Networks,” 2016. \n[17] B. Bahmei, E. Birmingham, and S. Arzanpour, “C NN-RNN and Data \nAugmentation Using Deep Convolutional Generative Ad versarial \nNetwork For Environmental Sound Classification,” IE EE Signal \nProcess. Lett., 2022, doi: 10.1109/LSP.2022.3150258 . \n[18] A. Vaswani et al., “Attention is All you Need, ” Adv. Neural Inf. \nProcess. Syst., vol. 30, 2017. \n[19] P. Remagnino et al., “Transformers for Urban S ound Classification—\nA Comprehensive Performance Evaluation,” mdpi.com, 2022, doi: \n10.3390/s22228874. \n[20] J. Devlin, M. W. Chang, K. Lee, and K. Toutano va, “BERT: Pre-\ntraining of Deep Bidirectional Transformers for Lan guage \nUnderstanding,” NAACL HLT 2019 - 2019 Conf. North A m. Chapter \nAssoc. Comput. Linguist. Hum. Lang. Technol. - Proc . Conf., vol. 1, \npp. 4171–4186, Oct. 2018, doi: 10.48550/arxiv.1810. 04805. \n[21] Y. Gong, Y. A. Chung, and J. Glass, “AST: Audi o Spectrogram \nTransformer,” Proc. Annu. Conf. Int. Speech Commun.  Assoc. \nINTERSPEECH, vol. 1, pp. 56–60, Apr. 2021, doi: \n10.48550/arxiv.2104.01778. \n[22] S. Park, Y. Jeong, T. L.- DCASE, and  undefine d 2021, “Many-to-\nMany Audio Spectrogram Tansformer: Transformer for Sound Event \nLocalization and Detection.,” dcase.community, Acce ssed: Jan. 20, \n2023. doi: \nhttps://dcase.community/documents/workshop2021/proc eedings/DC \nASE2021Workshop_Park_39.pdf \n[23] K. Koutini, J. Schlüter, H. Eghbal-Zadeh, and G. Widmer, “Efficient \nTraining of Audio Transformers with Patchout,” Proc . Annu. Conf. \nInt. Speech Commun. Assoc. INTERSPEECH, vol. 2022-S eptember, \npp. 2753–2757, 2021, doi: 10.21437/INTERSPEECH.2022 -227. \n[24] A. Dosovitskiy et al., “An Image is Worth 16x1 6 Words: Transformers \nfor Image Recognition at Scale,” Oct. 2020, doi: \n10.48550/arxiv.2010.11929. \n[25] K. J. Piczak, “ESC: Dataset for environmental sound classification,” \nin MM 2015 - Proceedings of the 2015 ACM Multimedia  Conference, \nOct. 2015, pp. 1015–1018, doi: 10.1145/2733373.2806 390. \n[26] D. P. Kingma and J. L. Ba, “Adam: A Method for  Stochastic \nOptimization,” 3rd Int. Conf. Learn. Represent. ICL R 2015 - Conf. \nTrack Proc., Dec. 2014, doi:  https://arxiv.org/abs /1412.6980v9."
    }
  ]
}