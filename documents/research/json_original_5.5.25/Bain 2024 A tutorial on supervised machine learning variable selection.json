{
  "doc_type": "scientific paper",
  "title": "A tutorial on supervised machine learning variable selection",
  "authors": [
    "Bain"
  ],
  "year": 2024,
  "journal": "Sciences in R",
  "doi": "10.21203/rs.3.rs-4425792/v1",
  "abstract": null,
  "keywords": [
    "Machine Learning",
    "Random Forest",
    "SVM",
    "LASSO",
    "Elastic Net",
    "Variable Selection",
    "Supervised"
  ],
  "research_topics": [
    "Machine Learning",
    "Random Forest",
    "SVM",
    "LASSO",
    "Elastic Net",
    "Variable Selection",
    "Supervised"
  ],
  "created_at": "2025-05-05T01:51:24.512800Z",
  "source_pdf": "documents/research/Global/Bain 2024 A tutorial on supervised machine learning variable selection.pdf",
  "sections": [
    {
      "section": "Page 1",
      "page_number": 1,
      "text": "A Tutorial on Supervised Machine Learning Variable\nSelection Methods for the Social and Health\nSciences in R\nCatherine M. Bain \nUniversity of Oklahoma\nDingjing Shi \nUniversity of Oklahoma\nLauren E. Ethridge \nUniversity of Oklahoma\nJordan E. Norris \nUniversity of Oklahoma\nJordan E. Loeffelman \nUniversity of Oklahoma\nResearch Article\nKeywords:  Machine Learning, Random Forest, SVM, LASSO, Elastic Net, Variable Selection, Supervised\nLearning, Genetic Algorithm, Metaheuristic, R, Misophonia\nPosted Date:  June 5th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-4425792/v1\nLicense:    This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations:  No competing interests reported."
    },
    {
      "section": "Page 2",
      "page_number": 2,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 1 \n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \nA Tutorial on Supervised Machine Learning Variable Selection Methods for the Social and 9 \nHealth Sciences in R 10 \nCatherine M. Bain1*, Dingjing Shi1, Lauren E. Ethridge1,2, Jordan E. Norris1, & Jordan E. 11 \nLoeffelman1 12 \n1Department of Psychology, University of Oklahoma, Norman, OK, United States of America. 13 \n2Department of Pediatrics, Section on Developmental and Behavioral Pediatrics, University of 14 \nOklahoma Health Sciences Center, Oklahoma City, OK, United States of America. 15 \nCorrespondence concerning this article should be addressed to Catherine M. Bain, Department of 16 \nPsychology, University of Oklahoma, 455 W. Lindsey Street, Dale Hall Tower, Room 705, Norman, 17 \nOK 73019, United States. Email: cbain1@ou.edu  18 \nKeywords: Machine Learning; Random Forest; SVM; LASSO; Elastic Net; Variable Selection; 19 \nSupervised Learning; Genetic Algorithm; Metaheuristic; R; Misophonia 20 \n 21 \n 22 \n 23 \n 24 \n 25 \n 26 \n 27 \n 28 \n 29"
    },
    {
      "section": "Page 3",
      "page_number": 3,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 2 \nPrimary Abstract 30 \nWith recent increases in the size of datasets currently available in the behavioral and health 31 \nsciences, the need for efficient and effective variable selection techniques has increased. A 32 \nplethora of techniques exist, yet only a few are used within the psychological sciences (e.g., 33 \nstepwise regression, which is most common, the LASSO, and Elastic Net). The purpose of this 34 \ntutorial is to increase awareness of the various variable selection methods available in the popular 35 \nstatistical software R, and guide researchers through how each method can be used to select 36 \nvariables in the context of classification using a recent survey-based assessment of misophonia. 37 \nSpecifically, readers will learn about how to implement and interpret results from the LASSO, 38 \nElastic Net, a penalized SVM classifier, an implementation of random forest, and the genetic 39 \nalgorithm. The associated code and data implemented in this tutorial are available on OSF to 40 \nallow for a more interactive experience. This paper is written with the assumption that individuals 41 \nhave at least a basic understanding of R.  42 \n 43 \n 44 \n 45 \n 46 \n 47 \n 48 \n 49 \n 50 \n 51 \n 52 \n 53 \n 54 \n 55 \n 56 \n 57 \n 58 \n 59 \n 60 \n 61 \n 62 \n 63 \n 64"
    },
    {
      "section": "Page 4",
      "page_number": 4,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 3 \nA Tutorial on Supervised Machine Learning Variable Selection Methods for the Social and 65 \nHealth Sciences in R 66 \nIn the field of behavioral and health sciences, selecting the right variables for a model is a 67 \ncrucial step in working to understand how aspects of human behavior are related. For example, we 68 \nmight want to know how one’s personality or other traits affect their engagement with a particular 69 \ntype of treatment or how the symptoms of a particular disorder may present. In many cases, 70 \nresearchers not only want to understand how these aspects (i.e., variables) are related to each other 71 \nand to overarching constructs but may also want to use the variables to classify individuals into 72 \ngroups (e.g., diagnosing clinical disorders, determining participant complian ce, etc.). The accuracy 73 \nof these classifications or predictions is greatly influenced by which variables a researcher uses to 74 \ncreate the classifications. For example, if a researcher is interested in diagnosing someone with 75 \ndepression, the accuracy of the diagnosis would suffer if relying solely on the presence of a 76 \ndepressed mood. However, if they use a variety of variables like depressed mood, loss of interest in 77 \nactivities, hours slept, and change in appetite or weight, their classification would be more accurate.  78 \nEssentially, researchers must think critically about building their classification model. A good 79 \nmodel not only allows researchers to understand the interrelationships among variables, but also 80 \ncreates an accurate model in terms of predicted classes. Variable selection techniques can help 81 \nresearchers to identify and select informative variables to build these models. The use of variable 82 \nselection techniques can lead to more accurate predictions, reduce the computational cost of creating 83 \nthe model, and improve the parsimony of the model by eliminating redundant and irrelevant 84 \nvariables. For example, variable selection techniques have been used to build models pertaining to 85 \nidentifying exposure-outcome associations [1] as well as predicting mortality rates [2,3], psychological 86 \nstrain in teachers [4], and nomophobia [5]. 87"
    },
    {
      "section": "Page 5",
      "page_number": 5,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 4 \nBehavioral researchers often turn to stepwise regression to perform variable selection. An 88 \nAPA PsychINFO database search for the term “stepwise regression ” returned 222 peer-reviewed 89 \narticles published in the last 3 years using stepwise regression for variable selection. Stepwise 90 \nregression, however, has many severe limitations and statistical experts do not recommend its use in 91 \nany context. These limitations include the inability to distinguish signal (i.e., true predictor variables) 92 \nfrom noise [6–9], underestimation of p-values, and failure to replicate [10,11]. As such, many alternative 93 \nvariable selection algorithms have been proposed in the literature, but behavioral researchers have 94 \nbeen slow to adopt these new methods in place of more traditional methods [12,13]. One potential 95 \nreason for this delay may be the disconnect between methodological and applied behavioral 96 \nresearchers, as much methodological research is often inaccessible for applied researchers at first 97 \n(e.g., complex techniques, lack of published code, or no tutorials). An APA PsychINFO database 98 \nsearch for the term “variable selection ” returned 253 papers published in quantitative methods 99 \njournals in the last 20 years, indicating that methodological researchers are dedicated to developing 100 \nbetter approaches to variable selection than stepwise regression. Of these publications, however, only 101 \none is a tutorial [14].  102 \nGiven the clear gap in the popularity of variable selection methodological research and lack 103 \nof tutorials on how to apply them, the field would benefit greatly from additional tutorials on 104 \nvariable selection techniques with demonstrations of how to apply them to psychological datasets. 105 \nThe goal of this pap er is to provide a tutorial on five variable selection techniques freely available to 106 \nresearchers in R. We will introduce the Least Absolute Shrinkage and Selection Operator (LASSO), 107 \nElastic Net, a version of the genetic algorithm (GA), and implementations of Support Vector 108 \nMachines (SVMs) and Random Forest that have been adapted to perform variable selection. The 109 \nmanuscript is organized as follows. The first section illustrates the importance of variable selection 110 \nin machine learning and explains why each of the five methods method were selected. Then, a 111"
    },
    {
      "section": "Page 6",
      "page_number": 6,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 5 \nmotivating example is provided pertaining to diagnosis of misophonia. The dataset was collected 112 \nfrom a psychology research pool and represents an excellent example of an average dataset available 113 \nto many behavioral and health researchers [15]. Within this example, there are three major sections. 114 \nThe first discusses methods using a logistic regression model (i.e., LASSO, EN, and the GA), the 115 \nsecond discusses SVM, and the third pertains to random forest. Each technique is introduced, the 116 \ncode necessary to implement each technique is provided, and each technique ’s associated strengths 117 \nand weaknesses are discussed.   118 \nVariable Selection in Machine Learning 119 \nVariable selection is a fundamental step in the process of building robust and efficient 120 \nmachine learning models, and its importance cannot be overstated [16,17]. It serves as a critical 121 \nmechanism for optimizing model performance and ensuring its reliability across various tasks and 122 \ndatasets. Variable selection is advantageous with any model (e.g., regression, structural equation 123 \nmodeling, etc.) because, as mentioned previously, it leads to more accurate predictions, reduces the 124 \ncomputational cost of the model, and improves the parsimony of the model by eliminating redundant 125 \nand irrelevant variables. However, there are additional advantages of variable selection when paired 126 \nwith machine learning models. First, variable selection helps to manage dimensionality problems 127 \n(i.e., when a dataset contains more predictors than it does observations). Over the years, technology 128 \nsuch as the invention of online data collection platforms like Prolific or the creation of mobile health 129 \napps has allowed researchers to collect more complex data from increasingly larger samples. As 130 \ndatasets grow in both size and complexity, the number of variables may also increase, leading to 131 \ncomputational inefficiencies and reduced model interpretability [18]. By carefully selecting relevant 132 \nvariables, we can effectively reduce the dimensionality of the data, thereby streamlining the 133 \ncomputational process and facilitating easier interpretation of the model [19]. 134"
    },
    {
      "section": "Page 7",
      "page_number": 7,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 6 \nMoreover, the process of variable selection enables models to achieve higher accuracy and 135 \nbetter generalization capabilities. For example, van Vuuren and colleagues [20] found that LASSO 136 \ncreated a model that was able to classify students as at risk for suicide with a higher accuracy than 137 \nsimple inclusion rules (i.e., predicting based on history of suicide alone). Pratik and colleagues [21] 138 \nutilized Elastic Net to select variables that were able to predict smoking addiction in young adults 139 \nwith higher accuracy than previous research. By focusing on the most informative variables, the 140 \nmodel can discern meaningful patterns within the data, leading to more precise predictions and 141 \nimproved performance on either unseen or new data. This selective approach prevents the model 142 \nfrom being overwhelmed by noise or irrelevant information, allowing it to focus on capturing the 143 \nunderlying relationships that drive the outcome of interest. For example, researchers found that 144 \napplying Elastic Net regularization to classifiers based on clinical notes reduced the number of 145 \nfeatures selected by more than a thousandfold, making these classifiers more easily interpretable as 146 \nwell as maintaining performance [22].  147 \nFurthermore, the inclusion of irrelevant variables in the modeling process can introduce bias 148 \nand adversely affect the estimation of model parameters. Additional, extraneous variables may 149 \nintroduce noise or confounding factors, leading to skewed parameter estimates and potentially 150 \nmisleading conclusions [23]. By excluding such variables through proper selection techniques, we can 151 \nensure that the model ’s estimates remain unbiased and reflective of the true underlying relationships 152 \nin the data which increases the ecological validity of study results and models produced. 153 \nLastly, a well-selected set of variables not only enhances the model ’s predictive performance 154 \nbut also contributes to its stability and reliability [24,25]. Models built on a carefully chosen subset of 155 \nvariables are less susceptible to overfitting, where the model simply memorizes the data rather than 156 \nlearning meaningful patterns. Avoiding overfitting leads to more robust models that generalize better 157"
    },
    {
      "section": "Page 8",
      "page_number": 8,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 7 \nand are less prone to erratic behavior or unexpected deviations which may lead to harmful 158 \nclassifications (e.g., classifying an individual as having a particular disorder when they do not ) [26,27]. 159 \nPut simply, variable selection is indispensable in the realm of machine learning. It serves as a 160 \ncornerstone for improving computational efficiency, enhancing model accuracy and generalization, 161 \nreducing bias in parameter estimation, and fostering the stability and reliability of the resulting 162 \nmodels. As such, behavioral and health researchers must employ rigorous techniques and 163 \nconsiderations during the variable selection process to ensure the effectiveness and generalizability 164 \nof their models and conclusions. 165 \nThe five techniques utilized in this paper were chosen for a variety of reasons. First and 166 \nforemost, LASSO and Elastic Net are arguably the most popular modern variable selection 167 \ntechniques within the behavioral sciences. Social psychology researchers have used such techniques 168 \nto create better environments that promote prosocial environments for children [28] and health 169 \nresearchers have used them to model the progression of Alzheimer’s disease  [29]. Implementations of 170 \nSVM and random forest were chosen because of their strength as classification algorithms and 171 \nbecause they can handle more complex data types (e.g., mixed variable types or non-linearly 172 \nseparable). Lastly, the GA was chosen 1) to introduce the reader to the concept of metaheuristic 173 \napproaches to variable selection, and 2) because it has been shown to outperform more common 174 \nmethods like LASSO and Elastic Net across a variety of different data conditions [30].  175 \nA Motivating Example 176 \n This tutorial uses the assessment of misophonia as an example through which we illustrate 177 \neach technique. Individuals with misophonia experience strong, negative, emotion al responses to 178 \nspecific sounds (i.e., triggers ) [31]. The original data sample consisted of undergraduate students (N 179 \n= 343) at a large southwestern university. Participants were predominately white (76.7%), female 180 \n(69.7 %) and students (96.5%) ranging from ages 18 to 36 (M = 18.96, SD = 1.7). The dataset 181"
    },
    {
      "section": "Page 9",
      "page_number": 9,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 8 \ncontains 106 independent variables related to both direct characteristics of misophonia and related 182 \ncharacteristics and one self-report binary diagnosis variable. Since misophonia is still not fully 183 \nunderstood (i.e., diagnostic criteria have not been set, and researchers are still trying to determine 184 \nthe most important symptoms), this dataset functions well as an illustrative example for variable 185 \nselection. It is possible that some symptoms are unimportant for, or not predictive of, a true 186 \nmisophonia diagnosis. One should note that this dataset does not contain any missing data, as it 187 \nwas handled a priori using listwise deletion. For more information on the larger previously 188 \npublished dataset from which this data was selected, and background on misophonia, see work by 189 \nNorris and colleagues [15]. One common problem for implementing variable selection techniques is 190 \nmodel overfitting (see Figure 1). Cross validation is one common way to circumvent overfitting. 191 \nThis paper implements holdout cross-validation which occurs when one splits the data into test 192 \nand training sets before conducting any analyses. Typically in holdout cross-validation, 70% of 193 \nthe data is used for the training set and the remaining 30% is used for the test dataset. This 194 \npractice was followed in this tutorial.  195 \nMethods 196 \nLogistic Regression Models  197 \nRegularization Techniques 198 \n 199 \nTwo of the techniques discussed in this paper, LASSO and Elastic Net, are regularization 200 \ntechniques. Regularization, within the context of classification problems, takes the following 201 \nform:  202 \n𝐿𝑅𝑒𝑔(β)= 𝐿𝑙𝑜𝑔𝑖𝑠𝑡𝑖𝑐(β)− λ𝑃 (β) (1) \nwhere 𝐿𝑅𝑒𝑔 is the penalized optimization function, 𝐿𝑙𝑜𝑔𝑖𝑠𝑡𝑖𝑐 is the logistic regression function, 𝜆 is 203 \na regularization parameter (i.e., a tuning parameter), and 𝑃 is a penalty function that will vary 204 \nacross regularization technique. The goal of regularization is to find the optimal balance between 205"
    },
    {
      "section": "Page 10",
      "page_number": 10,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 9 \nbias (generalizability of the model) and variance (specific model fit [32]. Finding this balance is 206 \nachieved via the magnitude of the lambda ( 𝜆) penalty. A larger lambda will lead to a sparser and 207 \nmore generalizable model. One popular technique utilized to determine the value of the lambda 208 \nparameter is cross-validation. As mentioned above, cross-validation occurs when the data is split 209 \ninto multiple subsets, the model is developed (i.e. trained) on a subset, and evaluated (i.e., 210 \nvalidated) on another. This process is iterative, allowing for the selection of the lambda penalty 211 \nthat minimizes prediction error across different subsets. 212 \n An optimal model, in the context of this paper, is one that produces the most accurate 213 \nclassifications. Accuracy can be calculated using the following equation:  214 \nTP + TN \nAccuracy =  \nTP + TN + FP + FN (2) \n 215 \nwhere TP is the number of individuals who were correctly classified as having a diagnosis of 216 \nmisophonia, TN is the number of individuals who were correctly classified as not having a 217 \ndiagnosis of misophonia, FP is the number of individuals who were classified as having a 218 \ndiagnosis but did not truly have a diagnosis in the labeled data, and FN are the number of 219 \nindividuals who were incorrectly classified as not having a diagnosis when a diagnosis was 220 \npresent in the labeled data. It is worth noting that researchers may want to use a weighted 221 \naccuracy in their own research depending on the relative importance of a false positive versus a 222 \nfalse negative. For example, a clinician attempting to predict suicide attempts may prioritize a 223 \nfalse positive (i.e., saying the individual is likely to attempt suicide when they do not actually 224 \nattempt) over a false negative (i.e., saying the individual will not attempt when they actually will). 225 \nNon-weighted accuracy was chosen here for ease of explanation.  226 \nLASSO 227"
    },
    {
      "section": "Page 11",
      "page_number": 11,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 10 \nLASSO [33] is one of two penalized regression techniques that perform variable selection. 228 \nLASSO can handle data with multicollinear ity, be applied to various types of data (e.g., 229 \ncontinuous, categorical, mixed type), and is adaptable to sparse data (i.e., multiple predictors have 230 \nzero or near-zero coefficients ) [34,35]. The LASSO equation is as follows:  231 \n𝐿𝐿𝐴𝑆𝑆𝑂(𝛽)= ∑ 𝑙𝑜𝑔(𝑃 𝛽(𝑌𝑖|𝑋𝑖))𝑃\n𝑖=1− 𝜆 ∑ |𝛽𝑖|𝑃\n𝑖=1 (3) \n 232 \nwhere LLASSO(β) is the loss function, ∑ 𝑙𝑜𝑔(𝑃 𝛽(𝑌𝑖|𝑋𝑖))𝑃\n𝑖=1  is the summation of the MLE for all 233 \npredictor variables, λ ≥ 0 is the regularization hyperparameter that controls the degree of 234 \nshrinkage, and βi is a given regression coefficient. By applying the penalty ( 𝜆) to the absolute 235 \nvalue of the beta weights, LASSO allows the coefficient associated with a given variable to be 236 \nreduced (or “shr unk”) to  zero, eliminating those given variables from the model. For a more 237 \ndetailed discussion of LASSO, see Tibshirani’s paper [33]. 238 \n The following code utilizes the cv.glmnet () function from the glmnet package in R [36]. 239 \nMore information on the hyperparameters of the function can be found in Table 1. This function 240 \ndetermines the magnitude of lambda through a k-fold cross-validation approach.  241 \nlasso.model <- cv.glmnet( x = predTrain, y = outcomeTrain, type.measure = “class ”, alpha= 1, 242 \nfamily= “binomial ”, nfolds = 10) 243 \n 244 \nThrough this model, we can obtain the chosen  lambda value. To obtain a full list of all evaluated 245 \nlambda values, use lambda.model$lambda . One can also plot the k-fold cross-validation procedure 246 \nto obtain λ using plot(lasso.model)  (Figure 2). Although one may use the one standard error (1se) 247 \nrule to select lambda, selecting the model with prediction error one standard error above the 248 \nminimum cross-validated error, this rule performs poorly in regression [37], so we use the value 249"
    },
    {
      "section": "Page 12",
      "page_number": 12,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 11 \nwhich minimized cross validation error (lambda min). To obtain our lambda min value, specify 250 \nlasso.model$lambda. min. 251 \nUsing this specified lambda value produces the coefficients seen in Table 2. Out of the 252 \noriginal 106 predictor variables, only 16 were selected via LASSO, thus a sparse model has been 253 \nobtained. The coefficient estimates obtained through a LASSO approach are biased by the nature 254 \nof the algorithm [38], and thus research recommends recalculating them using a standard regression 255 \nbefore interpreting the coefficients of the model. A comparison of the biased coefficients obtained 256 \nfrom the LASSO model and the corrected coefficients obtained in the standard logistic model can 257 \nbe seen in Table 3. Obtaining the predicted classification prior to calculating accuracy is crucial. 258 \nAccuracy values (Formula 2) are then determined using the coefficient estimates from both 259 \nLASSO model and the logistic model. The following code can be used to obtain the accuracy 260 \nvalues from the logistic model.  261 \na.logistic <- mean(outcomeTest == pc.logistic) 262 \n 263 \nThe value obtained using the coefficient estimates from the LASSO model is an accuracy score of 264 \n.86. The value obtained using the coefficient estimates from the logistic model is .89.  265 \nDespite strong performance of LASSO on this data, LASSO does have limitations [39]. 266 \nFirst, it is unable to select more variables than there are observations. Second, LASSO will select 267 \na single variable in the presence of multicollinearity regardless of that variable’s predictive 268 \ncapacity. To combat these first two limitations, Zou and Hastie [40] proposed a new regularization 269 \ntechnique called Elastic Net. 270 \nElastic Net 271 \nElastic Net differs from LASSO through use of an additional penalty to the regression 272 \nequation. Elastic Net implements both the ℓ1 penalty, or the LASSO penalty, and the  ℓ2 penalty, 273"
    },
    {
      "section": "Page 13",
      "page_number": 13,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 12 \nor the ridge penalty, to the regression equation. With the inclusion of both penalties, the 274 \noptimization function for Elastic Net is as follows: 275 \n𝐿𝐸𝑙𝑎𝑠𝑡𝑖𝑐𝑁𝑒𝑡 (β)= ∑ 𝑙𝑜𝑔(𝑃 𝛽(𝑌𝑖|𝑋𝑖)𝑃\n𝑖=1− λ 1∑ 𝛽𝑖2𝑃\n𝑖=1− λ 2∑|𝛽𝑖|𝑃\n𝑖=1 (4) \n 276 \nwhere 𝐿𝐸𝑙𝑎𝑠𝑡𝑖𝑐𝑁𝑒𝑡 (𝛽) is the loss function, ∑ 𝑙𝑜𝑔(𝑃 𝛽(𝑌𝑖|𝑋𝑖)𝑃\n𝑖=1  is the summation of the MLE for all 277 \npredictor variables, λ1 ≥ 0 is the regularization hyperparameter that controls the degree of 278 \nshrinkage according to the ℓ1  penalty, λ1 ≥ 0 is the regularization hyperparameter that controls the 279 \ndegree of shrinkage according to the ℓ2 penalty, and βi is a given regression coefficient. 280 \nThrough the inclusion of the ℓ2 penalty, Elastic Net selects multiple variables that are highly 281 \ncorrelated while removing all irrelevant variables [39]. Thus, if you have a dataset with highly 282 \ncorrelated predictors (e.g., a set of dummy coded variables), Elastic Net would be more 283 \nappropriate as a variable selection technique than LASSO.  284 \nWe can obtain our lambda. min value using the cv.glmnet() equation as well and then use 285 \nthat value to fit our model using the following code.  286 \nen.model.min <- glmnet( x=predTrain y=outcomeTrain, \nalpha= 0.5, family= ”binomial ”, lambda = \nelasticNet $lamda.min ) \n 287 \nThe only difference between this code and the code for LASSO is that alpha = 0.5 rather 288 \nthan 1. Coefficient estimates from the Elastic Net model and unbiased coefficients from a standard 289 \nlogistic model can be seen in Table 4. An accuracy of 0.88 was obtained using the coefficient 290 \nestimates from the Elastic Net model while an accuracy of 0.80 was obtained using the coefficient 291 \nestimates from the logistic model.  292"
    },
    {
      "section": "Page 14",
      "page_number": 14,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 13 \nElastic Net also has some limitations. Namely, it may struggle with datasets containing 293 \nmany more variables than observations, it is sensitive to outliers, and, given that it is designed for 294 \nlinear relationships, it may not capture complex non-linear relationships between predictors and 295 \nthe response variable effectively [41]. 296 \nGenetic Algorithm (GA) 297 \nUnlike LASSO and Elastic Net, which utilize internal regression models, the genetic 298 \nalgorithm (GA) is different. Instead of relying on a predefined internal model, the GA operates as a 299 \nwrapper method. This means that the user must specify which model it should use (i.e., a user could 300 \nwrap the GA around a logistic regression model or something more complex like random forest or 301 \nSVM depending on the nature of their data).  302 \nA metaheuristic, such as the GA, operates at a higher level, employing strategies to 303 \nefficiently explore the solution space (i.e., all possible subsets of the variables) and locate optimal or 304 \nnear-optimal solutions to complex optimization problems like variable selection. Unlike specific 305 \nalgorithms tailored to certain types of problems, metaheuristics like the GA offer flexibility and 306 \nadaptability, making them suitable for a wide range of applications. 307 \nThe GA, inspired by the principles of natural selection and evolution, mimics the process of 308 \nbiological evolution to iteratively refine potential solutions. Through mechanisms such as crossover, 309 \nmutation, and selection, the GA explores and evolves a population of potential solutions over 310 \nsuccessive generations, gradually improving the overall quality of solutions. Figure 3 illustrates the 311 \ngeneral structure of the genetic algorithm, depicting its iterative process of generating, evaluating, 312 \nand evolving solutions. Each iteration refines the population, guiding the search towards promising 313 \nregions of the solution space. 314 \nFor a comprehensive understanding of the genetic algorithm and its application to variable 315 \nselection, interested readers are encouraged to refer to the work by Bain and colleagues [30]. Their 316"
    },
    {
      "section": "Page 15",
      "page_number": 15,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 14 \nresearch provides detailed insights into the underlying principles, implementation strategies, and 317 \npractical considerations associated with the GA’s use in solving two-group classification 318 \noptimization problems.  319 \nFor this paper, logistic regression is chosen as the model around which the GA will wrap. 320 \nThe optimization function used in this paper is the Hubert and Arabie [42] Adjusted Rand Index 321 \n(ARI). ARI is a measure of agreeability between predicted classifications and true (or known) 322 \nclassifications and can be calculated in the following way: 323 \n𝐴𝑅𝐼 =  𝑅𝐼− 𝑅𝐼 𝐸𝑥𝑝𝑒𝑐𝑡𝑒𝑑\n𝑚𝑎𝑥 (𝑅𝐼)− 𝑅𝐼 𝐸𝑥𝑝𝑒𝑐𝑡𝑒𝑑 \n \n𝑅𝐼 =  # 𝑝𝑎𝑖𝑟𝑠 𝑎𝑔𝑟𝑒𝑒𝑖𝑛𝑔\n# 𝑝𝑎𝑖𝑟𝑠 𝑑𝑖𝑠𝑎𝑔𝑟𝑒𝑒𝑖𝑛𝑔 (5) \n 324 \nThe R code needed to implement both logistic regression and ARI can be found on lines 248 325 \nthrough 260 in the companion code file on OSF. For a guide of hyperparameters and their default 326 \nvalues (if a default is selected), see Table 5. To implement the GA, the following code can be run: 327 \n ga.solution <- ga(fitness = function(vars)  \n    gaOpt( vars= vars, IV.train= data.frame(predTrain),  \n        DV.train= outcomeTrain), \n        type = \"binary\" , nBits = ncol(predTrain),  \n        names = colnames(predTrain), seed = 123456 , \n        run= 5 \n) \n 328 \nTo view the selected subset of variables from the ga() function, one calls, 329 \nga.solution@solution[1,] . Note, the returned solution (given by ga.solution@solution ) contains 330 \nmany potential subsets of variables, but by referencing only the first row (using the indexing [1,]), 331 \nthe optimal subset of variables as determined by the GA can be accessed. Since the ga() function 332 \ndoes not have a specified method for model building, but rather simply returns a list of variable 333"
    },
    {
      "section": "Page 16",
      "page_number": 16,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 15 \nselections, one must first build a model to obtain an accuracy value for the selected variables. 334 \nGiven that the internal model we specified was a logistic regression model, it makes sense to use a 335 \nsimple logistic model which can be built using the following code. The coefficients from this 336 \nmodel can be seen in Table 6.  337 \nallVarNames <- colnames(predTrain)  338 \nselectedVarNames <- allVarNames[ga.solution@solution[ 1,]==1]  339 \nselectedVars <- data.frame(predTest[,selectedVarNames],  340 \n    outcomeTest)  341 \nga.model <- glm(outcomeTest~., family= ’binomial ’,  342 \n    data= selectedVars) 343 \n 344 \nAfter building the model, an accuracy can be obtained using the following code: 345 \np <- predict(ga.model, newx = predTest) 346 \nc <- ifelse(p >= .8, 1,0) 347 \naccuracy <- mean(c == outcomeTest)  348 \n 349 \nAn accuracy value of 1 is obtained, indicating overfitting, as with the past models built in this 350 \ntutorial. Current literature indicates that the GA is prone to overfitting [43–45], suggesting the model 351 \nwould not fit quite as well if a new sample was collected, despite the accuracy of the model fit for 352 \nthe test sample used in this tutorial.  353 \nSupport Vector Machines 354 \nSupport Vector Machines (SVM) are a class of supervised learning models widely 355 \nemployed in classification and regression tasks [46,47]. SVMs operate by finding the optimal 356 \nhyperplane that maximizes the margin between different classes of data points. By maximizing 357 \nthe margin between classes, SVM achieves good generalizability and is robust to outliers [48,49]. 358 \nSVM can handle both linearly separable and non-linearly separable data by use of a kernel 359 \nfunction which artificially projects the original data into a higher-dimensional space [47]. 360"
    },
    {
      "section": "Page 17",
      "page_number": 17,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 16 \nElastic SCAD SVM 361 \nSVM by itself is a classification algorithm. However, researchers have created 362 \nimplementations of SVM that simultaneously perform classification and variable selection [50–52]. 363 \nThis tutorial uses an approach like LASSO and Elastic Net in that it selects variables via the 364 \naddition of a penalty [50]. The penalty utilized in this tutorial is the Elastic SCAD penalty which 365 \nwhen included in an SVM, reads: 366 \n𝑆𝑉𝑀𝐸𝑙𝑎𝑠𝑡𝑖𝑐𝑆𝐶𝐴𝐷 =𝑚𝑖𝑛𝑏,𝑤(𝑠𝑖𝑔𝑛 (𝒘𝑇𝒙 + 𝒃 )+ ∑ 𝑃 𝑆𝐶𝐴𝐷𝑝\n𝑗=1𝜆1(𝑊 𝑗) + 𝜆 2‖𝑤‖22) (6) \n 367 \nwhere λ1,λ2 ≥ 0 control the degree of shrinkage applied by the SCAD ( 𝑃𝑆𝐶𝐴𝐷 λ1(𝑊 𝑗)) and Elastic 368 \nNet ( λ2||𝑤||22) penalties, respectively. For more information on the SCAD penalty, see work by 369 \nBecker and colleagues [53]. The initial part of the equation ( 𝑠𝑖𝑔𝑛 (𝒘𝑇𝒙 + 𝒃 )) is the base equation 370 \nfor an SVM where 𝒘 is the weight vector, 𝒙 is the input feature vector, 𝒃 is the bias term vector, 371 \n𝑠𝑖𝑔𝑛 (.) is the sign function, which returns +1 if the argument is positive, -1 if negative, and 0 if 372 \nzero. All hyperparameters are set to default values in this tutorial. In addition, data needs to be 373 \nrestructured for this function. For a clearer understanding of the additional hyperparameters in the 374 \nsvmfs()  function, see Table 7. The svmfs()  function can be applied in the following manner. 375 \nBounds <- t(data.frame( log2lambda1= c(-10, 10), log2lambda2= c(- 376 \n10,10)))  377 \ncolnames(bounds) <-c(“lower ”, “upper ”)  378 \nsvm.model <- svmfs( x=predTrain, y = svmTrainOutcome, fs.method =  379 \n“scad+L2 ”, bounds= bounds, grid.search = “interval ”,  inner.val.method = 380 \n“cv”, show = “none ”, parms.coding = “none ”, 381 \nseed= 123456 ) 382 \n 383 \nThe output of the model created using the svmfs()  function has its own nomenclature that 384 \nrequires explanation. First, rather than referring to the coefficients as coefficients, the model uses 385"
    },
    {
      "section": "Page 18",
      "page_number": 18,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 17 \nthe w parameter (coming from the term beta weight). The b parameter illustrates the intercept of 386 \nthe SVM hyperplane and can be thought of like the b0 of a regression model. The xind parameter 387 \ntells the user the index (or column location) of the variables selected in the dataset. The full 388 \noutput can be seen in Table 8. To examine the accuracy of this model, the same predict function 389 \ncan be used as was implemented previously, but the outputted predictions will require some 390 \nrestructuring, as they come in the form of a factor with underlying numeric values 1 and 2 and 391 \nthey need to have numeric values of 0 and 1. The Elastic SCAD SVM model obtained an 392 \naccuracy of 0.83. The code required to calculate that accuracy value is below. 393 \nesvm.predictions <- predict(svm.model, newdata = svmTestPreds) esvm.predictions.formatted <- 394 \nas.numeric(esvm.predictions$pred.class)- 1 esvm.accuracy <- mean(esvm.predictions.formatted 395 \n== outcomeTest) 396 \n 397 \nLimitations of SVM include the researcher ’s selection of the kernel function, computation 398 \ntime, and dimension constraints. By default, the svmfs()  function utilizes a linear kernel function. 399 \nSince the kernel is chosen a priori by the researcher, for optimal results, an optimal function must 400 \nbe used. SVM models are computationally more expensive than a simpler classification technique 401 \n(e.g., logistic regression) and will take longer to compute. SVM models face the same degrees of 402 \nfreedom problem as LASSO and Elastic Net, where it is limited by the number of observations. 403 \nAs such, an ideal dataset for SVM would contain more observations than variables. 404 \nTree Based Models 405 \nRandom Forest 406 \nAnother powerful classifier is a decision (or classification) tree. An example can be seen in 407 \nFigure 4. From this decision tree, it can be concluded that anyone whose score on variable S5_57  408 \nis less than 3 and score on variable S5_60  is less than 3 does not qualify for a misophonia 409 \ndiagnosis. Decision trees are not only powerful classifiers, but they also produce an output that is 410 \neasy to interpret. However, decision trees are prone to overfitting – so much so that overfitting is 411"
    },
    {
      "section": "Page 19",
      "page_number": 19,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 18 \nalmost guaranteed [54]. One of the most efficient ways to avoid overfitting is by using multiple 412 \ntrees (i.e., creating a random forest). Random forest creates many decision trees using a randomly 413 \nselected subset of the data to create each individual tree. The results of all trees are then 414 \naggregated to predict the desired outcome. Some major benefits of a random forest classifier are 415 \nthat it can be used with an outcome variable that has any number of levels [55], meaning that unlike 416 \nlogistic regression which only works with binary variables, random forest could handle a variable 417 \nwith 3, 4, or even 10 different levels. However, these trees are only used for classification, 418 \nmeaning that they do not perform variable selection. Thus, researchers have had to adapt the 419 \nclassifier to perform variable selection. The utilization of random forest in the Boruta  package 420 \nperforms well in many different conditions [56], and therefore, is the implementation demonstrated 421 \nin this tutorial. 422 \nThe Boruta package contains a series of functions pertaining to variable selection 423 \ntechniques using different measures of importance to select the variables. A measure of 424 \nimportance simply indicates the value of a given variable to the overall strength of the model. The 425 \nvariables that are more useful, meaning that they are stronger predictors of the outcome variable, 426 \nare deemed more important, and thus are more likely to be selected than those of a lesser 427 \nimportance (i.e., less predictive power). To run the model, a simple regression formula statement 428 \nis used:  outcome ~ predictors.  Because all predictors will be used, a shortcut can be implemented 429 \nthrough use of a period ( .) in place of predictors as is seen in the code below. If not, all variables 430 \nwere to be included in the model, the user would need to type all the relevant predictors names in 431 \nthe formula statement concatenated with addition symbols ( +). Knowing this, the model can then 432 \nbe built using the following code: 433 \nset.seed( 123456 ) 434 \nboruta.model <- Boruta(as.factor(MQDX) ~. , data= trainDat)  435"
    },
    {
      "section": "Page 20",
      "page_number": 20,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 19 \nThe Boruta()  function classifies variables as either important, unimportant, or of tentative 436 \nimportance. Regarding the misophonia dataset, 15 variables were deemed important, 74 variables 437 \nwere deemed unimportant, and the remaining 17 variables were placed in the tentative category. 438 \nFor a list of all variables that were classified in each category and a visualization of the 439 \nboruta.model  output, see Table 9. Figure 5 illustrates variability of the importance score 440 \ncalculated for each variable during the Boruta process, as well as their ultimate classification. A 441 \nmodel can be built using either a) all variables that were not deemed unimportant (non-rejected 442 \nvariables) or b) only the confirmed important variables. For the purpose of this tutorial, only 443 \nvariables which have been confirmed important are included in the model. This model is then 444 \nbuilt using the randomForest()  function since Boruta()  internally implements a random forest 445 \nmodel. The model is built in the following way. 446 \nset.seed( 123456 ) 447 \nselectedModel <- randomForest(getConfirmedFormula(boruta.model),    448 \n    data= trainDat) 449 \n 450 \nThe predictive accuracy of the random forest model can be calculated using the predict()  451 \nfunction, just as it has been for other models. An accuracy of .88 was obtained for this model. The 452 \nalgorithm may not perform well with highly unbalanced classifications or in situations where a 453 \ngiven level contains a very small number of classifications.  454 \nComparing All Models 455 \nFor a comparison of the accuracy values obtained by all techniques implemented in this 456 \ntutorial, see Table 10. From this, we can state that the GA produced the most accurate model. 457 \nHowever, there was no difference in the LASSO non-biased, Boruta, and Elastic Net models in 458 \nterms of accuracy. Depending on the purpose of your model, you may want to use a performance 459 \nmetric other than accuracy. Within the context of our motivating example, it may be worth 460 \nexamining the following:  461"
    },
    {
      "section": "Page 21",
      "page_number": 21,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 20 \n• Sensitivity: given the individual truly has misophonia, how likely is the classifier to realize 462 \nthat? 463 \n• Specificity: given the individual truly does not have misophonia, how likely is the 464 \nclassifier to realize that? 465 \n• Positive predictive value: given the classifier claims the individual to have misophonia, 466 \nhow likely is it that the individual really has misophonia? 467 \n• Negative predictive value: given the classifier claims the individual does not have 468 \nmisophonia, how likely is it that the individual really does not have the disease? 469 \nGiven that our example pertains to diagnosis, it is possible that one may favor sensitivity over 470 \nspecificity in that we want to minimize the number of missed cases. However, it is also possible 471 \nthat we would want to minimize the number of false diagnoses to save individuals the cost of 472 \nunnecessary intervention. A confusion matrix (discussed briefly in Appendix B) might be useful. 473 \nAlternatively, one could use the Area Under the Curve (AUC) of the Receiver operating 474 \ncharacteristic (ROC) curve. One should carefully consider these factors when deciding on the 475 \nperformance metric by which to evaluate a model.  476 \n We can also examine the different variables each method selected (Tables 2 – 4, 6, and 8 – 477 \n9). The GA selected the most variables which likely accounts for its very strong accuracy. 478 \nHowever, Elastic SCAD SVM selected many more variables than LASSO, but had a worse 479 \naccuracy. Given this outcome, it may not be ideal to use all variables selected by Elastic SCAD 480 \nSVM in this dataset. There was only one variable (S5_57) that was selected by all five methods. 481 \nSo, there is a clear method effect on the variables that are deemed to be important. Within the 482 \ncontext of our example, we could interpret this to mean that the question, “ Sometimes in response 483"
    },
    {
      "section": "Page 22",
      "page_number": 22,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 21 \nto sounds I feel rage that is difficult to control ” is an incredibly important predictor for 484 \nmisophonia and may capture a defining characteristic of the disorder. 485 \nDiscussion 486 \nThis tutorial provided an overview and a practical guide for implementation of LASSO 487 \n[36], Elastic Net [36], a genetic algorithm [57,58], Elastic SCAD SVM [50], and random forest via 488 \nBoruta [56] in R v. 4.2.1. Proper analysis of the output as well as comparisons on the predictive 489 \naccuracy of each method are also discussed. More information on R, other useful machine 490 \nlearning software, and some of these functions were provided in the Appendices. Lastly, an OSF 491 \nproject  containing all code implemented in this tutorial, as well as the data used, is available. 492 \nVariable selection allows researchers to find parsimonious models that are also good 493 \npredictive or classifying models. Given R’s increasing popularity among researchers due to the 494 \nfree and open access nature of the software, it is of value to the field to provide more guidance on 495 \nthe variable selection methods available in R. In addition, the extent to which some of these 496 \nmethods overfit data should not be ignored when implementing them on real-world data. If a 497 \nresearcher is concerned with creating a generalizable model, it is recommended to validate the 498 \nresults not only through some form of cross-validation, but also through collection of a new 499 \nsample. Through this tutorial, we aim to push the field towards more transparent guidelines and 500 \nstandardization for the use of variable selection techniques and machine learning in psychological 501 \nresearch. 502 \nThere are many additional R packages that will perform variable selection using random 503 \nforest as well as SVMs, but only one of each was demonstrated in this tutorial. The demonstrated 504 \nmethods in the current tutorial were selected because they are commonly used in the 505 \npsychological sciences, are powerful techniques for both classification (e.g., diagnosing 506 \nindividuals with misophonia) and variable selection, and because they are all freely available to 507"
    },
    {
      "section": "Page 23",
      "page_number": 23,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 22 \nresearchers in R. In a similar vein, we have included only five machine learning methods here but 508 \nmany more exist, and additional tutorials should be provided to applied researchers about how 509 \nbest to implement them following research demonstrating each algorithm ’s performance to 510 \nindicate which algorithm is best for addressing certain research questions. For the interested 511 \nreader, a comparison of the performance of each method demonstrated in this tutorial can be 512 \nfound in Bain and colleagues [30].  513 \nThis tutorial presented instructions for using five variable selection techniques in R which 514 \nserves as a useful starting point for understanding how to perform variable selection. We 515 \nencourage the user to refer to each method’s full documentation for additional examples and 516 \ndetail. We hope that this tutorial makes these methods more easily accessible to the everyday 517 \npsychological researcher, opens doors to applications of variable selection in new areas, and leads 518 \nto a decreased presence of less powerful methods (e.g., stepwise selection) in the literature.  519 \n 520 \n 521 \n 522 \nDeclarations 523 \nEthics approval and consent to participate 524 \nAll study procedures were approved by the University of Oklahoma, Norman, Institutional 525 \nReview Board (IRB; #12754). All participants electronically acknowledged their informed 526 \nconsent to participate prior to completing the survey. All responses remained anonymous and no 527 \npersonal identifiable information was collected from participants. 528 \nConsent for publication 529 \nNot applicable. 530 \nAvailability of data and materials 531"
    },
    {
      "section": "Page 24",
      "page_number": 24,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 23 \nThe accompanying code and data utilized in this tutorial can be found here: 532 \nhttps://osf.io/pr6j8/?view_only=de4405aae6fd4bc7bffb85f0e872216 . Additional supplementary 533 \ninformation such as a glossary of key terms, R package recommendations, etc. are also available 534 \nthrough OSF.  535 \nCompeting interests 536 \nNo authors have any competing interests to report at this time.  537 \nFunding 538 \nNo funding was obtained for this project. 539 \nAuthors’ contributions  540 \nCMB conducted all analyses and drafted the manuscript; regarding the data utilized here, JEN 541 \naided in the original study design, led all data collection and data preprocessing while LEE 542 \nconceived of the study design of the project and supervised all aspects of funding, participant 543 \nrecruitment, and data collection; JEL supervised data analysis and manuscript preparation. DS 544 \nsupervised manuscript preparation and provided guidance about organization and focus of the 545 \npaper. All authors contributed significantly to manuscript preparation.  546 \nAcknowledgements 547 \nNot applicable. 548 \nAuthors’ information  549 \nCatherine M. Bain. https://orcid.org/0000-0002-2767-6882   550 \nDingjing Shi. https://orcid.org/0000-0002-5652-3818 551 \nLauren E. Ethridge. https://orcid.org/0000-0003-0601-6911   552 \nJordan E. Norris. https://orcid.org/0000-0002-4438-3416   553 \nJordan E. Loeffelman. https://orcid.org/0000-0002-0269-7708  554"
    },
    {
      "section": "Page 25",
      "page_number": 25,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 24 \n 555 \n 556 \n 557 \n 558 \n 559 \n 560 \n 561 \n 562 \n 563 \n 564 \n 565 \n 566 \n 567 \n 568 \n 569 \n 570 \n 571 \n 572 \n 573 \n 574 \n575"
    },
    {
      "section": "Page 26",
      "page_number": 26,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 25 \nReferences 576 \n 577 \n1. Lenters, V., Vermeulen, R., & Portengen, L. (2018). Performance of variable selection methods 578 \nfor assessing the health effects of correlated exposures in case –control studies. Occupational 579 \nand Environmental Medicine , 75(7), 522 –529. https://doi.org/10.1136/oemed-2016-104231 580 \n2. Amene, E., Hanson, L. A., Zahn, E. A., Wild, S. R., & Döpfer, D. (2016). Variable selection and 581 \nregression analysis for the prediction of mortality rates associated with foodborne diseases. 582 \nEpidemiology and Infection , 144(9), 1959 –1973. 583 \nhttps://doi.org/10.1017/S0950268815003234 584 \n3. Bourdès, V., Bonnevay, S., Lisboa, P., Defrance, R., Pérol, D., Chabaud, S., Bachelot, T., Gargi, 585 \nT., & Négrier, S. (2010). Comparison of Artificial Neural Network with Logistic Regression 586 \nas Classification Models for Variable Selection for Prediction of Breast Cancer Patient 587 \nOutcomes. Advances in Artificial Neural Systems , 2010 , 1–11. 588 \nhttps://doi.org/10.1155/2010/309841 589 \n4. Wettstein, A., Jenni, G., Schneider, I., Kühne, F., grosse Holtforth, M., & La Marca, R. (2023). 590 \nPredictors of Psychological Strain and Allostatic Load in Teachers: Examining the Long- 591 \nTerm Effects of Biopsychosocial Risk and Protective Factors Using a LASSO Regression 592 \nApproach. International Journal of Environmental Research and Public Health , 20(10), 593 \nArticle 10. https://doi.org/10.3390/ijerph20105760 594 \n5. Luo, J., Ren, S., Li, Y., & Liu, T. (2021). The Effect of College Students’ Adaptability on 595 \nNomophobia: Based on Lasso Regression. Frontiers in Psychiatry , 12. 596 \nhttps://www.frontiersin.org/articles/10.3389/fpsyt.2021.641417 597 \n6. Derksen, S., & Keselman, H. J. (1992). Backward, forward and stepwise automated subset 598 \nselection algorithms: Frequency of obtaining authentic and noise variables. British Journal of 599"
    },
    {
      "section": "Page 27",
      "page_number": 27,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 26 \nMathematical and Statistical Psychology , 45(2), 265 –282. https://doi.org/10.1111/J.2044- 600 \n8317.1992.TB00992.X 601 \n7. Kok, B. C., Choi, J. S., Oh, H., & Choi, J. Y. (2021). Sparse Extended Redundancy Analysis: 602 \nVariable Selection via the Exclusive LASSO. Multivariate Behavioral Research , 56(3), 426 – 603 \n446. https://doi.org/10.1080/00273171.2019.1694477 604 \n8. Whittingham, M. J., Stephens, P. A., Bradbury, R. B., & Freckleton, R. P. (2006). Why do we still 605 \nuse stepwise modelling in ecology and behaviour? Journal of Animal Ecology , 75(5), 1182 – 606 \n1189. https://doi.org/10.1111/j.1365-2656.2006.01141.x 607 \n9. Wiegand, R. E. (2010). Performance of using multiple stepwise algorithms for variable selection. 608 \nStatistics in Medicine , 29(15), 1647 –1659. https://doi.org/10.1002/sim.3943 609 \n10. Thompson, B. (1995). Stepwise Regression and Stepwise Discriminant Analysis Need Not Apply 610 \nhere: A Guidelines Editorial. Educational and Psychological Measurement , 55(4), 525 –534. 611 \nhttps://doi.org/10.1177/0013164495055004001 612 \n11. Smith, G. (2018). Step away from stepwise. Journal of Big Data , 5(1), 32. 613 \nhttps://doi.org/10.1186/s40537-018-0143-6 614 \n12. Serang, S., Jacobucci, R., Brimhall, K. C., & Grimm, K. J. (2017). Exploratory Mediation 615 \nAnalysis via Regularization. Structural Equation Modeling : A Multidisciplinary Journal , 616 \n24(5), 733 –744. https://doi.org/10.1080/10705511.2017.1311775 617 \n13. Shi, D., Shi, D., & Fairchild, A. J. (2023). Variable Selection for Mediators under a Bayesian 618 \nMediation Model. Structural Equation Modeling: A Multidisciplinary Journal , 0(0), 1 –14. 619 \nhttps://doi.org/10.1080/10705511.2022.2164285 620 \n14. Gunn, H. J., Hayati Rezvan, P., Fernández, M. I., & Comulada, W. S. (2023). How to apply 621 \nvariable selection machine learning algorithms with multiply imputed data: A missing 622 \ndiscussion. Psychological Methods , 28(2), 452 –471. https://doi.org/10.1037/met0000478 623"
    },
    {
      "section": "Page 28",
      "page_number": 28,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 27 \n15. Norris, J. E., Kimball, S. H., Nemri, D. C., & Ethridge, L. E. (2022). Toward a Multidimensional 624 \nUnderstanding of Misophonia Using Cluster-Based Phenotyping. Frontiers in Neuroscience , 625 \n16. https://doi.org/10.3389/fnins.2022.832516 626 \n16. Guyon, I., & Elisseeff, A. (2003). An Introduction to Variable and Feature Selection. Journal of 627 \nMachine Learning Research , 3, 1157 –1182. 628 \n17. Chowdhury, M. Z. I., & Turin, T. C. (2020). Variable selection strategies and its importance in 629 \nclinical prediction modelling. Family Medicine and Community Health , 8(1), e000262. 630 \nhttps://doi.org/10.1136/fmch-2019-000262 631 \n18. Barceló, P., Monet, M., Pérez, J., & Subercaseaux, B. (2020). Model interpretability through the 632 \nlens of computational complexity. Proceedings of the 34th International Conference on 633 \nNeural Information Processing Systems , 15487 –15498. 634 \n19. Jia, W., Sun, M., Lian, J., & Hou, S. (2022). Feature dimensionality reduction: A review. 635 \nComplex & Intelligent Systems , 8(3), 2663 –2693. https://doi.org/10.1007/s40747-021-00637- 636 \nx 637 \n20. van Vuuren, C. L., van Mens, K., de Beurs, D., Lokkerbol, J., van der Wal, M. F., Cuijpers, P., & 638 \nChinapaw, M. J. M. (2021). Comparing machine learning to a rule-based approach for 639 \npredicting suicidal behavior among adolescents: Results from a longitudinal population-based 640 \nsurvey. Journal of Affective Disorders , 295, 1415 –1420. 641 \nhttps://doi.org/10.1016/j.jad.2021.09.018 642 \n21. Pratik, S., Nayak, D., Prasath, R. R., & Swarnkar, T. (2022). Prediction of Smoking Addiction 643 \nAmong Youths Using Elastic Net and KNN: A Machine Learning Approach  (pp. 199 –209). 644 \nhttps://doi.org/10.1007/978-3-031-21517-9_20 645 \n22. Marafino, B. J., John Boscardin, W., & Adams Dudley, R. (2015). Efficient and sparse feature 646 \nselection for biomedical text classification via the elastic net: Application to ICU risk 647"
    },
    {
      "section": "Page 29",
      "page_number": 29,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 28 \nstratification from nursing notes. Journal of Biomedical Informatics , 54, 114 –120. 648 \nhttps://doi.org/10.1016/j.jbi.2015.02.003 649 \n23. Kerkhoff, D., & Nussbeck, F. W. (2019). The Influence of Sample Size on Parameter Estimates 650 \nin Three-Level Random-Effects Models. Frontiers in Psychology , 10. 651 \nhttps://doi.org/10.3389/fpsyg.2019.01067 652 \n24. Arjomandi-Nezhad, A., Guo, Y., Pal, B. C., & Varagnolo, D. (2023). A Model Predictive 653 \nApproach for Enhancing Transient Stability of Grid-Forming Converters  654 \n(arXiv:2308.01020). arXiv. http://arxiv.org/abs/2308.01020 655 \n25. Fox, E. W., Hill, R. A., Leibowitz, S. G., Olsen, A. R., Thornbrugh, D. J., & Weber, M. H. 656 \n(2017). Assessing the accuracy and stability of variable selection methods for random forest 657 \nmodeling in ecology. Environmental Monitoring and Assessment , 189(7), 316. 658 \nhttps://doi.org/10.1007/s10661-017-6025-0 659 \n26. Cateni, S., Colla, V., & Vannucci, M. (2010). Variable Selection through Genetic Algorithms for 660 \nClassification Purposes. Artificial Intelligence and Applications . Artificial Intelligence and 661 \nApplications, Innsbruck, Austria. https://doi.org/10.2316/P.2010.674-080 662 \n27. Heinze, G., Wallisch, C., & Dunkler, D. (2018). Variable selection – A review and 663 \nrecommendations for the practicing statistician. Biometrical Journal. Biometrische 664 \nZeitschrift , 60(3), 431 –449. https://doi.org/10.1002/bimj.201700067 665 \n28. Chu, M., Fang, Z., Mao, L., Ma, H., Lee, C.-Y., & Chiang, Y.-C. (2024). Creating A child- 666 \nfriendly social environment for fewer conduct problems and more prosocial behaviors among 667 \nchildren: A LASSO regression approach. Acta Psychologica , 244, 104200. 668 \nhttps://doi.org/10.1016/j.actpsy.2024.104200 669"
    },
    {
      "section": "Page 30",
      "page_number": 30,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 29 \n29. Liu, X., Cao, P., Gonçalves, A. R., Zhao, D., & Banerjee, A. (2018). Modeling Alzheimer’s 670 \nDisease Progression with Fused Laplacian Sparse Group Lasso. ACM Transactions on 671 \nKnowledge Discovery from Data , 12(6), 65:1-65:35. https://doi.org/10.1145/3230668 672 \n30. Bain, C., Shi, D., Boness, C. L., & Loeffelman, J. (2023). A Simulation Study Comparing the Use 673 \nof Supervised Machine Learning Variable Selection Methods in the Psychological Sciences . 674 \nPsyArXiv. https://doi.org/10.31234/osf.io/y53t6 675 \n31. Wu, M. S., Lewin, A. B., Murphy, T. K., & Storch, E. A. (2014). Misophonia: Incidence, 676 \nPhenomenology, and Clinical Correlates in an Undergraduate Student Sample: Misophonia. 677 \nJournal of Clinical Psychology , 70(10), 994 –1007. https://doi.org/10.1002/jclp.22098 678 \n32. Helwig, N. E. (2017). Adding bias to reduce variance in psychological results: A tutorial on 679 \npenalized regression. The Quantitative Methods for Psychology , 13(1), 1 –19. 680 \nhttps://doi.org/10.20982/tqmp.13.1.p001 681 \n33. Tibshirani, R. (1996). Bias, variance and prediction error for classification rules . University of 682 \nToronto. 683 \n34. Foucart, S., Tadmor, E., & Zhong, M. (2023). On the Sparsity of LASSO Minimizers in Sparse 684 \nData Recovery. Constructive Approximation , 57(2), 901 –919. 685 \nhttps://doi.org/10.1007/s00365-022-09594-1 686 \n35. Mendez-Civieta, A., Aguilera-Morillo, M. C., & Lillo, R. E. (2021). Adaptive sparse group 687 \nLASSO in quantile regression. Advances in Data Analysis and Classification , 15(3), 547 – 688 \n573. https://doi.org/10.1007/s11634-020-00413-8 689 \n36. Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear 690 \nModels via Coordinate Descent. Journal of Statistical Software , 33(1). 691 \nhttps://doi.org/10.18637/jss.v033.i01 692"
    },
    {
      "section": "Page 31",
      "page_number": 31,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 30 \n37. Chen, Y., & Yang, Y. (2021). The One Standard Error Rule for Model Selection: Does It Work? 693 \nStats , 4(4), 868 –892. https://doi.org/10.3390/stats4040051 694 \n38. Yarkoni, T., & Westfall, J. (2017). Choosing Prediction Over Explanation in Psychology: 695 \nLessons From Machine Learning. Perspectives on Psychological Science , 12(6), 1100 –1122. 696 \nhttps://doi.org/10.1177/1745691617693393 697 \n39. Algamal, Z. Y., & Lee, M. H. (2015). Applying penalized binary logistic regression with 698 \ncorrelation based elastic net for variables selection. Journal of Modern Applied Statistical 699 \nMethods , 14(1), 168 –179. https://doi.org/10.22237/jmasm/1430453640 700 \n40. Zou, H., & Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal 701 \nof the Royal Statistical Society. , 67(2), 301 –320. 702 \n41. Wang, L., Cheng, H., Liu, Z., & Zhu, C. (2014). A robust elastic net approach for feature 703 \nlearning. Journal of Visual Communication and Image Representation , 25(2), 313 –321. 704 \nhttps://doi.org/10.1016/j.jvcir.2013.11.002 705 \n42. Hubert, L., & Arabie, P. (1985). Comparing partitions. Journal of Classification , 2(1), 193 –218. 706 \nhttps://doi.org/10.1007/BF01908075 707 \n43. Cunningham, P., & Loughrey, J. (2005). Overfitting in Wrapper-Based Feature Subset Selection: 708 \nThe Harder You Try the Worse it Gets. Research and Development in Intelligent Systems 709 \nXXI, 33–43. https://doi.org/10.1007/1-84628-102-4_3 710 \n44. Fröhlich, H., Chapelle, O., & Schölkopf, B. (2003). Feature Selection for Support Vector 711 \nMachines by Means of Genetic Algorithms . https://doi.org/10.1109/TAI.2003.1250182 712 \n45. Leardi, R. (2000). Application of genetic algorithm-PLS for feature selection in spectral data 713 \nsets. Journal of Chemometrics , 14, 643 –655. 714 \n46. Fernandez, M., Caballero, J., Fernandez, L., & Sarai, A. (2011). Genetic algorithm optimization 715 \nin drug design QSAR: Bayesian-regularized genetic neural networks (BRGNN) and genetic 716"
    },
    {
      "section": "Page 32",
      "page_number": 32,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 31 \nalgorithm-optimized support vectors machines (GA-SVM). Molecular Diversity , 15(1), 269 – 717 \n289. https://doi.org/10.1007/s11030-010-9234-9 718 \n47. Karatzoglou, A., Meyer, D., & Hornik, K. (2006). Support Vector Machines in R. Journal of 719 \nStatistical Software , 15(9). https://doi.org/10.18637/jss.v015.i09 720 \n48. Singla, M., & Shukla, K. K. (2020). Robust statistics-based support vector machine and its 721 \nvariants: A survey. Neural Computing and Applications , 32(15), 11173 –11194. 722 \nhttps://doi.org/10.1007/s00521-019-04627-6 723 \n49. Xu, H., Caramanis, C., & Mannor, S. (2009). Robustness and Regularization of Support Vector 724 \nMachines. Journal of Machine Learning Research 1 , 10, 1485 –1510. 725 \n50. Becker, N., Werft, W., & Benner, A. (2018). penalizedSVM: Feature Selection SVM using 726 \nPenalty Functions  [Computer software]. https://CRAN.R- 727 \nproject.org/package=penalizedSVM 728 \n51. Bierman, S., & Steel, S. (2009). Variable selection for support vector machines. Communications 729 \nin Statistics: Simulation and Computation , 38(8), 1640 –1658. 730 \nhttps://doi.org/10.1080/03610910903072391 731 \n52. Tharwat, A., & Hassanien, A. E. (2019). Quantum-Behaved Particle Swarm Optimization for 732 \nParameter Optimization of Support Vector Machine. Journal of Classification , 36, 576 –598. 733 \nhttps://doi.org/10.1007/s00357-018-9299-1 734 \n53. Becker, N., Toedt, G., Lichter, P., & Benner, A. (2011). Elastic SCAD as a novel penalization 735 \nmethod for SVM classification tasks in high-dimensional data. BMC Bioinformatics , 12. 736 \nhttps://doi.org/10.1186/1471-2105-12-138 737 \n54. Bengio, Y., Delalleau, O., & Simard, C. (2010). Decision trees do not generalize to new 738 \nvariations. Computational Intelligence , 26(4), 449 –467. https://doi.org/10.1111/j.1467- 739 \n8640.2010.00366.x 740"
    },
    {
      "section": "Page 33",
      "page_number": 33,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 32 \n55. Brieuc, M. S. O., Waters, C. D., Drinan, D. P., & Naish, K. A. (2018). A practical introduction to 741 \nRandom Forest for genetic association studies in ecology and evolution. Molecular Ecology 742 \nResources , 18(4), 755 –766. https://doi.org/10.1111/1755-0998.12773 743 \n56. Kursa, M. B., & Rudnicki, W. R. (2010). Feature Selection with the Boruta  Package. Journal of 744 \nStatistical Software , 36(11). https://doi.org/10.18637/jss.v036.i11 745 \n57. Scrucca, L. (2013). GA: A package for genetic algorithms in R. Journal of Statistical Software , 746 \n53(4), 1 –37. https://doi.org/10.18637/jss.v053.i04 747 \n58. Scrucca, L. (2017). On some extensions to GA package: Hybrid optimisation, parallelisation and 748 \nislands evolution. R Journal , 9(1), 187 –206. https://doi.org/10.32614/rj-2017-008 749 \n59. Ghojogh, B., & Crowley, M. (2019). The Theory Behind Overfitting, Cross Validation, 750 \nRegularization, Bagging, and Boosting: Tutorial. In arXiv . http://arxiv.org/abs/1905.12787 751 \n 752 \n  753 \n 754 \n 755 \n 756 \n 757 \n 758 \n 759 \n 760 \n 761 \n 762 \n 763 \n 764 \n 765 \n 766 \n 767 \n 768 \n 769 \n 770 \n 771 \n 772 \n 773 \n 774 \n 775 \n 776"
    },
    {
      "section": "Page 34",
      "page_number": 34,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 33 \nAppendix A  777 \nDemographic information on the original sample: 778 \nVariable  \nAge (Years) M = 18.96 SD = 1.7 \nGender  \nMale 104 (30.3%) \nFemale 239 (69.7%) \nEthnicity  \nWhite 263 (76.7%) \nBlack/African American 32 (9.3%) \nLatino/Hispanic 46 (13.4%) \nAsian/Asian American 28 (8.2%) \nAmerican Indian/Alaska Native 26 (7.6%) \nNative Hawaiian/Other Pacific Islander 2 (0.6%) \nOther 2 (0.6%) \nEducation  \nLess than high school 2 (0.6%) \nHigh school graduate 129 (37.6%) \nSome years of college/university (no degree) 194 (56.6%) \nVocational training 2 (0.6%) \nAssociates degree 8 (2.3%) \nBachelor’s degree  5 (1.5%) \nMaster’s degree  1 (0.3%) \n 779 \n 780 \n 781 \n782"
    },
    {
      "section": "Page 35",
      "page_number": 35,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 34 \nAppendix B 783 \nThe output of a randomForest()  model takes the following form: 784 \n## 785 \n## Call: 786 \n## randomForest(formula = getConfirmedFormula(boruta.model), data = trainDat) 787 \n## Type of random forest: classification 788 \n## Number of trees: 500 789 \n## No. of variables tried at each split: 3 ## 790 \n## OOB estimate of error rate: 0% 791 \n## Confusion matrix: 792 \n## 0 1 class.error 793 \n## 0 124 0 0 794 \n## 1 0 24 0 795 \n 796 \nThe random forest output contains different information than any other technique 797 \ndiscussed in this paper because it performs a type of cross-validation internally through looking at 798 \nsomething called Out of Bag error (OOB; sometimes referred to as the out-of-bag estimate). The 799 \nOOB is an approach to measuring the prediction error of a random forest model or of other 800 \ndecision tree models. OOB error is the mean prediction error of a given sample, using only the 801 \ntrees which did not have that sample in their bootstrapped sample. This sounds potentially 802 \nconfusing, but it simply means that the OOB error is the average prediction error of a given 803 \nsample of data when that sample of data is treated as a test sample rather than a train sample (i.e., 804 \na tree is evaluated on that data since it has yet to see it). OOB error is also used for other machine 805 \nlearning models implementing something called bootstrap aggregation (bagging). Bagging is the 806"
    },
    {
      "section": "Page 36",
      "page_number": 36,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 35 \nofficial term for only considering a random sample of the data when random forest creates each 807 \ntree. It is unique in that it is a random sample that allows for repetition, meaning that the records 808 \nfor a single participant could be represented more than once in the sample. For more on the theory 809 \nbehind bagging, see work by Ghojogh and Crowley [59]. 810 \nIn addition to the OOB error rate, the output provides a confusion matrix, something that is 811 \noften used to discuss the performance of a classification method. A confusion matrix follows the 812 \nform below: 813 \n True 0 True 1 \nPredicted 0 Correct Rejection Miss \nPredicted 1 False Alarm Hit \n 814 \nIt is ideal to have a high number of both hits and correct rejections and a low number of 815 \nboth false alarms and misses. It is possible that one may wish to allow for more false alarms so as 816 \nto decrease miss rates in some cases (e.g., a doctor would likely rather have a false positive 817 \nscreening for cancer than miss a cancer diagnosis). In other cases, one may want to minimize false 818 \nalarms (e.g., in the court system, it is ideal to minimize the number of innocent people who are 819 \nsent to jail). Thus, it is incredibly beneficial to understand each of these statistics when evaluating 820 \nthe performance of a classification model, as they both factor into calculating accuracy. 821 \nThe randomForest() output provides a classification error representing the proportion of a 822 \ngiven class which has been misclassified (e.g., a true 0 that was classified as 1 or the reverse). For 823 \nthe model demonstrated, there is no classification error for either class since perfect accuracy 824 \noccurred.  825 \n 826"
    },
    {
      "section": "Page 37",
      "page_number": 37,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 36 \n 827 \n 828 \n 829 \n 830 \n 831 \n 832 \n 833 \n 834 \n 835 \n 836 \n 837 \n 838 \n 839 \n 840 \n 841 \n 842 \n 843 \n 844 \n 845 \n 846 \n 847 \nFigures 848 \nTable 1 849 \nA table containing the hyperparameters of the cv.glmnet() function and their corresponding  850 \ndefinitions.  851"
    },
    {
      "section": "Page 38",
      "page_number": 38,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 37 \nParameter Description \nx A matrix of predictor (or input) variables \ny The vector containing the response (or outcome) variable \ntype.measure \nThe optimization measure to be used within the internal \ncross-validation procedure. By setting this to “class” \nmisclassification error is optimized. \nalpha \nThe Elastic Net mixing hyperparameter. Because the same \nfunction is used to implement ridge, LASSO, and Elastic \nNet, the value for alpha determines which regularization \ntechnique is run. Alpha is constrained between 0 and 1, with \na value of 0 implementing ridge regression, 1 implementing \nLASSO regression, and anything in between implementing \nan Elastic Net regression. \nfamily \nThe type of regression to be implemented. By setting this \nhyperparameter to “binomial ” an MLE regression is \nimplemented. \nnfolds The number of partitions implemented in the internal \nk-fold cross-validation. \n 852 \n  853"
    },
    {
      "section": "Page 39",
      "page_number": 39,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 38 \nTable 2 854 \nA table containing the variables selected by the LASSO model and their corresponding estimated 855 \ncoefficients.  856 \nVariable Coefficient \n(Intercept) -5.741 \nMQ4 0.154 \nMQ11 0.039 \nMQ12 0.137 \nMQ13 0.112 \nMQ17 0.045 \nS5_7 0.087 \nS5_24 -0.032 \nS5_25 0.318 \nS5_31 0.159 \nS5_32 0.024 \nS5_35 0.123 \nS5_36 0.089 \nS5_56 0.124 \nS5_57 0.419 \nS5_75 0.252 \nS5_78 -0.018 \n 857 \n 858 \n 859 \n 860 \n 861 \n 862 \n 863 \n 864 \n 865 \n 866 \n 867 \n 868 \n 869 \n 870 \n 871"
    },
    {
      "section": "Page 40",
      "page_number": 40,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 39 \nTable 3 872 \nA table containing the variables selected by the LASSO model and the coefficient estimates 873 \nobtained directly from the LASSO model as well as the re-estimated (non-biased) coefficients 874 \nobtained by creating a typical logistic model using the selected variables.  875 \nVariable LASSO Estimate Logistic Estimate \n(Intercept) -5.741 -8.802 \nMQ4 0.154 0.361 \nMQ11 0.039 0.480 \nMQ12 0.137 0.760 \nMQ13 0.112 -0.193 \nMQ17 0.045 0.143 \nS5_7 0.087 -0.057 \nS5_24 -0.032 -1.154 \nS5_25 0.318 1.051 \nS5_31 0.159 0.538 \nS5_32 0.024 0.245 \nS5_35 0.123 0.110 \nS5_36 0.089 0.285 \nS5_56 0.124 0.387 \nS5_57 0.419 0.867 \nS5_75 0.252 0.186 \nS5_78 -0.018 -0.600 \n 876 \n 877 \n 878 \n 879 \n 880 \n 881 \n 882 \n 883 \n 884 \n 885 \n 886 \n 887 \n 888 \n 889"
    },
    {
      "section": "Page 41",
      "page_number": 41,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 40 \nTable 4 890 \nA table containing the variables selected by the Elastic Net model and their corresponding 891 \nestimated coefficients obtained directly from the Elastic Net model as well as the coefficients 892 \nestimated by implementing a logistic model (non-biased coefficients).  893 \nVariable Elastic Net Estimate Logistic Estimate \n(Intercept) -5.796 -1732.902 \nMQ4 0.133 15.606 \nMQ11 0.046 20.788 \nMQ12 0.119 81.502 \nMQ13 0.103 -6.765 \nMQ15 0.057 101.386 \nMQ16 0.075 5.368 \nMQ17 0.086 145.615 \nS5_2 0.023 -6.618 \nS5_7 0.091 12.560 \nS5_11 -0.051 -190.866 \nS5_24 -0.095 -39.021 \nS5_25 0.262 31.171 \nS5_27 0.031 94.479 \nS5_31 0.165 58.559 \nS5_32 0.092 97.889 \nS5_35 0.147 56.132 \nS5_36 0.088 38.234 \nS5_38 0.036 30.691 \nS5_40 0.048 111.532 \nS5_42 0.032 42.788 \nS5_53 -0.020 12.132 \nS5_56 0.19 94.590 \nS5_57 0.259 -87.586 \nS5_68 0.081 126.088 \nS5_74 0.018 5.356 \nS5_75 0.197 -17.256 \nS5_78 -0.089 -101.315 \nS5_82 0.033 -2.060 \n 894 \n 895 \n 896 \n 897"
    },
    {
      "section": "Page 42",
      "page_number": 42,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 41 \nTable 5 898 \nA table containing the hyperparameters of the ga() function and their corresponding definitions 899 \nand default values.  900 \nParameter Description \nfitness The hyperparameter containing the optimization function is passed. No \ndefault \nis set. \ntype The type of ga that needs to be run is dependent upon the nature of the \noutcome variable. ’binary’ is selected.  \ncrossover The type of crossover performed. The default for a binary implementation is \nfound via the ‘ga_Crossover() ’ function. \npopSize An R function to generate the initial population. To access available functions, \nrun ‘ga_Population() ’. \npcrossover The probability of crossover, default of 0.8 is used. \npmutation The probability of mutation, default of 0.1 is used. \nelitism The number of best fitted chromosomes to survive at the end of each \ngeneration, default of max(1, round(popSize*0.05)) is used. \nnBits A value specifying the number of bits in a potential solution, set equal to the \nnumber of predictors. \nnames The variable names. \nmaxIter The maximum number of iterations to run before the GA search is halted, \ndefault of 100 is used. \nkeepBest A logical argument specifying if best solutions at each iteration should be \nsaved, default FALSE. \nseed A number allowed to control randomness for reproducibility. \nrun The number of consecutive generations that can occur without any \nimprovement before the GA is halted, default is modified from maxiter to 5. \n 901 \n 902 \n 903"
    },
    {
      "section": "Page 43",
      "page_number": 43,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 42 \nTable 6 904 \nA table containing the variables selected by the GA and their corresponding estimated coefficient s 905 \nin the logistic regression model.  906 \nVariable Coefficient Variable Coefficient \n(Intercept) 72.896 S5_42 -8.713 \nMQ4 -7.952 S5_45 8.668 \nMQ6 -3.454 S5_46 -10.066 \nMQ8 -5.707 S5_49 -1.448 \nMQ17 -6.154 S5_50 5.708 \nMQ18 21.166 S5_51 -0.198 \nS5_2 9.767 S5_52 -8.592 \nS5_3 -3.703 S5_53 -2.791 \nS5_4 -0.814 S5_55 -13.721 \nS5_6 3.907 S5_57 3.470 \nS5_7 -18.858 S5_58 -2.086 \nS5_8 1.915 S5_60 -16.660 \nS5_9 14.258 S5_62 4.408 \nS5_10 10.305 S5_63 5.143 \nS5_11 -11.589 S5_64 -0.372 \nS5_12 43.946 S5_65 4.143 \nS5_13 -36.764 S5_66 -8.781 \nS5_18 10.628 S5_68 -13.752 \nS5_19 2.410 S5_69 7.001 \nS5_20 -6.446 S5_72 12.343 \nS5_21 -0.442 S5_73 19.055 \nS5_23 3.657 S5_76 -9.715 \nS5_25 2.824 S5_77 -4.178 \nS5_26 1.490 S5_78 5.347 \nS5_27 4.120 S5_79 -7.315 \nS5_31 -4.880 S5_81 7.567 \nS5_32 -14.408 S5_83 -4.304 \nS5_33 -11.206 S5_84 -1.235 \nS5_38 5.880 S5_86 5.970 \nS5_41 11.462 S5_87 -14.504 \n 907 \n 908 \n 909 \n 910 \n 911 \n 912 \n 913"
    },
    {
      "section": "Page 44",
      "page_number": 44,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 43 \nTable 7 914 \nA table containing the hyperparameters of the svmfs() function as well as their corresponding  915 \ndefinitions.  916 \nParameter Description \nx Matrix of the input or predictor variables where the \ncolumns are the variables, and the rows are the observa-\ntions. \ny A numerical vector of class labels, -1, 1. \nfs.method The feature (or variable) selection method. Available \nmethods include ’scad’, ’1norm’ used for LASSO,  \n’DrHSVM’ for Elastic Net, and ’scad+L2; for Elastic  \nSCAD. \nbounds For an interval grid search a list of values for lambda1 and \nlambda 2 must be provided to the model. \ngrid.search The inner validation method used to obtain the values for \nlambda1 and lambda2. \ninner.val.method Whether or not the plots of DIRECT algorithm should be \nshown. \nshow Specification of how hyperparameters should be recoded \nor if no recoding should occur. \nparms.coding By specifying a seed, the results become reproducible. It is \nincluded here for the sake of those readers following along. \nseed Matrix of the input or predictor variables where the \ncolumns are the variables, and the rows are the observa- \ntions. \n 917 \n 918 \n 919 \n 920 \n 921 \n 922 \n 923 \n 924"
    },
    {
      "section": "Page 45",
      "page_number": 45,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 44 \nTable 8 925 \nA table containing the full output of the SVM model.  926 \nVariable Coefficient Variable Coefficient \n(Intercept) -1.209 S5_38 0.003 \nMQ3 0.002 S5_39 0.003 \nMQ5 0.003 S5_40 0.001 \nMQ8 0.003 S5_41 -0.002 \nMQ11 0.002 S5_42 0.002 \nMQ12 0.003 S5_43 0.002 \nMQ16 0.003 S5_49 0.002 \nMQ17 0.003 S5_53 -0.003 \nMQ18 0.002 S5_55 0.003 \nS5_1 0.001 S5_56 0.007 \nS5_2 0.005 S5_57 0.005 \nS5_7 0.006 S5_59 0.003 \nS5_10 -0.003 S5_65 -0.005 \nS5_11 -0.002 S5_66 0.001 \nS5_13 -0.001 S5_68 0.004 \nS5_24 -0.005 S5_69 0.001 \nS5_25 0.006 S5_72 0.001 \nS5_26 0.002 S5_74 0.005 \nS5_28 0.002 S5_75 0.007 \nS5_31 0.005 S5_78 -0.008 \nS5_32 0.005 S5_82 0.005 \nS5_35 0.005 S5_83 0.006 \nS5_37 0.002 S5_85 0.003 \n 927 \n 928 \n 929 \n 930 \n 931 \n 932 \n 933 \n 934 \n 935 \n 936 \n 937 \n 938 \n 939 \n 940 \n 941 \n 942 \n 943 \n944"
    },
    {
      "section": "Page 46",
      "page_number": 46,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 45 \nTable 9  945 \nA table containing the classifications of importance for each variable as determined by the  946 \nBoruta() function.  947 \nFinal Classification of Importance Variables \nConfirmed Important MQ12, MQ13, MQ16, S5_3, S5_34, S5_35, S5_39, \nS5_40, S5_53, S5_56, S5_57, S5_59, S5_60, S5_67, \nS5_75 \nRejected MQ1, MQ2, MQ3, MQ4, MQ5, MQ6, MQ7, MQ8, \nMQ10, MQ11, MQ14, MQ15, MQ18, MQ20, S5_1, \nS5_4, S5_6, S5_7, S5_8, S5_9, S5_10, S5_11, S5_12, \nS5_13, S5_14, S5_15, S5_16, S5_17, S5_19, S5_20, \nS5_23, S5_24, S5_26, S5_28, S5_29, S5_30, S5_32, \nS5_33, S5_36, S5_37, S5_41, S5_42, S5_43, S5_44, \nS5_45, S5_46, S5_47, S5_48, S5_49, S5_50, S5_51, \nS5_52, S5_54, S5_55, S5_58, S5_64, S5_65, S5_66, \nS5_68, S5_70, S5_71, S5_72, S5_73, S5_74, S5_76, \nS5_77, S5_78, S5_79, S5_80, S5_82, S5_83, S5_84, \nS5_86, S5_87 \nTentative MQ17, MQ19, S5_2, S5_5, S5_18, S5_21, S5_22, \nS5_25, S5_27, S5_31, S5_38, S5_61, S5_62, S5_63, \nS5_69, S5_81, S5_85 \n 948 \n 949 \n 950 \n 951 \n 952 \n 953 \n 954 \n 955 \n 956 \n 957 \n 958 \n 959 \n 960 \n 961 \n 962 \n 963 \n 964 \n 965 \n 966 \n 967"
    },
    {
      "section": "Page 47",
      "page_number": 47,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 46 \nTable 10 968 \nA table containing the predictive accuracy values obtained by all models built in this tutorial paper. 969 \nMethods are listed such that the accuracy values are ordered from least accurate to most accurate. 970 \nSignificance is determined relative to the previous model (i.e., Elastic SCAD SVM was determined 971 \nto have a statistically significant better accuracy than Elastic Net non-biased) according to a 972 \nMcNemar's Chi-squared test with continuity correction.  973 \nMethod Cross-validated Accuracy \nElastic Net non-bias ed 0.797 \nElastic SCAD SVM 0.828** \nLASSO 0.859** \nElastic Net 0.875 \nBoruta 0.875 \nLASSO non-biased 0.891 \nGA 1*** \n* p < .05, ** p < .01, *** p < .0001 974 \n 975 \n 976 \n 977 \n 978 \n 979 \n 980 \n 981 \n 982 \n 983 \n 984 \n 985 \n 986 \n 987 \n 988 \n 989 \n 990 \n 991 \n 992 \n 993 \n 994 \n 995 \n 996"
    },
    {
      "section": "Page 48",
      "page_number": 48,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 47 \nFigure 1  997 \nThe leftmost graph illustrates an overfit model on a small amount of data. We then see the 998 \ninfluence of overfitting with the introduction of new data in the center graph. Lastly, in the 999 \nrightmost graph, a new model was fit using both the old and new data. 1000 \n 1001 \n 1002 \n 1003 \n 1004 \n 1005 \n 1006 \n 1007 \n 1008 \n 1009 \n 1010 \n 1011 \n 1012 \n 1013 \n 1014 \n 1015 \n 1016 \n 1017 \n 1018 \n 1019 \n 1020 \n 1021 \n 1022 \n 1023 \n 1024 \n 1025 \n 1026 \n 1027 \n 1028 \n 1029 \n 1030"
    },
    {
      "section": "Page 49",
      "page_number": 49,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 48 \nFigure 2  1031 \nCross- validated estimate of the mean squared prediction error for LASSO as a function of the log λ. 1032 \nThe upper axis indicates the number of non-zero coefficients in the regression model at the given log 1033 \nλ. The dashed vertical line illustrates the location  of the CV minimum and the one standard error 1034 \nrule locations for λ.  1035 \n 1036 \n 1037 \n 1038 \n 1039 \n 1040 \n 1041 \n 1042 \n 1043 \n 1044 \n 1045 \n 1046 \n 1047 \n 1048 \n 1049"
    },
    {
      "section": "Page 50",
      "page_number": 50,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 49 \nFigure 3 1050 \n The basic algorithmic steps of the Genetic Algorithm. 1051 \n 1052 \n 1053 \n 1054 \n 1055 \n 1056 \n 1057 \n 1058 \n 1059 \n 1060 \n 1061 \n 1062 \n 1063 \n 1064 \n 1065 \n 1066 \n 1067 \n 1068 \n 1069 \n 1070 \n 1071 \n 1072 \n 1073 \n 1074 \n 1075 \n 1076 \n 1077 \n 1078 \n 1079 \n 1080 \n 1081 \n 1082 \n 1083 \n 1084 \n 1085 \n 1086 \n 1087 \n 1088 \n 1089 \n 1090 \n 1091"
    },
    {
      "section": "Page 51",
      "page_number": 51,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 50 \nFigure 4 1092 \nAn example of a decision tree built on the misophonia data using the ctree() function. 1093 \n 1094 \n 1095 \n 1096 \n 1097 \n 1098 \n 1099 \n 1100 \n 1101 \n 1102 \n 1103 \n 1104 \n 1105 \n 1106 \n 1107 \n 1108 \n  1109"
    },
    {
      "section": "Page 52",
      "page_number": 52,
      "text": "A TUTORIAL ON VARIABLE SELECTION METHODS 51 \n  \n \nFigure 5  \nA plot containing the Z-score transformed estimates of variable importance scores for each variable in the Boruta() model. Blue \nboxplots correspond to minimal, average, and maximum Z-scores of a shadow attribute. Red and green boxplots represent Z-scores \nof rejected and confirmed attributes respectively."
    }
  ]
}