{
  "doc_type": "scientific paper",
  "title": "Examining Physiological Responses to Misophonic Triggers",
  "authors": [
    "O_Reilley et al"
  ],
  "year": 2025,
  "journal": "Science and Engineering, University of South Carolina, Columbia, SC, USA 2 Institute for Mind and Brain, University of South Ca",
  "doi": null,
  "abstract": null,
  "keywords": [
    "biosignals",
    "misophonia",
    "electromyogram",
    "electrodermal activity",
    "electrocardiogram",
    "peripheral skin temperature",
    "ma-chine learning.  1. Introduction  Misophonia is a condition characterized by a decreased tolerance to innocuous sounds (i.e",
    "triggers)",
    "such as chewing",
    "which can be highly debilitating for some indi-viduals [1]. Nearly 5% of the adult population is affected by misophonia [2",
    "3]. Despite its prevalence",
    "research on misophonia remains limited",
    "and the biological mecha-nisms underlying this condition are poorly understood. This gap in knowledge hinders the development of effec-tive treatments and preventive interventions. Current ap-proaches to studying misophonia often rely on partici-pants’ self-report ratings of distress in response to trig-gers",
    "which are subjective and prone to variability within participants. To address this issue",
    "identifying miso-phonic triggers through physiological signals offers a more objective means of confirming responses and as-sessing the severity of this condition. Electrodermal ac-tivity (EDA) and heart rate have previously been used to describe misophonic responses [4-7]. This study aims to further characterize the physiological responses associ-ated with misophonia by also including facial electromy-ography (EMG) and peripheral skin temperature (SKT)",
    "two recording modalities that have not been previously explored in this condition.  2. Methods  Experimental paradigm. We recorded five physiolog-ical signals: EMG for the corrugator supercilii (EMG C) and zygomaticus major (EMG Z) muscles",
    "EDA",
    "SKT (measured on the palm side of the thumb of the left hand)",
    "and electrocardiogram (ECG)",
    "at 1",
    "000 Hz1 [8] in partic-ipants with and without misophonia. Participants listened to sounds",
    "viewed silent videos associated with sounds",
    "or were asked to think about sounds in a 3 (sensory mo-dality: auditory",
    "visual",
    "mental imagery) by 3 (stimuli:  1 Before preprocessing",
    "six recordings acquired at 200 Hz and two at 2",
    "000 Hz were resampled at 1",
    "000 Hz using the MNE-Python resample() function",
    "which trigger",
    "aversive",
    "non-aversive) factorial design. Partici-pants rated how distressing (misophonia group) or anti-social (control group) the stimulus was for each trial (Fig. 1). This distinction between groups is necessary because distress is a key feature of misophonic responses",
    "which is not typically experienced by control participants",
    "even though they may find the stimuli unpleasant or antisocial. Participants aged 18 to 45 without a history of neurolog-ical or psychiatric diagnoses",
    "hearing loss",
    "or hyperacusis were drawn from the South Carolina community and compensated for their time. The Misophonia group (N=35",
    "30 female",
    "Mage=25.3",
    "SDage=8.10) consisted of individuals who experience misophonic reactions trig-gered by sounds with distinct visual components but not associated with specific individuals. Trigger sets for each misophonia participant were selected based on the cate-gories of triggers identified during the screening inter-views. Control group participants (N=25",
    "20 female",
    "Mage=25.0",
    "SDage=8.10",
    "the recruitment of control partic-ipants is ongoing) were matched with misophonia partic-ipants on age",
    "biological sex",
    "handedness",
    "and stimuli presentation. Control participants were also screened to ensure they did not have misophonia. All participants completed two misophonia severity questionnaires",
    "the Duke-Vanderbilt Misophonia Screening Questionnaire (DVMSQ) [9] and the Selective Sound Sensitivity Syn-drome Scale (S-Five) [3]. They also filled two mental im-agery questionnaires",
    "the Vividness of Visual Imagery Questionnaire (VVIQ) [10] and the Bucknell Auditory Imagery Scale (BAIS) [11]",
    "which included two sub-scales for vividness (BAIS-V) and control (BAIS-C). The study was carried out in accordance with the proce-dures and protocols approved by the University of South Carolina Institutional Review Board",
    "and all participants signed an informed consent.  adopts the same approach as SciPy",
    "relying on the Fast Fourier Transform. We observed no systematic effects of this resampling on classification. Accepted for publication in the proceeding of the 7th International Conference on Advances in Signal Processing and          Artificial Intelligence (ASPAI' 2025)",
    "8-10 April 2025",
    "Innsbruck",
    "Austria"
  ],
  "research_topics": [
    "biosignals",
    "misophonia",
    "electromyogram",
    "electrodermal activity",
    "electrocardiogram",
    "peripheral skin temperature",
    "ma-chine learning.  1. Introduction  Misophonia is a condition characterized by a decreased tolerance to innocuous sounds (i.e",
    "triggers)",
    "such as chewing",
    "which can be highly debilitating for some indi-viduals [1]. Nearly 5% of the adult population is affected by misophonia [2",
    "3]. Despite its prevalence",
    "research on misophonia remains limited",
    "and the biological mecha-nisms underlying this condition are poorly understood. This gap in knowledge hinders the development of effec-tive treatments and preventive interventions. Current ap-proaches to studying misophonia often rely on partici-pants’ self-report ratings of distress in response to trig-gers",
    "which are subjective and prone to variability within participants. To address this issue",
    "identifying miso-phonic triggers through physiological signals offers a more objective means of confirming responses and as-sessing the severity of this condition. Electrodermal ac-tivity (EDA) and heart rate have previously been used to describe misophonic responses [4-7]. This study aims to further characterize the physiological responses associ-ated with misophonia by also including facial electromy-ography (EMG) and peripheral skin temperature (SKT)",
    "two recording modalities that have not been previously explored in this condition.  2. Methods  Experimental paradigm. We recorded five physiolog-ical signals: EMG for the corrugator supercilii (EMG C) and zygomaticus major (EMG Z) muscles",
    "EDA",
    "SKT (measured on the palm side of the thumb of the left hand)",
    "and electrocardiogram (ECG)",
    "at 1",
    "000 Hz1 [8] in partic-ipants with and without misophonia. Participants listened to sounds",
    "viewed silent videos associated with sounds",
    "or were asked to think about sounds in a 3 (sensory mo-dality: auditory",
    "visual",
    "mental imagery) by 3 (stimuli:  1 Before preprocessing",
    "six recordings acquired at 200 Hz and two at 2",
    "000 Hz were resampled at 1",
    "000 Hz using the MNE-Python resample() function",
    "which trigger",
    "aversive",
    "non-aversive) factorial design. Partici-pants rated how distressing (misophonia group) or anti-social (control group) the stimulus was for each trial (Fig. 1). This distinction between groups is necessary because distress is a key feature of misophonic responses",
    "which is not typically experienced by control participants",
    "even though they may find the stimuli unpleasant or antisocial. Participants aged 18 to 45 without a history of neurolog-ical or psychiatric diagnoses",
    "hearing loss",
    "or hyperacusis were drawn from the South Carolina community and compensated for their time. The Misophonia group (N=35",
    "30 female",
    "Mage=25.3",
    "SDage=8.10) consisted of individuals who experience misophonic reactions trig-gered by sounds with distinct visual components but not associated with specific individuals. Trigger sets for each misophonia participant were selected based on the cate-gories of triggers identified during the screening inter-views. Control group participants (N=25",
    "20 female",
    "Mage=25.0",
    "SDage=8.10",
    "the recruitment of control partic-ipants is ongoing) were matched with misophonia partic-ipants on age",
    "biological sex",
    "handedness",
    "and stimuli presentation. Control participants were also screened to ensure they did not have misophonia. All participants completed two misophonia severity questionnaires",
    "the Duke-Vanderbilt Misophonia Screening Questionnaire (DVMSQ) [9] and the Selective Sound Sensitivity Syn-drome Scale (S-Five) [3]. They also filled two mental im-agery questionnaires",
    "the Vividness of Visual Imagery Questionnaire (VVIQ) [10] and the Bucknell Auditory Imagery Scale (BAIS) [11]",
    "which included two sub-scales for vividness (BAIS-V) and control (BAIS-C). The study was carried out in accordance with the proce-dures and protocols approved by the University of South Carolina Institutional Review Board",
    "and all participants signed an informed consent.  adopts the same approach as SciPy",
    "relying on the Fast Fourier Transform. We observed no systematic effects of this resampling on classification. Accepted for publication in the proceeding of the 7th International Conference on Advances in Signal Processing and          Artificial Intelligence (ASPAI' 2025)",
    "8-10 April 2025",
    "Innsbruck",
    "Austria"
  ],
  "created_at": "2025-05-05T02:05:56.329591Z",
  "source_pdf": "documents/research/Global/O_Reilley et al 2025 Examining Physiological Responses to Misophonic Triggers.pdf",
  "sections": [
    {
      "section": "Page 1",
      "page_number": 1,
      "text": "Accepted for publication in the proceeding of the 7th International Conference on Advances in Signal Processing and          Artificial Intelligence (ASPAI' 2025), 8-10 April 2025, Innsbruck, Austria  \n1  Examining Physiological Responses to Misophonic Triggers  C. O’Reilly1-4, X. Yang2,5, S. Oh2,5, D. Wedell2,5, and S. V. Shinkareva2,5 1 Department of Computer Science and Engineering, University of South Carolina, Columbia, SC, USA 2 Institute for Mind and Brain, University of South Carolina, Columbia, SC, USA 3 Artificial Intelligence Institute, University of South Carolina, Columbia, SC, USA 4 Carolina Autism and Neurodevelopment Research Center, University of South Carolina, Columbia, SC, USA 5 Department of Psychology, University of South Carolina, Columbia, SC, USA E-mail: christian.oreilly@sc.edu; shinkareva@sc.edu   Summary: We collected and analyzed face electromyogram, skin electrodermal activity, peripheral skin temperature, and elec-trocardiogram in 60 participants with (N=35) and without (N=25) misophonia, a condition characterized by decreased tolerance to innocuous sounds. Our goal was to characterize the physiological response to misophonia-triggering sounds objectively. We found that misophonic responses can be identified in some cases through atypical physiological reactions to triggering stimuli, though not all participants exhibited this response. Our analyses suggest a large interindividual variability in response to misophonic triggers and highlight the need for methodological adjustments in future experiments to increase the detectability of misophonic reactions.  Keywords: biosignals, misophonia, electromyogram, electrodermal activity, electrocardiogram, peripheral skin temperature, ma-chine learning.  1. Introduction  Misophonia is a condition characterized by a decreased tolerance to innocuous sounds (i.e., triggers), such as chewing, which can be highly debilitating for some indi-viduals [1]. Nearly 5% of the adult population is affected by misophonia [2, 3]. Despite its prevalence, research on misophonia remains limited, and the biological mecha-nisms underlying this condition are poorly understood. This gap in knowledge hinders the development of effec-tive treatments and preventive interventions. Current ap-proaches to studying misophonia often rely on partici-pants’ self-report ratings of distress in response to trig-gers, which are subjective and prone to variability within participants. To address this issue, identifying miso-phonic triggers through physiological signals offers a more objective means of confirming responses and as-sessing the severity of this condition. Electrodermal ac-tivity (EDA) and heart rate have previously been used to describe misophonic responses [4-7]. This study aims to further characterize the physiological responses associ-ated with misophonia by also including facial electromy-ography (EMG) and peripheral skin temperature (SKT), two recording modalities that have not been previously explored in this condition.  2. Methods  Experimental paradigm. We recorded five physiolog-ical signals: EMG for the corrugator supercilii (EMG C) and zygomaticus major (EMG Z) muscles, EDA, SKT (measured on the palm side of the thumb of the left hand), and electrocardiogram (ECG), at 1,000 Hz1 [8] in partic-ipants with and without misophonia. Participants listened to sounds, viewed silent videos associated with sounds, or were asked to think about sounds in a 3 (sensory mo-dality: auditory, visual, mental imagery) by 3 (stimuli:  1 Before preprocessing, six recordings acquired at 200 Hz and two at 2,000 Hz were resampled at 1,000 Hz using the MNE-Python resample() function, which trigger, aversive, non-aversive) factorial design. Partici-pants rated how distressing (misophonia group) or anti-social (control group) the stimulus was for each trial (Fig. 1). This distinction between groups is necessary because distress is a key feature of misophonic responses, which is not typically experienced by control participants, even though they may find the stimuli unpleasant or antisocial. Participants aged 18 to 45 without a history of neurolog-ical or psychiatric diagnoses, hearing loss, or hyperacusis were drawn from the South Carolina community and compensated for their time. The Misophonia group (N=35, 30 female; Mage=25.3, SDage=8.10) consisted of individuals who experience misophonic reactions trig-gered by sounds with distinct visual components but not associated with specific individuals. Trigger sets for each misophonia participant were selected based on the cate-gories of triggers identified during the screening inter-views. Control group participants (N=25, 20 female; Mage=25.0, SDage=8.10; the recruitment of control partic-ipants is ongoing) were matched with misophonia partic-ipants on age, biological sex, handedness, and stimuli presentation. Control participants were also screened to ensure they did not have misophonia. All participants completed two misophonia severity questionnaires, the Duke-Vanderbilt Misophonia Screening Questionnaire (DVMSQ) [9] and the Selective Sound Sensitivity Syn-drome Scale (S-Five) [3]. They also filled two mental im-agery questionnaires, the Vividness of Visual Imagery Questionnaire (VVIQ) [10] and the Bucknell Auditory Imagery Scale (BAIS) [11], which included two sub-scales for vividness (BAIS-V) and control (BAIS-C). The study was carried out in accordance with the proce-dures and protocols approved by the University of South Carolina Institutional Review Board, and all participants signed an informed consent.  adopts the same approach as SciPy, relying on the Fast Fourier Transform. We observed no systematic effects of this resampling on classification."
    },
    {
      "section": "Page 2",
      "page_number": 2,
      "text": "Accepted for publication in the proceeding of the 7th International Conference on Advances in Signal Processing and          Artificial Intelligence (ASPAI' 2025), 8-10 April 2025, Innsbruck, Austria  \n2  Fig. 1. Experimental design.  Preprocessing and feature computation. EMG signals were notch-filtered at 60 Hz and band-pass filtered in the 20-500 Hz range. They were then rectified, baseline-cor-rected using the average amplitude over one second be-fore the stimulus onset, and smoothed with a rolling av-erage (window size = 100 samples, or 0.1 s). From these preprocessed signals, we extracted the peak amplitude within the 7-second window following the stimulus as features for classification. EDA and temperature were similarly baseline-corrected and smoothed. For these sig-nals, the amplitude 15 seconds after stimulus onset was used as a feature. For the ECG signal, R peaks were au-tomatically detected using AcqKnowledge (Biopac Sys-tems, Inc., Goleta, CA, USA). Two research assistants manually edited the R peaks to correct motion artifacts and misclassifications. The instantaneous heart rate was computed as the inverse of the R-R interval and used to characterize the ECG signals. We used the mean-square-root of the baseline-corrected heart rate within a 2-7 sec-ond window following stimulus onset as a feature. The defined windows were based on preliminary analyses. EMG Z data from one control participant were excluded due to issues related to the data acquisition process. Classical statistical analysis. Student’s t-tests and correlation analysis were used to compare stimulus types within individuals for each physiological signal. The cor-relation between ratings and physiological responses was assessed using Pearson’s correlation across all trials, re-gardless of stimulus type or sensory modality. All tests were conducted at an\talpha level of 0.001 without correc-tion for multiple comparisons. For statistical significance between time series, multiple comparisons were cor-rected using a cluster-level statistical permutation test, as implemented in MNE-Python [12]. Since not all trigger trials led to misophonic responses, we selected only half of the trigger trials based on the higher distress/antisocial ratings reported by the participants. This selection en-sures that the average physiological response to triggers is not diluted by trials in which participants did not sub-jectively feel triggered. Since preliminary analyses showed a weaker response for the mental imagery, it has been excluded from the classical statistical analyses. We report on the relative strength of responses by modality in the machine learning analysis.  Machine learning analysis. We used machine learn-ing to determine whether physiological responses could predict group membership (i.e., misophonia versus con-trol) and stimulus type (i.e., trigger, aversive, and non-aversive). Linear Support Vector Machines with default parameterization were employed (using the LinearSVC implementation in Scikit-Learn [13], based on LIBLIN-EAR [14]). A leave-one-out cross-validation was used to assess group classification, and a leave-one-group-out cross-validation, with the participant serving as the grouping factor, was used for assessing stimulus type classification. Features were standardized by removing the mean and scaling to unit variance before classifica-tion. For stimulus classification, both accuracy and weighted f1 scores were used to measure performance for the balanced case. For the unbalanced case of group clas-sification, only weighted f1 scores were reported. To as-sess the statistical significance of f1 scores and accura-cies, we bootstrapped the trial selection within partici-pants 100 times using random selection with full sample size and replacement. We evaluated the relative im-portance of the different features in the classification us-ing permutation feature importance [15]. We imputed missing data (i.e., features from the excluded EMG Z channel for one control participant) using the nearest neighbors imputation approach [16] provided by the scikit-learn KNNImputer class.  3. Results  First, we examined individual differences in physio-logical responses to trigger versus aversive stimuli for each signal. We found that misophonia triggers could be identified in some participants (Fig. 2A) and at the group level (Fig. 2D) based on physiological responses. Partic-ipants who exhibited significant physiological responses to triggers were generally those who self-reported expe-riencing the most distress when presented with these trig-gers. This is demonstrated by the fact that all but one sig-nificant physiological response differences at the individ-ual level (Fig. 2A) were observed in the nine participants who showed a clear difference in ratings between aver-sive and trigger stimuli, with t-statistics greater than 10 (Fig. 2C). Additionally, self-report and physiological re-sponses were strongly correlated in a large proportion of participants, particularly within the misophonia group (Fig. 2B).  To further investigate the predictive value of physi-ological responses, we used machine learning to assess whether group membership and stimulus types could be predicted from physiological data. When all features of the physiological responses were considered together, participant classification into misophonia and control groups was not statistically significant. Not surprisingly, self-report distress/antisocial ratings were strong predic-tors of group membership (Fig. 3A). However, predictive information was found in physiological responses when contrasts between stimulus types were used (Fig. 3B). The contrast between aversive and non-aversive stimuli appeared to be as indicative of misophonia as contrasts involving trigger stimuli. Additionally, mental imagery contrasts between stimulus types seemed to provide as much predictive information as perceived auditory or visual stimuli."
    },
    {
      "section": "Page 3",
      "page_number": 3,
      "text": "Accepted for publication in the proceeding of the 7th International Conference on Advances in Signal Processing and          Artificial Intelligence (ASPAI' 2025), 8-10 April 2025, Innsbruck, Austria  \n3  Fig. 2. Detecting triggers from physiology. A) Significant t values from Student t-tests comparing trigger and aversive responses by physiological signal and participant. B) Significant Pearson's correlations between physiological responses and distress (miso-phonia) or antisocial (control) ratings. C) Column 1: Student’s t values for the difference in means between distress/antisocial ratings for triggers (T) and aversive (Av) stimuli, noted t(T, Av). Columns 2-4: Average distress/antisocial ratings for T, Av, and NAv (non-aversive) stimuli. Columns 5-6: DVMSQ and S-Five misophonia severity scores. For A-C, the dashed light red line separates misophonia and control participants. Within groups, participants are sorted by decreasing t(T, Av) values. The dashed light gray line identifies misophonia participants with t(T, Av) > 10. D) Physiological response to stimuli. Shaded regions represent the 95% bootstrapped confidence intervals. Black overlines show time intervals where differences between responses to trigger and aversive stimuli were statistically significant according to a cluster-based permutation test.On average, physiological responses to auditory and visual but not mental imagery trials were predictive of stimulus types (Fig. 3C, E). However, these classification results only held for a subset of participants with miso-phonia (Fig. 3F), similar to results in Figure 2A. These results were primarily driven by responses to triggers in misophonia participants (Fig. 3G, H), particularly those who self-reported being strongly triggered (Fig. 3I). In-terestingly, there was greater confusion between triggers and non-aversive stimuli than between triggers and aver-sive stimuli, suggesting that in some cases, misophonic responses might not have been triggered as expected. Al-ternatively, if ECG was driving this pattern in the confu-sion matrices (Fig. 3G-I), the ECG response for non-aversive stimuli being midway between the response for triggers and aversive stimuli (Fig. 2D) could also explain this observation. To test that possibility, we evaluated the relative importance of the classification features. In line with the results displayed in Figure 2, EMG C was the most important feature, followed by SKT and ECG (Fig. 4). Further, we performed an ablation study where we re-moved the features derived from ECG and the confusion pattern did not change. Thus, it appears unlikely the dif-ference in ECG response to the different stimulus types is responsible for this observation.   Finally, we assessed the relationship between predic-tion accuracies for classifying stimulus types (Fig. 3D, F) and misophonia severity scores (Fig. 2C) across partici-pants. As expected, we found a significant correlation be-tween these measures (Fig. 5). However, contrary to our expectations, we did not observe a significant correlation between prediction accuracies and mental imagery scores in the mental imagery condition."
    },
    {
      "section": "Page 4",
      "page_number": 4,
      "text": "Accepted for publication in the proceeding of the 7th International Conference on Advances in Signal Processing and          Artificial Intelligence (ASPAI' 2025), 8-10 April 2025, Innsbruck, Austria  \n4  Fig. 3. Predictivity of the physiological response assessed through machine learning. A) Prediction (weighted f1 scores) of group membership (misophonia vs. controls) for the average response per sensory modality and stimulus type using physiology data only (i.e., 60 participants; 45 features: 3 stimulus types × 3 sensory modalities × 5 physiological signals), self-report ratings only (9 features), or both (54 features). B) Prediction (weighted f1 scores) of group membership for each sensory modality (V: visual; A: auditory; I: mental imagery) and stimulus type contrasts (T: trigger; Av: aversive; NAv: non-aversive). C, E) Weighted f1 scores (C) and accuracies (E) for stimulus type prediction per sensory modality. D, F) Prediction (weighted f1 scores) for the stimulus types per participant in the control group (D) and the misophonia group (F). Participants were sorted by decreasing order of accu-racy. Participant pairing between groups is indicated using the same numbers. For panels A-F, the pale dashed lines indicate the chance level. Participant numbers were prefixed with V, A, or I when the 5th percentile of the bootstrapped accuracy distribution was above chance levels (0.33). G-I) Confusion matrices per sensory modality for predicting stimulus types in the control (G), the misophonia (H) groups, and the misophonia participants with t(T, Av) > 10 as defined in Figure 2 (I), scaled so that the chance level is 1.0.    Fig. 4. Feature importance for stimulus type prediction by mo-dality. Larger numbers indicate features with a higher influence on the classification.       Fig. 5. P-values of Pearson’s correlations between stimulus type prediction accuracy (Fig. 3D, F) and both misophonia (Fig. 2C) and mental imagery scores.    4. Discussion Triggers were robustly identifiable from physiologi-cal responses for a relatively small subset of participants with misophonia (Fig. 2A). The absence of significant physiological responses to triggers in some misophonia sufferers may be due to factors such as the failure to rep-licate the triggering context (e.g., chewing sound in the library) [7], the idiosyncratic nature of the stimuli (e.g., the wrong type of chewing), or insufficient trigger dura-tion. Participants with the highest severity scores were not always the ones for whom triggers were identified, suggesting that we may not have elicited the expected re-sponse in all participants. Previous studies used longer stimuli (e.g., 15 s [5]), and as shown in Figure 2D, phys-iological responses were still building at the end of the 5-second stimulus.  Self-report ratings were sufficient to identify group membership, though not perfectly (Fig. 3A). This could be attributed to several factors: misophonia participants"
    },
    {
      "section": "Page 5",
      "page_number": 5,
      "text": "Accepted for publication in the proceeding of the 7th International Conference on Advances in Signal Processing and          Artificial Intelligence (ASPAI' 2025), 8-10 April 2025, Innsbruck, Austria  \n5 may not have been triggered by the designated trigger stimuli, self-report measures may have been noisy or un-reliable, or the classifier may not have fully captured the complexity of the data. The differences in physiological responses between the stimuli in the misophonia group were more pronounced and driven primarily by trigger stimuli (Fig. 3C-I). Additionally, physiological re-sponses to both aversive and non-aversive non-trigger stimuli differed between individuals with misophonia and those without (Fig. 3B), suggesting that misophonia is associated with a broader intolerance to sound [17-19]. In future work, we plan to explore using autoencoders to extract richer features and apply pre-trained deep-learn-ing models to the time-frequency representation of these physiological signals to improve our ability to identify individual misophonic triggers.  5. Conclusion  Physiological data from multiple recording modali-ties offer valuable insights into misophonia, highlighting distinct response patterns to trigger and non-trigger stim-uli. These physiological responses differ between indi-viduals with misophonia and those without, indicating a clear distinction in how each group reacts to these stim-uli. These differences extend beyond sounds to include silent videos and even the mental imagery of sounds. To-gether, these findings suggest that misophonia may be identifiable through unique physiological patterns, re-gardless of the stimulus type, highlighting the potential of physiological measures for improving our understand-ing and diagnosis of misophonia.  Acknowledgment  This work was supported by a grant from the Miso-phonia Research Fund.  References  [1] S. E. Swedo et al., “Consensus Definition of Misophonia: A Delphi Study,” Front. Neurosci., vol. 16, 2022, Acces-sed: Mar. 05, 2024. [Online]. Available: https://www.frontiersin.org/journals/neuroscience/ar-ticles/10.3389/fnins.2022.841816 [2] L. J. Dixon, M. J. Schadegg, H. L. Clark, C. J. Sevier, and S. M. Witcraft, “Prevalence, phenomenology, and impact of misophonia in a nationally representative sample of U.S. adults,” J. Psychopathol. Clin. Sci., vol. 133, no. 5, pp. 403–412, Jul. 2024, doi: 10.1037/abn0000904. [3] S. Vitoratou, C. Hayes, N. Uglik-Marucha, O. Pearson, T. Graham, and J. Gregory, “Misophonia in the UK: Preva-lence and norms from the S-Five in a UK representative sample,” PLOS ONE, vol. 18, no. 3, p. e0282777, Mar. 2023, doi: 10.1371/journal.pone.0282777. [4] A. Schröder et al., “Misophonia is associated with altered brain activity in the auditory cortex and salience net-work,” Sci. Rep., vol. 9, no. 1, p. 7542, May 2019, doi: 10.1038/s41598-019-44084-8. [5] S. Kumar et al., “The Brain Basis for Misophonia,” Curr. Biol., vol. 27, no. 4, pp. 527–533, Feb. 2017, doi: 10.1016/j.cub.2016.12.048. [6] M. Edelstein, D. Brang, R. Rouw, and V. S. Ramachand-ran, “Misophonia: physiological investigations and case descriptions,” Front. Hum. Neurosci., vol. 7, p. 296, Jun. 2013, doi: 10.3389/fnhum.2013.00296. [7] M. Siepsiak, S. R. Vrana, A. Rynkiewicz, M. Z. Rosen-thal, and W. Ł. Dragan, “Does context matter in misopho-nia? A multi-method experimental investigation,” Front. Neurosci., vol. 16, Jan. 2023, doi: 10.3389/fnins.2022.880853. [8] S. Oh, X. Yang, W. M. Hayes, A. Anderson, D. H. Wedell, and S. V. Shinkareva, “Physiological responses to aver-sive and non-aversive audiovisual, auditory, and visual stimuli,” Biol. Psychol., vol. 195, p. 108994, Jan. 2025, doi: 10.1016/j.biopsycho.2025.108994. [9] Z. J. Williams, C. J. Cascio, and T. G. Woynaroski, “Psy-chometric validation of a brief self-report measure of mis-ophonia symptoms and functional impairment: The duke-vanderbilt misophonia screening questionnaire,” Front. Psychol., vol. 13, p. 897901, 2022, doi: 10.3389/fpsyg.2022.897901. [10] D. F. Marks, “New directions for mental imagery re-search,” J. Ment. Imag., vol. 19, no. 3–4, pp. 153–167, 1995. [11] A. R. Halpern, “Differences in auditory imagery self-re-port predict neural and behavioral outcomes,” Psychomu-sicology Music Mind Brain, vol. 25, no. 1, pp. 37–47, 2015, doi: 10.1037/pmu0000081. [12] A. Gramfort et al., “MEG and EEG data analysis with MNE-Python,” Front. Neurosci., vol. 7, 2013, doi: 10.3389/fnins.2013.00267. [13] F. Pedregosa et al., “Scikit-learn: Machine Learning in Python,” J. Mach. Learn. Res., vol. 12, no. 85, pp. 2825–2830, 2011. [14] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, “LIBLINEAR: A Library for Large Linear Classi-fication,” J Mach Learn Res, vol. 9, pp. 1871–1874, Jun. 2008. [15] L. Breiman, “Random Forests,” Mach. Learn., vol. 45, no. 1, pp. 5–32, Oct. 2001, doi: 10.1023/A:1010933404324. [16] O. Troyanskaya et al., “Missing value estimation methods for DNA microarrays,” Bioinformatics, vol. 17, no. 6, pp. 520–525, Jun. 2001, doi: 10.1093/bioinforma-tics/17.6.520. [17] N. Andermane, M. Bauer, E. Sohoglu, J. Simner, and J. Ward, “A phenomenological cartography of misophonia and other forms of sound intolerance,” iScience, vol. 26, no. 4, p. 106299, Apr. 2023, doi: 10.1016/j.isci.2023.106299. [18] N. Andermane, M. Bauer, J. Simner, and J. Ward, “A symptom network model of misophonia: From heightened sensory sensitivity to clinical comorbidity,” J. Clin. Psy-chol., vol. 79, no. 10, pp. 2364–2387, 2023, doi: 10.1002/jclp.23552. [19] H. A. Hansen, A. B. Leber, and Z. M. Saygin, “What sound sources trigger misophonia? Not just chewing and breathing,” J. Clin. Psychol., vol. 77, no. 11, pp. 2609–2625, 2021, doi: 10.1002/jclp.23196."
    }
  ]
}