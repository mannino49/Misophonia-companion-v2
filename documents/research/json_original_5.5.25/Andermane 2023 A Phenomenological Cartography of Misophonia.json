{
  "doc_type": "scientific paper",
  "title": "A Phenomenological Cartography of Misophonia",
  "authors": [
    "Andermane"
  ],
  "year": 2023,
  "journal": "Journal Pre-proof",
  "doi": "10.1016/j.isci.2023.106299",
  "abstract": null,
  "keywords": [],
  "research_topics": [],
  "created_at": "2025-05-05T01:28:30.313004Z",
  "source_pdf": "documents/research/Global/Andermane 2023 A Phenomenological Cartography of Misophonia.pdf",
  "sections": [
    {
      "section": "Page 1",
      "page_number": 1,
      "text": "Journal Pre-proof\nA Phenomenological Cartography of Misophonia and Other Forms of Sound\nIntolerance\nNora Andermane, Mathilde Bauer, Ediz Sohoglu, Julia Simner, Jamie Ward\nPII: S2589-0042(23)00376-0\nDOI: https://doi.org/10.1016/j.isci.2023.106299\nReference: ISCI 106299\nTo appear in: ISCIENCE\nReceived Date: 17 November 2022\nRevised Date: 24 January 2023\nAccepted Date: 24 February 2023\nPlease cite this article as: Andermane, N., Bauer, M., Sohoglu, E., Simner, J., Ward, J., A\nPhenomenological Cartography of Misophonia and Other Forms of Sound Intolerance, ISCIENCE\n(2023), doi: https://doi.org/10.1016/j.isci.2023.106299 .\nThis is a PDF file of an article that has undergone enhancements after acceptance, such as the addition\nof a cover page and metadata, and formatting for readability, but it is not yet the definitive version of\nrecord. This version will undergo additional copyediting, typesetting and review before it is published\nin its final form, but we are providing this version to give early visibility of the article. Please note that,\nduring the production process, errors may be discovered which could affect the content, and all legal\ndisclaimers that apply to the journal pertain.\n© 2023"
    },
    {
      "section": "Page 2",
      "page_number": 2,
      "text": "Journal Pre-proof"
    },
    {
      "section": "Page 3",
      "page_number": 3,
      "text": "1 \n \nA Phenomenological Cartography of Misophonia and Other Forms of Sound Intolerance  \n \nNora Andermane, Mathilde Bauer, Ediz Sohoglu, Julia Simner and Jamie Ward * \nSchool of Psychology, University of Sussex, Brighton, UK  \n \n \n \n* Lead  Contact:  \nProf. Jamie Ward,  \nSchool of Psychology,  \nUniversity of Sussex,  \nFalmer, Brighton,  \nBN1 9QH, U.K.  \nTel. : +44 (0)1273 876598  \nE-mail : jamiew@sussex.ac.uk  \n \n \nSUMMARY  \nPeople with misophonia have strong aversive reactions to specific ‘trigger’ sounds .  Here we challenge \nthis key idea of specificity .  Machine learning was used to identify a misophonic profile from a \nmultivariate sound -response pattern.  Misophonia could be classified from most sounds (traditional \ntriggers and non -triggers) and, moreover, cross -classification showed that the profile was largely \ntransferable across sounds (rather than idiosyncrat ic for each sound).  By splitting our participants in \nother ways, we were able to show – using the same approach – a differential diagnostic profile factoring \nin potential  co-morbidities (autism, hyperacusis, ASMR).  The broad autism phenotype was classifi ed \nvia aversions to repetitive sounds rather than the eating sounds most easily classified in misophonia.  \nWithin misophonia, the presence of hyperacusis and sound -induced pain had widespread effects \nacross all sounds.  Overall, we show that misophonia is characterised by a distinctive reaction to most \nsounds that ultimately becomes most noticeable for a sub -set of those sounds.   \n \nKeywords : Misophonia, sound intolerance, hyperacusis, autism, ASMR, machine learning  \n \n  \nJournal Pre-proof"
    },
    {
      "section": "Page 4",
      "page_number": 4,
      "text": "2 \n \nINTRODUCTION  \n \n Misophonia is an extreme  aversive reaction to certain sounds (triggers) that can substantially \ndisrupt one ’s work and social life, as well as negatively impacting on wellbeing  1. This research \naddresses the question: what is it that makes a sound misophonic?  Is it something to do with the \nmanner (oral v. non -oral) or agent of production (human v. non -human)?  Or is it related to some \npsychoacoustic property of a sound linked to unpleasantness  judgments more generally  2?  To what \nextent are sounds that frequently appear as misophonic triggers also the same sounds that other people \n(non-misophonics)  dislike?   Alternatively, it may not be the sounds themselves that best identify \nmisophonia but, instead, a particular kind of response (e.g. , rage, anxiety).   In this study, w e introduce \na novel approach that we label ‘phenomenological cartography’ because we seek to map out what it is \nthat makes a person misophonic using first -person (phenomenological) responses to sounds. \nSpecifically, our map consists of 32 sounds (a mixture of typical ly described  triggers and non -triggers) \nfrom which participants give response ratings  to 17 descriptors (rage, anxiety, disgust, soothing , etc. ).  \nIn effect, determining who is misophonic and who is not (from this 17x32 array  of numbers ) lends itself \nto machine learning techniques as  a form of pattern recognition (where ‘pattern ’ is synony mous with \n‘multivariate ’ in statistical terms).   Using this approach , we address the questions posed above by \ndetermining whether some sounds permit accurate  classification of misophonia while  others do not, and \nwhether classification depends on the presence of some responses  (e.g., rage)  more than others.  It \nalso leaves open the possibility that there is not a “one-size-fits-all” solution.  For example, misophonics \nmay react with ‘rage’ to the sound of chewing but ‘hairs on end’ to creaking .  This idiosyncratic sound -\nresponse pattern  is discoverable b y our approach.  \n Misophonic triggers can sometimes appear to be highly specific – such as  the sound of one \nparticular member of the family eating – and they can vary from one person to the next.  On the face of \nit, this does  not bode well for finding a general izable  misophonic sound -response pattern .  But the \nobservation that misophonic triggers are generally described as falling into certain categories of sounds, \nnamely non -vocal oral/nasal sounds  (eating, heavy breathing) or other human -made sounds (e.g. \ntapping) contradicts the idea they are entirely idiosyncratic 3.  Specificity in misophonia could also be \nconceptualised as  a minority of aversive trigger sounds against a backdrop of normal sound tolerance.  \nEvidence in favour of this position come s from neuroimaging studies that show group differences only \nwhen misophonics, compared to non -misophonics, listen to trigger  sound s (e.g. , in the insular cortex)  \nbut not for other kinds of sounds 4 5.  However, an absence of a group difference for non -trigger sounds \nshould be interpreted with caution (limited sample sizes, limited sound exemplars).  When given sounds \nto rate for unpleasantness or discomfort, Hansen et al. 6 found that misophonics tended to give higher \nratings for all categories of sound (human oral/nasal, human other, non -human) inc luding for ones not \ntraditionally described as triggers  (e.g., animal sounds) .  In summary , there is good evidence that certain \nsounds are more affected by misophonia than others but the extent to which this reflects a narrow or \nbroad pattern of sound intolerance is unclear.   \nOutside of misophonia itself, there is also a debate as to how to differentially diagnose different \ntypes of atypical sound sensitivity.   Williams , He, Cascio and Woynaroski 7 argued that different forms \nJournal Pre-proof"
    },
    {
      "section": "Page 5",
      "page_number": 5,
      "text": "3 \n \nof sound intolerance shoul d have different kinds of specific reactions  not well captured by umbrella \nterms like ‘unpleasantness’. Specifically, they argue that misophonia should elicit anger, extreme \nannoyance, and disgust; hyperacusis should be linked to loudness and pain; and pho nophobia should \nbe linked to fear  and anxiety.  They argue that decreased sound tolerance linked to the autism spectrum \nshould have features of all three of these (i.e. , be much broader in response categories ).  Others have \nargued that misophonia is primarily characterised by physical reactions such as muscle tension  8.  \nFinally, a non-clinical trait linked to sound over -responsivity is ASMR , autonomous sensory  meridian \nresponse  e.g. 9 for which a more accurate term may be audio -visual elicited somatosensation 10.  ASMR  \nis typically described as a pleasurable experience triggered by sound s linked to  physical reactions such \nas tingling  (notably to the hair/head ). ASMR  triggering sounds are often described as whispering, \npopping and rustling sounds.  It has even been suggested that ASMR and misophonia may tend to  co-\noccur  11-13.  That is, m isophonia may be  linked to atypically intense experiences to sounds that, at least \nin some instances, have a  positive valence 8. \n In the present pre-registered study, we aim to document the sound -response profile of \nmisophonia using phenomenological cartography. We use machine learning to classify each and every \nsound and also perform cross -classification: to determine whether a misophonic response to, e.g.,  the \nsound of apple crunch can be used to predict misophonia status from responses to a completely \ndifferent sound.  Successful cross -classification would indicate a general response profile common for \nall sounds. Natural sounds are selected from the same three  categories as Hansen et al. 6, i.e., including \nsounds not normally considered as trigger s, plus a fourth category which is made up of ‘scrambled’ \nversions of misophonic triggers (such that they have similar acoustic properties but are unrecognisable).  \nAs this study is part of a wider project 14, we are also in the fortunate position of having a set of detailed \nmeasures on other relevant individual differences that speak to the question of a differential diagnosis.  \nFor the misophonics, they were asked ( in a separate session) about hyperacusis and about pain \nresponses to sounds.  Hyperacusis is typically defined  by the perception that sounds feel subjectively \ntoo loud, sometimes causing pain in the ears or head 15,16. For the non -misophonics, we split the data \naccording to being high/low in autistic traits  17, high/low sensory sensitivity  across all senses  18, and \nhigh/low interoceptive awareness  19.  Our goal here is  to ascertain whether any misophonia profile is \nspecific  to misop honia, rather than reflecting a co-morbid trait  also linked to atypical sound tolerances .  \nFinally, in a separate study , we recruit a group of people with ASMR and add a set of ASMR trigger  \nsound s which are presented alongside the same set of sounds given to misophonics .   \n \nRESULTS  \n \nThe method and analysis plan was pre -registered ( https://osf.io/27fgv/ ) and any deviations from this are \nexplicitly noted.   \n \nA Phenomenological Cartography of Misophonia  \n For each sound, responses are measured using a 0 -100 visual analog scale to descriptors such \nas rage and anxiety.  Figure 1 shows the 32 sounds x 17 responses displayed as a heatmap, with colour \nJournal Pre-proof"
    },
    {
      "section": "Page 6",
      "page_number": 6,
      "text": "4 \n \ndenoting  the magnitude of the group difference for misophonic s versus non -misophonics ( Cohen’s d ). \nA classifier (a machine learning algorithm) was developed for each of the 32 sounds in which it attempts \nto predict group status from the 17 responses , taking  different training versus test resamples of the \ndata. AUC accuracy (area -under -curve, where 0.5 is chance and 1 is perfect classification) is indicated \nfor each row, reflecting the ability of the classifier to predict group status. For visualisation purposes, \nthe sounds are ranked, top to bottom, in terms of AUC , and the responses are organised so that \nfrequently co -occurring ones are nearby  (hierarchical clustering based on Pearson’s correlation; see \nFigure S6  for the corresponding dendrogram).  \nThirty sounds could discriminate misophonics from controls above chance (where significance \nabove chance was calculated via randomly permuted group labels), and two were at chance (scrambled \nbreathing and scrambled coughing).  All 30 significant results ar e retained after FDR correction for \nmultiple comparisons (p < .05).  The most discriminating sounds were well -documented misophonic \ntriggers (crunching chips, apple crunch).  However, the 32 sounds do not cluster , in a strict way, \naccording to our initial four categories (human oral/nasal, human actions, non -human, scrambled).  \nInstead, the overall finding is of a broad pattern of atypical responsiveness to most sounds including to \nsome of the unrecognizable scrambled sounds.  Training a single classifier a cross the whole 32x17 \n(N=544) multivariate pattern results in a diagnostic accuracy for misophonia of AUC = 0.925 (specificity \n= 0.865, sensitivity = 0.802) which compares favourably against questionnaire approaches  20,21. \n With regards to which features are important for classific ation, the VSURF algorithm Variable \nSelection using Random Forests, 22 determines  the most important features for successful performance \nof the  classifier. Summing a cross all 32 classifiers  the most important features were (in descending \norder): body tension (20), annoyance (19), anxiety (16), visual experiences (16), rage  (15), discomfort \n(15), too loud (14), soothing (13), pleasurable (10), flinching (10), distress (9), disgust (8), pain (4), \ntingling (2), hairs -on-end (1), and nausea (1).  Three of these features tended to yield higher responses \nin controls relative to  misophonics (pleasurable, soothing, visual experiences) – as depicted in blue.  A \nsimple averaging of response  effect sizes, down each  column  (ignoring sign) , produces a similar rank \norder: body tension, rage, anxiety, distress, annoyance, discomfort, flinching, too loud, disgust, pain, \nsoothing, hairs on end, visual experiences, pleasurable, headache, nausea, tingling . \n \nFIGURE 1 ABOUT HERE  \n \nComparisons between Sounds : Cross -classification  \n Cross -classification is a way of testing if there is a common response profile across sounds.   \nThat is, we take the classifier for one sound (e.g., the ‘apple crunch’ classifier) and apply it to the set of \nresponse s evoked by every other sound  (the ratings of rage, annoyance and so on elicited by sounds \nof ‘chewing gum’, ‘dog drinking ’, etc.). We then repeated this for each of the 32 sounds in our stimulus \nset, yielding a 32x32 matrix of cross -classification accuracies. Figure 2  visualizes  this matrix, where the \non-diagonal cells represent classification performance within sounds  (i.e. the AUC values already noted \nin Figure 1) and the off -diagonal cells represent cross -classification between sounds  (again expressed \nas AUC) .  For visualization, we group the 32 sounds into the four original categories.   The rows are \nJournal Pre-proof"
    },
    {
      "section": "Page 7",
      "page_number": 7,
      "text": "5 \n \ndifferent classifiers  and the columns are different response profiles to enter into the classifier .  Overall, \nthere is a high degree of cr oss-classification: 58%  of cross -classifications are significant at p < .05  and \n50.1% are retained after FDR correction for multiple comparisons.  Responses to human oral/nasal \nsounds are the easiest to classify (shown here in the left eight columns).  But this is generally true \nacross most classifiers .  For example, when we train a classifier to predict misophonia status from \nscrambled breathing sounds it has AUC = 0.541 but when we test the same classi fier on a new set of \nresponses to chewing gum it has AUC = 0.646.  That is, th e response pattern from this meaningless \nsound  acts as a good predictor (/a better predictor) of a trigger sound.  The fact that the classification \naccuracy appears to go up in this example is because the group differences are more extreme for \nchewing gum .  But the fact that cross -classification occurs at all is indicative of a common response \npattern across a wide range of sounds.  \n \nFIGURE 2 ABOUT HERE  \n \nComparisons between Sounds: Why are some sounds rated highly?  \n One possibility for why some sounds gener ate consistently high ratings across descriptors is \nthat they contain common acoustic features. To test this, f or each of our 32 sounds, we measured the \nenergy in the psychoacoustic space of 2500 -5500, Hz and 1 -16 Hz temporal modulations .  This  has \npreviously been shown to predict unpleasantness ratings for natural sounds 2.  Across our set of 32 \nsounds, w e correlated these values with the mean ratings across participants ( separately for each \nparticipant group and for distress, rage, etc.).  The re were four significant correlations when considering \nall 32 sounds together  although non e survived FDR correction (headache/controls  r = 0.345 , \ntingling/misophonia  r = 0.471 , too loud/controls  r = 0.471 , visual experiences/misophonia  r = 0.373 ).  \nHowever , we suggest it is not meaningful to group all the sounds together.  P ost hoc we note d many \nlarge (and significant) correlations for our unrecognizable scrambled sounds , but far smaller \ncorrelations for the natural sounds . This was true both for the misophonics and controls  (see Figure 3) .  \nGiven the limited number of sounds in our study, we conducted a reanalysis of the N= 125 natural \nsounds from Hansen et al. 6.  Here we use their published mean ratings of discomfort (which is also \none of the 17 responses  in our study ) and conducted our own psychoacoustic analysis of their sound \nfiles (up to the first 30s).  There was no significant correlation between  mean discomfort ratings from \neither misophonics (r = -0.028) or controls (r = 0.004)  and the energy  in the region  of interest (2500 -\n5500 Hz and 1 -16 Hz temporal modulations) .  In summary, we conclude that whilst this psyc hoacoustic \nfeature has some relevance for aversive responses to sounds (as indicated by the large  correlations \nfor scrambled stimuli), it tends to be  masked by other factors when sounds are easily recognizable.   \nThis occurs irrespective of misophonia stat us (although further research is warranted given the small \nnumber of scrambled sounds used here).   \n \nFIGURE 3 ABOUT HERE  \n \nJournal Pre-proof"
    },
    {
      "section": "Page 8",
      "page_number": 8,
      "text": "6 \n \n More generally, one can ask the question whether the sounds which induce annoyance  (rage, \netc.) are the same or different across groups.  Put simply, i f we were to rank the 32 sounds according \nto their mean ratings of annoyance  for misophonics then again for controls, would we find these \nrankings to be similar?   Across all 17 features, there is a large and statistically significant correlation \nbetween the cross -group ranking of the sounds  (see Figure 4).  That is, although misophonics tend to \nrate sounds  more negatively  (in absolute terms) the relative differen ce between the sounds (i.e. , ranking \nthem from least to worst) is largely preserved between both groups.   \n \nFIGURE 4 ABOUT HERE  \n \nHeterogeneity within  Misophonics  \n This rich dataset offers opportunities for exploring individual differences amongst misophonics. \nAs a starting point, one can identify misophonics who are easy or hard to classify according to the \nnumber of sounds for which they were correctly classified as ha ving misophonia (a score from 0 to 32).  \nFor the mi sophonics, the mean number of sounds that led to an accurate group classification was 22.17 \nand S.D. was 5.11 (range = 5 to 32) .  These d ifferences amongst misophonics in classification \nperformance  were significantly correlated  with self -reported severity and breadth of misophonia from \ndata collected in a previous session using the Sussex Misophonia Scale , SMS  21.  The Pearson’s \ncorrelation s between classification accuracy and  the number of triggers or total questionnaire score  \nwere r = .381 and .417 respectively  (p < .001) , as shown in Figure 5 .  Supplemental Figure S 13 shows \nheatmaps for categorical differences in severity, comparing the transition from non -misophonia to \nmoderate misophonia, and from moderate to severe misophonia (based on the three groups identified \nin our prior research 14). These results show that our machine learning approach is tracking real -world \nseverity and, moreover, that misophonia severity  is linked to a less specific profile (i.e., extending to \nmore sounds).  \n \nFIGURE 5 ABOUT HERE  \n   \nDifferential Diagnostic Profiles : Within Misophonia  \n As our participants had been asked about the presence of other traits and co -morbidities, we \ncan determine whether the  pattern observed for misophonia is specific to misophonia .  Effect sizes \n(Cohen’s d) were computed for different splits of the dataset.  They are displayed as heatmap s for \nvisualization purposes , and we trained a classifier (as before) to determine which sounds predict the \nvarious group memberships .  The potential presence of hyperacusis within the misophonics  was \nascertained in two ways: either by directly asking them (based on 15) or through a set of questions \nincluded within the SMS that ask about pain.  These two different approaches yield different results, as \nshown in Figure 6, which depicts differences in responses as a function of self -reported hyperacusis \n(present vs absent)  and pain (present vs absent) .  The results emphasise the importance of  different \nresponses  (which are visualised as prominent columns ), rather than different sounds.  Those \nmisophonics who agree to the hyperacusis question went on to give higher r esponses  for ‘body tension’ \nJournal Pre-proof"
    },
    {
      "section": "Page 9",
      "page_number": 9,
      "text": "7 \n \n(across most sounds) whereas those who agreed to the ‘pain’ questions on the SMS went on to give \nhigher responses  for headache and pain (across most sounds) – noting that these latter two features \nare not the strongest indicators of mis ophonia per se  (see Figure 1 for example) .  The association \nbetween pain measures is not entirely trivial given that the measures were given months apart and \ndiffered substantially in method (trait -based questionnaire versus state -based responses to specif ic \nsounds).   For classifiers trained to predict  high/low  pain for misophonics as reported in the SMS \nquestionnaire , these could predict group membership above chance (p < .05) for 20 sounds with 12 \nremaining significant after FDR correction: sniffling (AUC=0.672), pen clicking (AUC=0.662), breathing \n(AUC=0.658), rusty swing (AUC=0.641), crunching chips (AUC=0.639), chewing gum (AUC=0.638), \ntyping (AUC=0.634), nails on chalkboard (AUC=0.628), clock ticking (AUC=0.625), basketball \n(AUC =0.625 ), chopping ve getables (AUC=0.620), and finger tapping (AUC=0.618) .  For classifiers \ntrained to predict self-reported hyperacusis, these could predict group membership above chance (p < \n.05) for 13 sounds with six remaining significant after FDR correction: scrambled ty ping (AUC=1.000), \nfinger tapping (AUC=0.649), basketball (AUC=0.629), scrambled coughing (AUC=0.624), scrambled \napple crunch (AUC=0.622), and typing (AUC=0.613) .  In summary, we show that comorbid traits of \nhyperacusis and sound -induced pain have a differe nt sound -response profile to misophonia and that \nthe phenomenological cartography approach is sensitive to different forms of sound intolerance.     \n \n \nFIGURE 6 ABOUT HERE  \n \nDifferential Diagnostic Profiles: Within Non -misophonics  \n With regards to individual differences amongst the non -misophonic controls, the effect sizes \nare generally less extreme .  High versus low autistic traits ( measured by the AQ) has some similarities \nwith misophonia in that the four responses depicted on the right of the heatmap in Figure 7 showed the  \nsame reversed ‘blue’ profile as misophonics (pleasurable, soothing, tingling, visual experiences).  But, \nunlike misophonia, the to p-to-bottom ordering of effect sizes was not apparent and – instead – there \nwas a pervasive low -level negativity towards most sounds.  The most  discriminating sounds between \nhigh v. low AQ were  in descending order : scrambled basketball (AUC=0.692), typing (AUC=0.690), \nfinger tapping (AUC=0.637), basketball (AUC=0.631), dog drinking (AUC=0.61 2), and rusty swing \n(AUC=0.601).  These were all significant with the top four surviving FDR correction.    \nNeither the data splits of high v. low sensory sensitivity (using the GSQ) or high v. low \ninteroceptive awareness (using the MAIA) produced any significant results that survived FDR correction  \n(see Figure s S14 and S15 ).   \n \nFIGURE 7 ABOUT HERE  \n \nComparison of Misophonia and ASMR Responders  \n A separate set of N = 254 participants were recruited  and given questionnaire s relating to \nmisophonia ( SMS ) 21 and ASMR  (ASMR -15) 23.  In addition , they completed the phenomenological \nJournal Pre-proof"
    },
    {
      "section": "Page 10",
      "page_number": 10,
      "text": "8 \n \ncartography study above with the original 32 sounds  plus the addition of  a further set of 8 ASMR triggers \n(e.g., whispering).  There was no significant correlation between the ASMR -15 and the SMS scores (r \n= .113, p = 0.072 ).  We applied the aggregated  classifier  from the previous sample  (pooled from 32 \nsounds and 17 responses ) to predict the misophonia status of these new participants .  The probability \nof being classified as misophonic (from the machine learning algorithm) was correlated with the SMS \nscores ( r = .399, p < 0.001 ) but not the ASMR -15 scores ( r = -.089, p = 0.159 ).  The differences in \ncorrelation were  significant ( t(251) = 6.368, p<.001 ). \n Excluding misophonics (i.e., those with SMS scores greater than the diag nostic  threshold  of \n50.5), we created a separate phenomenological cartography heatmap contrasting  high and low ASMR \ngroups , by applying a  mean ASMR -15 value of 2.5 (1-5 scale) to divide  the groups .   This gives a mean  \nASMR -15 score  of 3.75 in the high group  which is compa rable with other studies of ASMR responders \n(3.49 in 24, and 3.73 in 23).  The ASMR heatmap for the main set of 32 sounds is shown in Figure 8.  \nThe strongest group differences are found for the responses of soothing, tingling and pleasurable , and \nwere most strongly elicited by the scrambled sounds.  ASMR status could be classified above chance \nfrom 21 sounds (after FDR correction) suggesting that ASMR, like misophonia, is not limited to its \ntraditional ly conceptualised  triggers.  The eight ASMR triggers show s the same basic pattern ( albeit \nwith far larger effect sizes) .  These sounds also tended to be rated lower in  annoyance and discomfort  \nby ASMR responders  relative to non -responders .  Figure S1 6 shows the corresponding data for \nmisophonics in this sample (with low er effect sizes and a pattern more similar to that documented \npreviously for misophonia  e.g., Figure 1), and those reporting both ASMR and misophonia (who tend \nto resemble the ASMR profile) .  In summary, we show that misophonia and ASMR have distinctive \nprofiles with very little in common.  \nFIGURE 8 ABOUT HERE  \n \nDISCUSSION  \n \n There are significant individual differences in the ability to tolerate (or enjoy) certain sounds \nwhich, in extreme situations, impacts on quality of life enough to require intervention.  We developed \nan approach, which we term phenomenological cartography, to map out the sound -response profile of \nmisophonia .  We use this as a predictive tool by applying machine learning  to classify participants.  We \ndemonstrate that, despite heterogeneity,  there is a distinctive pattern linked to misophonia that extends \nacross most  sounds (but is clearly more extreme for certain sounds).  There are individual  differences \nwithin this profile: more severe misophonics are easier to classify, and the presence o f sound -induced \npain and/or hyperacusis within the misophonics can be considered a separate ‘layer’ superimposed on \nthe map  (for further discussion of hyperacusis subtypes  25).  We further show that th e misophonic \nsound -response pattern  is different to the sound -response profile linked to the broad autism phenotype \nor ASMR.  The former is linked to repetitive sounds (e.g., typing, basketball dribbling) and the latter to \nscrambled sounds, whispers, etc .  In effect, phenomenological cartograp hy enables us to dissect a \ncomplex s et of subjective sound -response relationships into distinct cognitive profiles.  This is important \nfor driving scientific theory and for real -world practice (differential diagnosis and possibly treatment).   \nJournal Pre-proof"
    },
    {
      "section": "Page 11",
      "page_number": 11,
      "text": "9 \n \n Misophonia is defined as a  disorder of decreased tolerance to  specific sounds 1, but o ur \nresearch challenges us to rethink the idea of specificity.  Specificity could be taken to mean that a \nminority of sounds evoke misophonic responses  against a backdrop of otherwise  normal sound \ntolerances. Alternatively,  specificity  could be taken to mean that misophonic sounds are highly \nidiosyncratic  or personal .  Our data is incompatible with these interpretations but supports , instead , a \nweaker  conclusion that  some sounds are far worse for people with misophonia than others.  Importantly, \nthis sits against a backdrop of general ized sound intolerance and a  high level of agreement (across \nindividuals and groups) about which sounds are aversive .  Similarly, there are a broad range of \nresponses linked to misophonia although some (e.g. , body tension, rage) we re found to be more \nimportant than others (e.g., nausea, hairs on end).  If specificity were the hallmark of misophonia then \nit is also puzzling t o find  an inverse relationship  between specificity and severity: more severe \nmisophonics have a less specific profile.  In effect there are two key properties  of misophonia that need \nto be explained: why there is a pervasive pattern of sound intolerances (generality) and why some \nsound s are worse than others  (weak specificity ). Whether a single explanation can account for both \nobservations is unclear and is discussed further below.  \nIt is conceivable, at least in theory, that specificity is an emergent property of generality such \nthat th ere is a ‘gain’ or amplification of the normal hierarchy of sound intolerances 26 – i.e., the degree \nof worsening in misophonia may be related to how aversive they are to begin with.  We provide evidence \nthat misophonics and controls do rank sounds similarly (across 17 rating scales) which speaks to t his \nidea.  However, there are important exceptions that suggest a more complex picture.  To give two \nexamples.  ‘Rusty [creaking] swing’ and ‘nails on chalkboard’ are strongly disliked by both groups but \nthe relative difference between the groups is margin al (as noted elsewhere  2). On the other hand , \ncrunching chips are far more misop honic than the sound of coughing  (despite the latter having clear \nnegative connotations) .  It is likely that the degree of aversion  to sounds reflect a complex evaluation \nof multiple sources of information including its meaning (semantic content, social context, etc.)  and \npsychoacoustic properties .  Changing the meaning of a sound (e.g., by presenting it with an incongruent \nvideo 27) or making it harder to identify (e.g., presenting it against noise 28) reduces the g ap between \nmisophonics and controls  although it does not necessarily eliminate it (as shown also for our scrambled \nsounds).   But whether a single dimension can predict the relative ranking of sounds (misophonic > \ncontrol) remains to be shown . \nHere we  also tested whether  a psychoacoustic feature linked to sound unpleasantness \n(frequencies between 2500 -5500, Hz and 1 -16 Hz temporal modulations  2) predicts ratings from \nmiso phonics and non -misophonics.  Our own dataset has the advantage of multiple rating  scale s but \nthe disadvantage of a low number of sounds.  But we also analysed the dat aset of Hansen et al. 6 that \nis complement ary to  our own (many more sounds albeit rated in a more limited way).  In neither dataset \ndo we find evidence of a relationship between this psychoacoustic feature and ratings of natural sounds.  \nHowever, we do find evidence of a relationship with our unrecognizable scrambled sounds (and so we \nspeculate th at many of the natural sounds in the original study by Kumar et al. . 2, may also have been \nhard to recognize).  As we only focussed on this one psychoacoustic feature it remains a possibility that \nother (unknown) psychoacoustic properties are relevant to misophonia.  It may also be the case that \nJournal Pre-proof"
    },
    {
      "section": "Page 12",
      "page_number": 12,
      "text": "10 \n \nthe psychoacoustic properties of sounds  are more relevant for autism or ASMR than they are for \nmisophonia.  In ASMR it was the unrecognizable sounds that were most potent which suggests a \nreliance on sound properties over meaning.  The repetitive sounds found to be predictive of autism  \nextended to both natural and scrambled  sounds , point ing to the same conclusion.   \nMisophonia has been found to be comorbid with autism  29, hyperacusis  14,30, and ASMR 11-13.  \nIt is important to speculate how th ese comorbidities can co -occur  if they have different sound -response \nprofiles.  One possibility is that the profiles are layered on top of each other (i.e., additive in statistical \nterms).  For example,  someone with misophon ia and autism may have both particularly strong \nresponses for misophonic triggers (chewing, etc.) and those linked of autism (tapping, etc.) against a \nbackdrop of general  sound intolerance that acts as a common denominator to both  conditions .  \nHyperacusis could be understood in a similar way, albeit manifesting itself more in terms of a \ncharacteristic response profile (e.g., pain) than sound specificity.  That is, i t is conceivable that sound \nintolerances in misophonia, hyperacusis, and autis m all stem from a general sensory hyper -sensitivity \nthat, whilst strongest for sounds, is by no means limited to it 31-33.  In effect, generalized sensory hyper -\nsensitivity may be a common seed from which somewhat different profiles emerge during  development.  \nA comorbidity between misophonia and ASMR is harder to understand as layering of different profiles \nbecause they would tend to cancel each other out (e.g., ASMR responders report high pleasure and \nsoothing, and misophonics report the opposit e).  Indeed,  our misophonia classifier does not predict \nASMR -15 scores  (the correlation is, if anything,  negative).  The self-report questionnaires used in our \nstudy hint at a weak positive relationship .  This  may reflect superficial similarity (sound over -\nresponsiveness) not borne out by detailed assessment.  ASMR and misophonia may also arguably \ndiffer in terms of whether the response is primarily s omatosens ory or affective/interoceptive  respectively  \n10 . \nOne surprising difference between our groups was that misophonics reported less ‘visual \nexperiences’ .  Further research is needed to understand how participants interpreted this term, but we \nspeculate  that it is related to mental imagery  (e.g., because visual experience ratings here were \nuniformly low for scrambled sounds  which can’t be easily imagined ).  By contrast, such a pattern would \nnot be expected for sound -vision synaesthesia , where visual expe riences are elicited by all sounds , and \nwhere these experiences are very rare  34.  An fMRI study also found that misophonics have less activity \n(compared to non -misophonics) in visual ventral stream when listening to misophonic triggers 5, which \nis consistent with our results .  Further research is needed to understand whether this is a gen eral \ndifference in the capacity to generate visual images, or if it is context specific.  It may be that when \nlistening to sounds, misophonics divert attention inwards (to the reaction in their body) whereas other \npeople imagine the source of the sound.  I f so, t his could potentially be developed as a target of \ntreatment for misophonia, in the form of strategic reorienting .   \n In sum, the  novel approach of phenomenological cartography has revealed important insights \ninto the likely mechanisms of misophonia (it is linked to broad sound intolerance), the differential \ndiagnoses of other atypical sound sensitivities  (e.g., requiring minimal tra nslation to be used cross -\nculturally) , and possible treatments for misophonia (visualizing the source).  \n \nJournal Pre-proof"
    },
    {
      "section": "Page 13",
      "page_number": 13,
      "text": "11 \n \nLimitations of the Study  \nMachine learning is commonly criticised for lack of interpretability of results (a ‘black box’ \napproach) and the risk of over-fitting (inflated performance due to, e.g., detecting confounds).  Both are \nvalid limitations of the present study, although we made attempts to mitigate them. Interpretability was \naided by performing more conventional analyses in parallel (e.g. , heatmaps derived from mass \nunivariate analyses) and separate train and test resamples guards against over -fitting.  Other choices \nof machine learning algorithms were not compared.  Finally, whilst we took advantage of having rich \nphenotypic information about o ur participants (enabling us to split the dataset in different ways) further \nresearch is needed to validate differential diagnoses on clinically defined autism and hyperacusis.  The \ngeneralizability to other demographics  also needs to be established , including cross -culturally.  \n \nAcknowledgments  \nThis research was financially support ed by the Misophonia Research Fund of the Ream Foundation.  \nWe thank Heather Hansen for sharing the sound files from their published study, and also thank the \nlate Prof.  David Baguley for his advice on the study design and measurement of sound intolerances \nincluding hyperacusis.  Dodi Swan -Capper assisted with the piloting and data collection for the ASMR \nsample.  The graphical abstract contains copyright -free icons from vecteezy.com and pixabay.com.  \n \nAuthor Contributions  \nThe study was designed by NA, JW and JS.  ES advised on sound scrambling and the psychoacoustic \nanalysis of sounds.  NA was responsible for setting up the study and for data collection.  NA and MB \nconduct ed data pre -processing, and machine learning analysis were conducted by JW.  JW wrote the \nmanuscript, and all authors contributed to editing it.  \n \nDeclaration of Interests  \nNo conflicts of interest are declared.  \n \nInclusivity and Diversity  \nWe support inclusive, diverse, and equitable conduct of research.  \n \nMain Figure Titles and Legends  \n \nFigure 1: A heatmap depicting misophonic > control effect sizes (Cohen’s d) for 32 sounds and 17 \nresponse features.  The sounds are ranked according to the ability of a  machine learning classifier to \npredict group membership (AUC).  * p < .05 significance relative to chance classification.  See also \nFigure S 2. \n \nFigure 2: A heatmap depicting cross -classification accuracy (AUC) when a classifer trained on \nresponses to one sound is used to predict group membership from a pattern of responses to another \nsound (note that the on -diagonal values are not cross -classifications).  The grid lines show divisions \nJournal Pre-proof"
    },
    {
      "section": "Page 14",
      "page_number": 14,
      "text": "12 \n \nbetween the four sound categories (human oral/nasal, human actions, non -human, and scrambled).  \nThe percentages within each grid cell refer to the number of cross -classifications that achieved \nsignificance (at p < .05 FDR corrected).  \n  \nFigure 3:  Top: Spearman’s correlations between the psychoacoustic region -of-interest identi fied by \nKumar et al. 2 and phenomenological ratings to the same sounds by misophonics and controls \nconsidering scrambled and real sounds (left and right).  Note, the critical values for alpha = 0.05 are \n0.643 (n = 8, i.e. scrambled sounds) and 0.344 (n = 24, i.e. natural sounds ). * p <.05  \n \nFigure 4.  As an example, mean ratings for ‘annoyance’ ranked from most to least annoying (according \nto the misophonic mean rating), with the mean correlation between ranks being 0.912.  Equivalent plots \nfor all 17 descriptors are provided in Supplemental Figure s S8-S12.  The right panel shows Spearman’s \ncorrelations between ratings to the same sounds across groups.  The critical value for alpha = 0.05 is \nrho >= 0.296 (for N = 32 sounds).  \n \nFigure 5: The ability to classify misophonics based on their response pattern to 32 different sounds (x -\naxis) correlates with the number of misophonic triggers reported (top figure) and the severity of the \nmisophonia (bottom).  Each point is an individual misophonic participant.  \n \nFigure 6: Heatmaps (showing Cohen’s d effect sizes) for 32 sounds (y -axis) and 17 responses (x -axis) \nfor within misophonia  comparisons, contrasting presence versus absence of hyperacusis (top) and high \nversus low scores on the SMS pain factor (bottom).   \n \nFigure 7: Heatmaps (showing Cohen’s d effect sizes) for 32 sounds (y -axis) and 17 responses (x -axis) \nfor comparisons within non -misophonics , contrasting high versus low AQ scorers.   \n \nFigure 8: Heatmaps (showing Cohen’s d effect si zes) for (top) 32 sounds (y -axis) and 17 responses (x -\naxis) and (bottom) the eight ASMR triggers, contrasting ASMR responders and non -responders \n(excluding misophonics).  \n \nSTAR METHODS  \n \nKEY RESOURCES TABLE  \n \nREAGENT or RESOURCE  SOURCE  IDENTIFIER  \nDeposited data \nRaw and analyzed data  This paper  https://osf.io/27fgv/  \n   \nSoftware and algorithms  \nJournal Pre-proof"
    },
    {
      "section": "Page 15",
      "page_number": 15,
      "text": "13 \n \nTime -domain scrambling of audio signals in Matlab  Ellis 35 https://www.ee.colu\nmbia.edu/~dpwe/res\nources/matlab/scram\nble/#1  \nNSL (Neural Systems Laboratory) auditory -cortical \nMatlab toolbox   Chi, Ru and Shamma \n36 http://nsl.isr.umd.edu\n/downloads.html  \n \nVSURF, Variable Selection using Random Forests, 22  Genuer  et al. 22 https://journal.r -\nproject.org/archive/2\n015/RJ -2015 -\n018/index.html  \n   \nOther  \nPre-registered methods and analysis plan  This paper  https://osf.io/27fgv/  \nSound stimuli  This paper  https://osf.io/27fgv/  \n   \n \nRESOURCE AVAILABILITY  \n \nLead contact  \nFurther information and requests for resources and materials may be directed to and will be fulfilled \nby the lead contact, Prof. Jamie Ward (jamiew@sussex.ac.uk).  \nMaterials availability  \n• Sounds used and created in this research are available for download and re -use (without \npermission) from https://osf.io/27fgv/  \nData and code availability  \n• All raw data has been deposited at Open Science Framework ( https://osf.io/27fgv/ ) and is \npublicly available as of the date of publication.  \n• All original code has bee n deposited at Open Science Framework ( https://osf.io/27fgv/ ) and is \npublicly available as of the date of publication.  \n• Any additional information required to reanalyze the data reported in this paper is available \nfrom the lead contact upon request.  \n \nEXPERIMENTAL MODEL AND SUBJECT DETAILS  \n \nThis study was approved by the Cross -Schools Science and Technology Research \nGovernance and Ethics Committee of the University of Sussex, UK.  \n \nSample comparing Misophonia and Non-misophonic Controls  \n A total of 418 participants completed the task and were divided into two groups: a non -\nmisophonic control group (N = 196; mean age = 33.08, S.D. = 14.29; gender = 140 female, 55 male, 1 \nnon-binary) and a misophonic group (N = 222; mean age = 41.56, S.D. = 14.32; gender = 175 female, \n42 male, 5 non -binary).  Misophonia is equally prevalent across sexes 37 and the over -representation \nof women in our samples is attributed to a recruitment bias.  This sample size is sufficient to detect \nCohen’s d of 0.3 and above (with po wer = 0.85) in univariate analyses.  Details of their recruitment and \nJournal Pre-proof"
    },
    {
      "section": "Page 16",
      "page_number": 16,
      "text": "14 \n \nassignment into groups are documented elsewhere 14, but briefly restated here.  The Sussex \nMisophonia Scale 21, rather than recruitment source, was used to determine grou p status.  Here we \nused a clustering method based on the five factors of this scale to identify three groups: non -\nmisophonic, moderate misophonia, and severe misophonia.  The latter two groups are collapsed in the \npresent study when performing a binary cla ssification, and we note that the binary group division closely \ncorresponds to the published cut -off value of > 50.5 using this measure 21.  The mean total SMS score \nof non -misophonics was 9.026 (S.D. = 9.006) and for misophonics it was 82.112 (S.D. = 22.767).   \n \nSample comparing Misophonia and ASMR Responders  \nParticipants were re cruited from the psychology undergraduate population at the University of \nSussex, but with some additional participants recruited from our misophonia database and an ASMR \ndiscussion forum (on reddit) to ensure we received sufficient high responses on these  measures.  \nMisophonia and ASMR tendencies were determined by the relevant measures rather than self -\ndeclaration or recruitment source.  A total of N=254 participants completed all relevant parts of the study \n(sound ratings, ASMR -15, SMS) and took a suffic iently long time to complete the sound task (> 10 \nminutes).  The mean age was 23.224 years (S.D. = 8.165) with 202 women, 45 men, and 7 non -binary.   \n \nMETHOD DETAILS  \n \nSound stimuli  \nThe final set of sound stimuli consisted of a core set of 32 sounds, together with an additional \nset of 8 sounds (ASMR triggers) which were given to a second sample.   \nThe core set of 32 sounds selected for this task were downloaded from a free sound datab ase \n(https://freesound.org/) and spliced or concatenated to be 15 seconds long. Their selection was \nmotivated by two previous studies investigating the reactions of misophonics and controls to natural \nsounds from different categories, and not limited to kn own misophonic triggers 6,38.  Our sound clips \nwere eight everyday sounds from each of three categories; these are human -made sounds originating \nfrom the nose or mouth (i.e., chewing apple, eating crisps, chewing gum, lip smacking, coughing, \nsnoring, breathing, sniffling), other hu man-made sounds (i.e., playing basketball, chopping vegetables, \nclicking a pen, finger tapping, nails on chalkboard, swinging on swing set, typing, walking in heels), non -\nhuman sounds (i.e., birds singing, clock ticking, crow cawing, dog drinking water, fr og croaking, printer, \nrainfall, wind chimes).  \nWe also present a new category that are used as control sounds; these sounds were created \nby using four of the human oral/nasal and four of the other human sounds (i.e., apple eating, breathing, \ncoughing, snif fling, basketball, chopping vegetables, finger tapping, typing), and scrambling them in \ntime. Scrambling was done by selecting 15ms snippets from the sound and randomly re -arranging these \nwithin 400ms from the original snippet location in the sound file us ing existing MATLAB (Mathworks \nInc., Natick, MA) scripts 35. This method of scram bling preserves the original spectral content of a sound \nwhilst removing local temporal structure, thus making it less recognisable whilst retaining its spectral \nsimilarity to the original sound.  \nJournal Pre-proof"
    },
    {
      "section": "Page 17",
      "page_number": 17,
      "text": "15 \n \nThe final selection of the 32 sounds was validated in two ea rlier pilot studies (N=28 and N=22 \nrecruited from students at the University of Sussex), with natural sounds and scrambled sounds \npresented to different groups (i.e., so they did not prime each other).  The pilot included several \nadditional sounds that wer e subsequently discarded. The primary aim was to check that participants \ncould recognise the natural sounds, and that the scrambled control sounds were not recognisable. \nLoudness and discomfort ratings were also collected for each sound using a 5 -point Lik ert scale (none, \na little, some, lots, extreme). Real sounds were on average highly recognisable (Mean recognisability \n= 87.4%, SD = 15.7%), whereas the scrambled sounds were on the whole not -recognisable (7.4% \nrecognisability, SD = 12.4%). The loudness of  7 real sounds were adjusted by scaling intensity in Praat \n39 based on the aver age subjective loudness rating from the pilot data, and the corresponding scrambled \nsounds were re -scrambled from the loudness -adjusted originals.  \n The eight ASMR triggers were selected from an initial pool of 16 after piloting them on a \nseparate group (N=24; mean age = 26.5 years, SD = 5.65; 15 female, 8 male, 1 non -binary).  The pilot \nsample were recruited from an ASMR reddit forum, all self -identified as having ASMR, and the average \nscore on the ASMR -15 was 3.86 (S.D. = 0.43).   The sound clips for th e pilot lasted one minute.  \nParticipants were asked after each sound whether they experienced ASMR. They were also asked to \npress the spacebar when they first began to experience ASMR (start time) and ‘enter’ when their ASMR \npeaked (peak time).  The eight most reliable ASMR triggers, eliciting a response in at least half of the \npilot sample, were selected.  In the selected stimuli, the average start time for onset of ASMR was \n21.79s (S.D. = 5.09, range=17 -30) and the average peak time was 35.30 (S.D. = 6.06 , range=26 -46).  \nThe original sound files were trimmed to 30s each for the main study (i.e. , after ASMR onset and close \nto peak).  A full list of triggers, and the pilot data, are included in Table S1.   \n \nProcedure  \nParticipants were required to use a compu ter screen, with headphones or a computer speaker.  \nThey were initially played a musical clip and were asked to adjust the loudness to be as loud as possible \nwithout causing any discomfort.  They were instructed to maintain this loudness for the duration o f the \ntask.  Participants listened to 32 sound clips (or 40 in the follow -up study) once or repeatedly if \nnecessary.  These were played in a random order.  Participants then rated their reactions to each of \nthese sounds. There were 17 descriptors used for the ratings and these were rated on a visual analog \nscale (ranging from 0 to 100) with endpoints marked as ‘none’ and ‘extreme’ (Figure S1).  The \ndescriptors were as follows: pain, rage, disgust, hairs -on-end, headache, flinching, tingling, soothing, \npleas urable, annoyance, nausea, visual experiences, distress, body tension, anxiety, discomfort, too \nloud.   \nAfter completing the rating task, the participants took part in a set of other questionnaires.  For \nour main study (N=418), these measures are written -up and analysed separately 14 although we make \nadditional use of some of them here: namely, the autism spectrum quotient AQ, 40, Glasgow Sensory \nQuestionnaire GSQ, 18, and the Multidimensional Assessment of Interoceptive Awareness MAIA, 19.  \nFor our second sample (N=254), the main measures of interest are the previously described SMS   and \nASMR -15 ASMR -15, 23.  The ASMR -15 consists of fifteen questions re quiring an answer on a five -point \nJournal Pre-proof"
    },
    {
      "section": "Page 18",
      "page_number": 18,
      "text": "16 \n \nLikert scale (Completely untrue for me; Somewhat untrue for me; Neither true nor untrue for me; \nSomewhat true for me; Completely true for me).  They were given the generic instructions: “This survey \nis looking at how certa in stimuli affect you. Some individuals experience intense physical and emotional \nresponses upon hearing particular sounds. These sensations and feelings can be pleasant or \nunpleasant. Sounds such as whispering, crackling, tapping, or scratching may produc e particular \nexperiences described below. Using the scale, please indicate your level of agreement with each \nstatement, upon hearing any of these, or similar sounds:” and then asked to consider it in the context \nof “When I hear certain sounds, such as whis pering, crinkling, tapping . . .”.  Example items include “It \nfeels like goosebumps on the back of my head” and “The experience is blissful.”  Items are averaged \ngiving a 1 -5 continuous scale.  \n \nQUANTIFICATION AND STATISTICAL ANALYSIS  \n \nMachine Learning  \n An aim of this analysis is to extract, in a data -driven way, a misophonic profile from the 32 \nsounds (creaking, crunching, etc.) x 17 responses (Rage, Disgust, Soothing, etc.).  Initially each sound \nwas analysed separately to consider the possibility that dif ferent sounds have a different misophonic \nprofile. The outcome of this stage is a ranked order of the 32 sounds according to their ability to classify \nmisophonics from controls. The dataset consists of, for each sound, a set of 17 features (ratings for \neach of the dimensions) from all respondents together with category labels (misophonic, non -\nmisophonic). A Random Forest classifier was trained using 10 -fold cross validation and one tuned \nhyper -parameter (the number of features in each tree varying between 2 -8). Random Forests have \nseveral advantages: they can be used where the number of features exceeds the number of examples \nwithout requiring data reduction, they have few hyper -parameters to tune, and the use of ensembles \nprotect against over -fitting.  All features were rescaled using the max -min approach. The main \ndependent variable was accuracy (measured as area -under -curve, AUC) where 0.5 is chance and 1.0 \nis perfect classification.  Specificity (proportion of misophonics correctly classed) and sensitivit y \n(proportion of non -misophonics correctly classed) are noted. Permuted datasets (N=1000) in which \ngroup labels are randomly shuffled were created to assess whether the AUC is significantly greater \nthan chance (p < .05).  When appropriate, FDR (False Disco very Rate), correction for multiple \ncomparisons is also performed 41.  The most important features that drive classification was determined \nusing the R package, VSURF (Varia ble Selection using Random Forests) 22 which has been shown to \nperform favourably compared to other solutions 42. This ranks the N = 17 features by their degree of \nimportance for classification and we report, for in terpretation purposes, those features that are not \neliminated in the first step.  \nTo analyse the degree to which the ability to classify a person as misophonic is sound -specific, \none can apply cross -classification. For example, one can determine whether the  ‘apple crunching’ \nclassifier also be used to identify misophonic people’s responses to the ‘pen clicking’ sound. Here we \ntook the trained classifiers from above and tested them on the response profile from all other sounds \ntaking AUC as a measure of class ification accuracy.  \nJournal Pre-proof"
    },
    {
      "section": "Page 19",
      "page_number": 19,
      "text": "17 \n \n The extent to which the putative ‘misophonia profile’ is specific to misophonia or related to other \nconditions/traits is explored by splicing our participant groups in other ways.  This is feasible because \nour participants completed a variety of other measures reported elsewhere.  Specifically, within our \ncontrol (non -misophonic) group we split the group into high/low according to their scores on the AQ, \nGSQ and MAIA.  For the AQ, we used the published cut -off of >= 23 for the ‘Broad Au tism Phenotype’ \nand above 17 (yielding groups of N = 57 and 139) and for the other measures a median split was taken \nin the absence of agreed cut -offs.  Within the misophonics, we were interested in the co -morbid \npresence of hyperacusis.  We had previously included a self -report question about hyperacusis \n(“Hyperacusis: When everyday sounds feel overwhelming, loud, intense, or painful that do not bother \nother people in the same way”), taken from a scoping review  15.  There were N=122 misophonics \nagreeing and N=99 disagreeing that this statement applied to them.  Another potential measure of \nhyperacusis, is the Pain factor of the SMS 21 which consists of 4 questions answered on a 5 -point Likert \nscale (never, hardly ever, sometimes, often, always).  Here we divided partici pants according to those \nanswering, on average, above or below the scale midpoint such that the high group was above \n‘sometimes’ and the low group was at or below that point (82 and 139 participants respectively).  For \neach of these splits of the dataset, the machine learning analysis procedure described above was re -\nrun although we adopted 5 -fold cross -validation to take into account the smaller sample sizes.  \n \nPsychoacoustic Properties of the Sounds  \nFor several analyses, it was important to know more about  the psychoacoustic features of our \nsound stimuli.  We applied the mathematical model of Chi, Ru and Shamma 36 on the spectral  and \ntemporal features of our sounds to generate a model of the cortical representation of sound \nfiltered/tuned to different spectro -temporal modulations.  This was done using the NSL (Neural Systems \nLaboratory) auditory -cortical Matlab toolbox ( http://nsl.isr.umd.edu/downloads.html ).  Specifically, the \ncortical representation of each sound was represented as four dimensions: sound frequency (128 bins \nwith centre frequencies ranging from 182 to 7 143 Hz); spectral modulations (7 bins: 0.125. 0.25, 0.5, 1, \n2, 4, 8 cycles per octave), temporal modulations (12 bins: +/ - 32, 16, 8, 4, 2 and 1 Hz), and time (5 ms \nbins).  Following the NSL code, the centre frequencies were calculated as follows: 440 * 2 ^ ([(0:128) -\n31]/24 (this covers around 5.3 octaves with 24 bins per octave, with 440 Hz being a central reference \nfrequency for music).  Following Kumar et al. 2 we calculated, for each sound, the mean intensity \nbetween 2500 -5500 Hz and 1 -16 Hz temporal modulations (averaging over time and spectral \nmodulations).  These values were correlated aga inst the mean group ratings for the same sounds.  We \nalso conducted the same psychoacoustic analysis of the N=125 sounds given to misophonics and \ncontrols by Hansen et al.  6 and rated on the single dimension of discomfort.  These analyses were not \npre-registered and are a post -hoc exploration of the data.   \nFinally, we also used the derived cortical representations to better understand the effect that \nour scrambling procedure had on the sounds by comparing the original 24 sounds against their \nscrambled counterparts.  A summary of this analysis is included in the supplemental information \n(Figures S2 to S5).  \n \nJournal Pre-proof"
    },
    {
      "section": "Page 20",
      "page_number": 20,
      "text": "18 \n \n \nREFERENCES  \n \n1. Swedo, S.E., Baguley, D.M., Denys, D., Dixon, L.J., Erfanian, M., Fioretti, A., Jastreboff, P.J., \nKumar, S., Rosenthal, M.Z., Rouw, R., et al. (2022). Consensus Definition of Misophonia: A \nDelphi Study. Frontiers in Neuroscience 16, 841816. 10.3389/fnins.2 022.841816.  \n2. Kumar, S., Forster, H.M., Bailey, P., and Griffiths, T.D. (2008). Mapping unpleasantness of \nsounds to their auditory representation. The Journal of the Acoustical Society of America 124, \n3810 -3817.  \n3. Brout, J.J. (2022). A Brief Commentary o n the Consensus Definition of Misophonia. Frontiers \nin Neuroscience 16, 879070. 10.3389/fnins.2022.879070.  \n4. Kumar, S., Tansley -Hancock, O., Sedley, W., Winston, J.S., Callaghan, M.F., Allen, M., Cope, \nT.E., Gander, P.E., Bamiou, D.E., and Griffiths, T.D.  (2017). The Brain Basis for Misophonia. \nCurrent Biology 27, 527 -533. 10.1016/j.cub.2016.12.048.  \n5. Schröder, A., van Wingen, G., Eijsker, N., San Giorgi, R., Vulink, N.C., Turbyne, C., and Denys, \nD. (2019). Misophonia is associated with altered brain acti vity in the auditory cortex and \nsalience network. Scientific Reports 9, 7542. doi.org/10.1038/s41598 -019-44084 -8. \n6. Hansen, H.A., Leber, A.B., and Saygin, Z.M. (2021). What sound sources trigger misophonia? \nNot just chewing and breathing. Journal of Clini cal Psychology 77, 2609 -2625. \n10.1002/jclp.23196.  \n7. Williams, Z.J., He, J.L., Cascio, C.J., and Woynaroski, T.G. (2021). A review of decreased sound \ntolerance in autism: Definitions, phenomenology, and potential mechanisms. Neuroscience \nand Biobehavioral Reviews 121, 1-17. 10.1016/j.neubiorev.2020.11.030.  \n8. Dozier, T.H., and Morrison, K.L. (2017). Phenomenology of Misophonia: Initial Physical and \nEmotional Responses. American Journal of Psychology 130, 431 -438. \n10.5406/amerjpsyc.130.4.0431.  \n9. Barratt, E. L., and Davis, N.J. (2015). Autonomous Sensory Meridian Response (ASMR): a flow -\nlike mental state. Peerj 3, e851. 10.7717/peerj.851.  \n10. Niven, E.C., and Scott, S.K. (2021). Careful whispers: when sounds feel like a touch. Trends in \nCognitive Sciences 25, 645-647. 10.1016/j.tics.2021.05.006.  \n11. Rouw, R., and Erfanian, M. (2018). A Large -Scale Study of Misophonia. Journal of Clinical \nPsychology 74, 453 -479. 10.1002/jclp.22500.  \n12. McErlean, A.B.J., and Banissy, M.J. (2018). Increased misophonia in self -repo rted Autonomous \nSensory Meridian Response. Peerj 6, e5351. 10.7717/peerj.5351.  \n13. Barratt, E.L., Spence, C., and Davis, N.J. (2017). Sensory determinants of the autonomous \nsensory meridian response (ASMR): understanding the triggers. Peerj 5, e3846. \n10.77 17/peerj.3846.  \n14. Andermane, N., Bauer, M., Simner, J., and Ward, J. (2022). A symptom network model of \nmisophonia: From heightened sensory sensitivity to clinical comorbidity. \nhttps://psyarxiv.com/t96wz/.  \n15. Fackrell, K., Potgieter, I., Shekhawat, G.S.,  Baguley, D.M., Sereda, M., and Hoare, D.J. (2017). \nClinical Interventions for Hyperacusis in Adults: A Scoping Review to Assess the Current \nPosition and Determine Priorities for Research. Biomed Research International 2017 , \n2723715. 10.1155/2017/2723715.  \n16. Baguley, D.M., and Hoare, D.J. (2018). Hyperacusis: major research questions. HNO 66, 358 -\n363.  \n17. Wheelwright, S., Auyeung, B., Allison, C., and Baron -Cohen, S. (2010). Defining the broader, \nmedium and narrow autism phenotype among parents using the A utism Spectrum Quotient \n(AQ). Molecular Autism 1, 10. 10.1186/2040 -2392 -1-10. \nJournal Pre-proof"
    },
    {
      "section": "Page 21",
      "page_number": 21,
      "text": "19 \n \n18. Robertson, A.E., and Simmons, D.R. (2013). The Relationship between Sensory Sensitivity and \nAutistic Traits in the General Population. Journal of Autism and Developmental Dis orders 43, \n775-784. 10.1007/s10803 -012-1608 -7. \n19. Mehling, W.E., Price, C., Daubenmier, J.J., Acree, M., Bartmess, E., and Stewart, A. (2012). The \nMultidimensional Assessment of Interoceptive Awareness (MAIA). Plos One 7, e48230. \n10.1371/journal.pone.0048 230.  \n20. Rosenthal, M.Z., Anand, D., Cassiello -Robbins, C., Williams, Z.J., Guetta, R.E., Trumbull, J., and \nKelley, L.D. (2021). Development and Initial Validation of the Duke Misophonia Questionnaire. \nFrontiers in Psychology 12, 709928. 10.3389/fpsyg.2021.709928.  \n21. Rinaldi, L.J., Ward, J., and Simner, J. (2022). A Factor Structure within Misophonia: The Sussex \nMisophonia Scale for researchers and clinicians.  \n22. Genuer, R., Poggi, J.M., and Tuleau -Malot, C. (2015). VSURF: An R Package f or Variable \nSelection Using Random Forests. R Journal 7, 19-33. \n23. Roberts, N., Beath, A., and Boag, S. (2019). Autonomous Sensory Meridian Response: Scale \nDevelopment and Personality Correlates. Psychology of Consciousness -Theory Research and \nPractice 6, 22-39. 10.1037/cns0000168.  \n24. Gillmeister, H., Succi, A., Romei, V., and Poerio, G.L. (2022). Touching you, touching me: Higher \nincidence of mirror -touch synaesthesia and positive (but not negative) reactions to social \ntouch in Autonomous Sensory Meridia n Response. Consciousness & Cognition 103. doi: \n10.1016/j.concog.2022.103380.  \n25. Williams, Z.J., Suzman, E., and Woynaroski, T.G. (2021). A Phenotypic Comparison of Loudness \nand Pain Hyperacusis: Symptoms, Comorbidity, and Associated Features in a Multina tional \nPatient Registry. American Journal of Audiology 30, 341 -358. 10.1044/2021_aja -20-00209.  \n26. Brout, J.J., Edelstein, M., Erfanian, M., Mannino, M., Miller, L.J., Rouw, R., Kumar, S., and \nRosenthal, M.Z. (2018). Investigating Misophonia: A Review of t he Empirical Literature, \nClinical Implications, and a Research Agenda. Frontiers in Neuroscience 12, 36. \n10.3389/fnins.2018.00036.  \n27. Samermit, P., Young, M., Allen, A.K., Trillo, H., Shankar, S., Klein, A., Kay, C., Mahzouni, G., \nReddy, V., Hamilton, V.,  and Davidenko, N. (2022). Development and Evaluation of a Sound -\nSwapped Video Database for Misophonia. Frontiers in Psychology 13, 890829. \n10.3389/fpsyg.2022.890829.  \n28. Savard, M.A., Sares, A.G., Coffey, E.B.J., and Deroche, M.L.D. (2022). Specificity of  Affective \nResponses in Misophonia Depends on Trigger Identification. Frontiers in Neuroscience 16, \n879583. 10.3389/fnins.2022.879583.  \n29. Williams, Z.J., Cascio, C.J., and Woynaroski, T.G. (2022). Psychometric validation of a brief self -\nreport measure of misophonia symptoms and functional impairment: The duke -vanderbilt \nmisophonia screening questionnaire. Frontiers in Psychology 13, 897901. \n10.3389/fpsyg.2022.897901.  \n30. Aazh, H., Erfanian, M., Danesh, A.A., and Moore, B.C.J. (2022). Audiological and Other  Factors \nPredicting the Presence of Misophonia Symptoms Among a Clinical Population Seeking Help \nfor Tinnitus and/or Hyperacusis. Frontiers in Neuroscience 16, 900065. \n10.3389/fnins.2022.900065.  \n31. Kaufman, A.E., Weissman -Fogel, I., Rosenthal, M.Z., Neema n, R.K., and Bar -Shalita, T. (2022). \nOpening a window into the riddle of misophonia, sensory over -responsiveness, and pain. \nFrontiers in Neuroscience 16, 907585. 10.3389/fnins.2022.907585.  \n32. Rinaldi, L.J., Simner, J., Koursarou, S., and Ward, J. Autistic  traits, emotion regulation, and \nsensory sensitivities in children and adults with Misophonia. Journal of Autism and \nDevelopmental Disorders. 10.1007/s10803 -022-05623 -x. \n33. Wu, M.S., Lewin, A.B., Murphy, T.K., and Storch, E.A. (2014). Misophonia: Incidenc e, \nPhenomenology, and Clinical Correlates in an Undergraduate Student Sample. Journal of \nClinical Psychology 70, 994 -1007. 10.1002/jclp.22098.  \nJournal Pre-proof"
    },
    {
      "section": "Page 22",
      "page_number": 22,
      "text": "20 \n \n34. Ward, J. (2011). Visual music in arts and minds: Explorations with synaesthesia. In Art and the \nSenses, F. Ba cci, and D. Melcher, eds. (Oxford University Press).  \n35. Ellis, D.P.W. (2010). Time -domain scrambling of audio signals in Matlab. \nhttps://www.ee.columbia.edu/~dpwe/resources/matlab/scramble/#1.  \n36. Chi, T., Ru, P.W., and Shamma, S.A. (2005). Multiresolutio n spectrotemporal analysis of \ncomplex sounds. Journal of the Acoustical Society of America 118, 887 -906. \n10.1121/1.1945807.  \n37. Jakubovski, E., Müller, A., Kley, H., de Zwaan, M., and K., M. -V. (2022). Prevalence and clinical \ncorrelates of misophonia sympt oms in the general population of Germany. Frontiers in \nPsychiatry 13. doi:10.3389/fpsyt.2022.1012424.  \n38. Edelstein, M., Brang, D., Rouw, R., and Ramachandran, V.S. (2013). Misophonia: physiological \ninvestigations and case descriptions. Frontiers in Human Neuroscience 7, 296. \n10.3389/fnhum.2013.00296.  \n39. Boersma, P., and Weenink, D. (2021). Praat: doing phonetics by computer [Computer \nprogram]. Version 6.1.50, retrieved 20 June 2021 from http://www.praat.org/.  \n40. Baron -Cohen, S., Wheelwright, S., Skinner,  R., Martin, J., and Clubley, E. (2001). The Autism -\nSpectrum Quotient (AQ): Evidence from Asperger syndrome/high -functioning autism, males \nand females, scientists and mathematicians. Journal of Autism and Developmental Disorders \n31, 5-17. 10.1023/a:1005653 411471.  \n41. Benjamini, Y., and Hochberg, Y. (1995). CONTROLLING THE FALSE DISCOVERY RATE - A \nPRACTICAL AND POWERFUL APPROACH TO MULTIPLE TESTING. Journal of the Royal Statistical \nSociety Series B -Statistical Methodology 57, 289 -300.  \n42. Speiser, J.L., Miller, M.E., Tooze, J., and Ip, E. (2019). A comparison of random forest variable \nselection methods for classification prediction modeling. Expert Systems with Applications \n134, 93-101. 10.1016/j.eswa.2019.05.028.  \n \n \n \nJournal Pre-proof"
    },
    {
      "section": "Page 23",
      "page_number": 23,
      "text": "Journal Pre-proof"
    },
    {
      "section": "Page 24",
      "page_number": 24,
      "text": "Journal Pre-proof"
    },
    {
      "section": "Page 25",
      "page_number": 25,
      "text": "Journal Pre-proof"
    },
    {
      "section": "Page 26",
      "page_number": 26,
      "text": "Journal Pre-proof"
    },
    {
      "section": "Page 27",
      "page_number": 27,
      "text": "Journal Pre-proof"
    },
    {
      "section": "Page 28",
      "page_number": 28,
      "text": "Journal Pre-proof"
    },
    {
      "section": "Page 29",
      "page_number": 29,
      "text": "Journal Pre-proof"
    },
    {
      "section": "Page 30",
      "page_number": 30,
      "text": "Journal Pre-proof"
    },
    {
      "section": "Page 31",
      "page_number": 31,
      "text": "Highlights  \n• Misophonia can be identified by applying machine learning to a sound -response profile  \n• Misophonia is linked to broad s ound intolerances beyond specific triggers  \n• Sound intolerance s linked to autism and hyperacusis  are different to misophonia  \n• Our findings do not support a link between misophonia and ASMR  \n \nJournal Pre-proof"
    }
  ]
}